[
  {
    "objectID": "notes/python_concepts.html",
    "href": "notes/python_concepts.html",
    "title": "Python concepts notes",
    "section": "",
    "text": "These are some of my notes on python concepts I gathered from various sources."
  },
  {
    "objectID": "notes/python_concepts.html#super",
    "href": "notes/python_concepts.html#super",
    "title": "Python concepts notes",
    "section": "Super()",
    "text": "Super()\nsuper() allows to call the base class implicitly without the need to use the base class name explicitly. The main advantage is seen during multiple inheritance. The child classes that may be using cooperative multiple inheritance will call the correct next parent class function in the Method Resolution Order(MRO).\nSyntax Python3 super().__init__() Python2 and still valid in python3 super(childclassname,self).__init__()\nAvoid using like this – super(self.__class__,self).__init__(). This leads to recursion but fortunately python3 syntax standard side steps this problem.\nMore info in stackoverflow"
  },
  {
    "objectID": "notes/python_concepts.html#pycache__",
    "href": "notes/python_concepts.html#pycache__",
    "title": "Python concepts notes",
    "section": "__Pycache__",
    "text": "__Pycache__\n__pycache__ is a directory containing python3 bytecode compiled and ready to be executed. This is done to speed up loading of the modules. Python caches the compiled version of each module in this directory. More info in the python official docs"
  },
  {
    "objectID": "notes/python_concepts.html#zip-function",
    "href": "notes/python_concepts.html#zip-function",
    "title": "Python concepts notes",
    "section": "Zip function",
    "text": "Zip function\nThis function enables combination of elements in 2 or more lists. If the lists are of unequal lengths then the minimum length is taken.\nFor example - zip([1,2,3],[\"one\", \"two\"]) returns a tuple (1, \"one\"), (2, \"two\"). Notice how 3 is not part of the zip operation."
  },
  {
    "objectID": "notes/python_concepts.html#lambda-function",
    "href": "notes/python_concepts.html#lambda-function",
    "title": "Python concepts notes",
    "section": "Lambda function",
    "text": "Lambda function\nAlso knows as anonymous functions helps reduce the need to define unnecessary custom functions. For example, a function that returns a simple arithmetic operation can be made as a lambda function.\nExample - lambda x,y,...: x+y+... - lambda takes several arguments but returns only one expression."
  },
  {
    "objectID": "notes/python_concepts.html#map-function",
    "href": "notes/python_concepts.html#map-function",
    "title": "Python concepts notes",
    "section": "Map function",
    "text": "Map function\nMap is a function that allows some kind of function upon a sequence. This sequence can be list, tuple, string etc.\nSyntax: map(function, seq)\nlambda functions are commonly used with map functions.\nExample -\n\nx = [1,2,3,4]\ndef square(x):\n    return x*x\nmap(square,x) #--&gt;returns an iterator in python3.\n#To return in the desired sequence, use that as prefix in map. \nlist(map(square,x)) #--&gt; returns as a list.\n\n[1, 4, 9, 16]\n\n\nWith lambda function:\n\na = [1,2,3,4]\nlist(map(lambda x:x*x,a))\n\n[1, 4, 9, 16]\n\n\nWith map more than one sequence can be used -\n\nb = [1, 1, 1, 1]\nlist(map(lambda x,y:x+y, a,b))    \n\n[2, 3, 4, 5]"
  },
  {
    "objectID": "notes/python_concepts.html#filter-function",
    "href": "notes/python_concepts.html#filter-function",
    "title": "Python concepts notes",
    "section": "Filter function",
    "text": "Filter function\nThis function is used to filter the outputs if the sequence satisfies some condition. This can be easily written as a list comprehension or a generator. list(filter(lambda x:x%2==0, range(1,11))"
  },
  {
    "objectID": "notes/python_concepts.html#reduce-function",
    "href": "notes/python_concepts.html#reduce-function",
    "title": "Python concepts notes",
    "section": "Reduce function",
    "text": "Reduce function\nThis was removed from the inbuilt functions in python3 and added to functools. It is similar to map function but unlike map it takes only one iterable. list(reduce(lambda x,y:x+y, a)). Internally assigns x and y and calculates the desired function.\nEach of the above function can be substituted with list comprehension.\n\nnamespace and variable scope\nNamespace is the space occupied by an entity in the program. When two or more programs contain the same name, a method to invoke a unique program is done using its module name.\n\n\nLEGB Rule\nVariable scope has three visibilty levels – builtin, global, enclosed, and local.\nLocal scope - variables defined inside a local function. Their lifecycle ends with the local function. Global scope - variable defined at the top of the module and is available for all the functions underneath it. Enclosed scope - seen in nested function. built-in scope - names within python built-in functionality like print().\nChange a global variable inside a local function? use global keyword. Change a enclosed variable inside an enclosed(nested)-local function? use nonlocal keyword. The order of execution follows local, enclosed, global, and built-in.\n\n\nClosures\nClosure is a concept to invoke an enclosed(nested)-local function outside of its scope. This uses a python’s property – functions are first class object. Example -\n\ndef f():\n    return 2\ng = f\n\ng here gets the function f’s location(reference) or the path of the function till the end of it. This functionality is helpful in accessing an enclosed(nested)-local function beyond its scope.\nexample -\n\ndef f():\n    x = 4\n    def g():\n        y = 3\n        return x+y\n    return g\n\na = f()\nprint(a) #--&gt; returns path till function 'g'\nprint(a()) #--&gt; returns 7\nprint(a.__name__) #--&gt; return function 'g' name.\n\n&lt;function f.&lt;locals&gt;.g at 0x1037b8dc0&gt;\n7\ng\n\n\nWhy closures? * Avoid global values * Data hiding * Implement decorators\n\n\nDecorators\nAny callable object that is used to modify a function or class. Adds additional functionality to existing function or class. Basically a wrapper around the existing function where the existing function code couldn’t be changed but additional features/checks are necessary. It is much easier to use the decorator than writing one. The wrapper becomes complex as the functions it wraps get longer.\nA decorator should: * take a function as parameter * add functionality to the function * function needs to return another function\nTwo types: - Function decorator - Class decorator\n\n\nfunction call as parameter\n\ndef f1():\n    print(\"hello from f1\")\ndef f2(function):\n    print(\"hello from f2\")\n    function()\nf2(f1)\n\nhello from f2\nhello from f1\n\n\n\n\nMultiple decorators\n\ndef f1(func):\n    def inner():\n        return \"first \" + func() +\" first\"\n    return inner\n\ndef f2(func):\n    def wrapper():\n        return \"second \" + func()+\" second\"\n    return wrapper\n\n@f1\n@f2\ndef ordinary():\n    return \"good morning\"\n\nprint(ordinary())\n\nfirst second good morning second first\n\n\n&gt;&gt;&gt;&gt; first second good morning second first\nAt first, the f2 decorator is called and prints second good morning second, then f1 decorator takes that output and prefixes and suffixes with first.\n\n\nDecorators with parameters\nTo pass parameters to a decorator, the nested function in the previous case must be defined inside a function.\n\ndef outer(x):\n    def f1(func):\n        def inner():\n            return func() + x\n        return inner\n    return f1\n\n\n@outer(\" everyone\")\ndef greet():\n    return \"good morning\"\n\nprint(greet)\n\n&lt;function outer.&lt;locals&gt;.f1.&lt;locals&gt;.inner at 0x13f874310&gt;\n\n\n\ndef div1(a,b):\n    return a/b\n\ndef div2(a,b,c):\n    return a/b/c\n\nprint(div1(5,0))\nprint(div2(3,4,5))\n\nZeroDivisionError: division by zero\n\n\nTo protect against division-by-zero error, a decorator is written.\n\ndef div_by_zero_dec(func):\n    def inner(*args):\n        for i in args[1:]:\n            if i == 0:\n               return \"Enter valid non-zero input for denominator\"\n        #gives the general error and not decorator output for div2 function if the input is zero\n        #answer =  [\"Enter valid non-zero input for denominator\" if i == 0 else func(*args) for i in args[1:]]\n        return func(*args)\n        #return answer  \n    return inner\n\n@div_by_zero_dec\ndef div1(a,b):\n    return a/b\n\n@div_by_zero_dec\ndef div2(a,b,c):\n    return a/b/c\n\nprint(div1(5,0))\nprint(div2(3,1,0))\n\nEnter valid non-zero input for denominator\nEnter valid non-zero input for denominator\n\n\n\n\nData hiding\nDecorators inherently hides the original function. To avoid that we can use wraps() method from functools.\n\nfrom functools import wraps\ndef outer(x):\n    def f1(func):\n        @wraps(func)\n        def inner():\n            return func() + x\n        return inner\n    return f1\n\n\n@outer(\" everyone\")\ndef greet():\n    return \"good morning\"\n\nprint(greet.__name__)\n\ngreet\n\n\n\n\nClass decorators\nDecorator function can be applied to class methods also.\n\n#check for name equal to Justin is done with a decorator\ndef check_name(func):\n    def inner(name_ref):\n        if name_ref.name == \"Justin\":\n            return \"There is another Justin\"\n        return func(name_ref)\n    return inner\n\nclass Printing():\n    def __init__(self, name):\n        self.name = name\n    \n    @check_name\n    def print_name(self):\n        print(f\"username is {self.name}\")\n\np = Printing(\"Justin\")\np.print_name()\n#username is Justin\n#There is another Justin\n\n'There is another Justin'\n\n\nTo make a class decorator, we need to know about a special method called __call__. If a __call__ method is defined, the object of a class can be used as function call.\n\nclass Printing():\n    def __init__(self, name):\n        self.name = name\n    \n    def __call__(self):\n        print(f\"username is {self.name}\")\n\np = Printing(\"Lindon\")\np()\n# username is Lindon\n\nusername is Lindon\n\n\n\n\nClass decorator on a function\n\nclass Decorator_greet:\n    def __init__(self, func):\n        self.func = func\n    def __call__(self):\n        return self.func().upper()\n\n@Decorator_greet\ndef greet():\n    return \"good morning\"\n\nprint(greet())\n#GOOD MORNING\n\nGOOD MORNING\n\n\n\n\nBuilt-in decorators\n\n@property\n@classmethod\n@staticmethod\n\n\n\n@property\n\nClass methods as attributes The idea of a wrapper is to make changes to code base without hindering its use for the end user. Using @property decorator, a class variable which is now a class method will give the result if used just like accessing the variable. For example - objectname.function() or objectname.function will give the same result without errors. So the user can access just like they did previously.\n\nBorrowing idea from other programming languages, the private variables are defined with __ prefix. So to access those variables, getter, setter, and deleter methods are necessary. Accessing is done with getter method. So it gets @property decorator. Both setters and deleters get @functionname.setter or @functionname.deleter. (verify this. could be wrong)\n\n@classmethod Insted of self, the classmethod decorator takes cls as first argument for its function. These methods can access and modify class states.\n@staticmethod This is similar to classmethod but takes no predefined argument like instance method(self) or classmethod(cls). These methods can’t access class state. So ideally they are used for checking conditions.\n\nDecorator Template\n\nimport functools\n\ndef my_decorator(func):\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        # Do something before\n        result = func(*args, **kwargs)\n        # Do something after\n        return result\n    return wrapper"
  },
  {
    "objectID": "notes/python_concepts.html#context-managers",
    "href": "notes/python_concepts.html#context-managers",
    "title": "Python concepts notes",
    "section": "Context Managers",
    "text": "Context Managers\nUsually with is used w.r.t to file operations, database connections. In addition to using with and as keywords, we can make custom context managers using @contextlib.contextmanger which is a generator decorator.\n\nfrom contextlib import contextmanager\n\n@contextmanager\ndef opening(filename, method):\n    print(\"Enter\")\n    f = open(filename, method) \n    try:\n        yield f\n    finally:\n        f.close()\n        print(\"Exit\")\n\nwith opening(\"hello.txt\", \"w\") as f:\n    print(\"inside\")\n    f.write(\"hello there\")\n\nEnter\ninside\nExit"
  },
  {
    "objectID": "posts/2023-06-12-Attach-iam-policy-to-roles/index.html",
    "href": "posts/2023-06-12-Attach-iam-policy-to-roles/index.html",
    "title": "Terraform - Attach IAM policies to a role",
    "section": "",
    "text": "To access services in AWS, permissions must be given to the services.\nTo control access, AWS has Identity and Access Management (IAM). Each user can take multiple roles. Each role can be restricted to only certain services. This restriction is given in the form of policies.\nPolicies are basically protocols defined in JSON format that allow smooth, secure communication between services.\nTerraform allows for infrastructure management. In this post we will see how to define policies and attach them to a role for our task."
  },
  {
    "objectID": "posts/2023-06-12-Attach-iam-policy-to-roles/index.html#aws-lambda---s3-bucket",
    "href": "posts/2023-06-12-Attach-iam-policy-to-roles/index.html#aws-lambda---s3-bucket",
    "title": "Terraform - Attach IAM policies to a role",
    "section": "AWS Lambda -> S3 bucket",
    "text": "AWS Lambda -&gt; S3 bucket\nFor our task we have a lambda function. It has to retrieve a file from a S3 bucket. Therefore, we need to give lambda function to access this S3 bucket.\n\n\nLambda-S3 policy\n\nresource \"aws_iam_policy\" \"lambda_s3artifact_role_policy\" {\n1  name = \"policy-s3-artifact-access-to-lambda\"\n  description = \"IAM Policy for s3-artifact-access-to-lambda\"\n  policy = jsonencode({\n\"Version\": \"2012-10-17\",\n\"Statement\": [{\n    \"Sid\": \"VisualEditor0\",\n    \"Effect\": \"Allow\",\n    \"Action\": [\n        \"s3:Get*\",\n2        \"s3:List*\"\n    ],\n    \"Resource\": [\n        \"arn:aws:s3:::${var.artifact_bucket}\",\n3        \"arn:aws:s3:::${var.artifact_bucket}/*\"\n    ]}]\n })\n}\n\n\n1\n\nIAM policy name.\n\n2\n\nWhat can this lambda function to do with the S3 bucket.\n\n3\n\nWhich S3 bucket to access."
  },
  {
    "objectID": "posts/2023-06-12-Attach-iam-policy-to-roles/index.html#aws-lambda---dynamodb",
    "href": "posts/2023-06-12-Attach-iam-policy-to-roles/index.html#aws-lambda---dynamodb",
    "title": "Terraform - Attach IAM policies to a role",
    "section": "AWS Lambda -> DynamoDB",
    "text": "AWS Lambda -&gt; DynamoDB\nAfter the file retrieval, the lambda function performs some function and outputs some result. We would like this result to stored inside a DynamoDB table. That means permission to DynamoDB table.\nThat is given by -\n\n\nLambda-DynamoDB policy\n\nresource \"aws_iam_policy\" \"lambda_dynamodb\" {\n  name = \"policy_lambda_access_to_dynamodb\"\n  description = \"\"\n  policy = jsonencode({\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [{\n        \"Effect\": \"Allow\",\n        \"Action\": [\n            \"dynamodb:BatchGetItem\",\n            \"dynamodb:GetItem\",\n            \"dynamodb:Query\",\n            \"dynamodb:Scan\",\n            \"dynamodb:BatchWriteItem\",\n            \"dynamodb:PutItem\",\n1            \"dynamodb:UpdateItem\",\n            \"logs:CreateLogStream\",\n            \"logs:PutLogEvents\",\n2            \"logs:CreateLogGroup\"\n        ],\n3        \"Resource\": \"arn:aws:dynamodb:${var.dynamodb_region}:${var.dynamodb_accountid}:table/${var.dbtable_name}\"\n    }]\n  })\n}\n\n\n1\n\nWhat dynamodb functions can the lambda function do?\n\n2\n\nEnabling logging functionality with Cloudwatch.\n\n3\n\nWhich DynamoDB table to access?"
  },
  {
    "objectID": "posts/2023-06-04-api-lambda-ecr/index.html",
    "href": "posts/2023-06-04-api-lambda-ecr/index.html",
    "title": "Building Data Pipelines - Part 2 - AWS Cloud",
    "section": "",
    "text": "In part 1, we saw how to make and test lambda function locally. In part 2, we will switch focus to AWS cloud provider. We will migrate our code to use AWS cloud platform.\n\n\nWe will use these services to form our data pipeline:\n\nS3 - to download our predicted sales price table.\nECR - to upload our docker image with installed dependencies.\nAWS Lambda - to make the business logic of the web application. It will use ECR image as source. Sends predicted outputs to DynamoDB.\nAPI Gateway - Rest API to invoke AWS Lambda function.\nDynamoDB - to store the predicted sales prices. Invoked from Lambda."
  },
  {
    "objectID": "posts/2023-06-04-api-lambda-ecr/index.html#services-used",
    "href": "posts/2023-06-04-api-lambda-ecr/index.html#services-used",
    "title": "Building Data Pipelines - Part 2 - AWS Cloud",
    "section": "",
    "text": "We will use these services to form our data pipeline:\n\nS3 - to download our predicted sales price table.\nECR - to upload our docker image with installed dependencies.\nAWS Lambda - to make the business logic of the web application. It will use ECR image as source. Sends predicted outputs to DynamoDB.\nAPI Gateway - Rest API to invoke AWS Lambda function.\nDynamoDB - to store the predicted sales prices. Invoked from Lambda."
  },
  {
    "objectID": "posts/2023-06-04-api-lambda-ecr/index.html#lambda-function-to-predict-sales",
    "href": "posts/2023-06-04-api-lambda-ecr/index.html#lambda-function-to-predict-sales",
    "title": "Building Data Pipelines - Part 2 - AWS Cloud",
    "section": "Lambda function to predict sales",
    "text": "Lambda function to predict sales\nTo build this pipeline we will start with our lambda_function using MLflow to download ML trained artifact from S3 bucket to predict sales prices. We will first check if it is possible to connect to S3 and then move forward to complete cloud solution.\n\n\nlambda_function.py\n\nimport os\n\nimport mlflow\nimport pandas as pd\n\nRUN_ID = os.getenv(\"RUN_ID\")  # \"5651db4644334361b10296c51ba3af3e\"\nS3_BUCKET_NAME = os.getenv(\n    \"S3_BUCKET_NAME\"\n)  # \"mlops-project-sales-forecast-bucket\"\nEXPERIMENT_ID = 1\nFILE_ADDRESS = \"artifacts/predictions/lgb_preds.parquet\"\npred_s3_location = (\n    f\"s3://{S3_BUCKET_NAME}/{EXPERIMENT_ID}/{RUN_ID}/{FILE_ADDRESS}\"\n)\n\n\ndef read_parquet_files(filename: str):\n    \"\"\"\n    Read parquet file format for given filename and returns the contents\n    \"\"\"\n    df = pd.read_parquet(filename, engine=\"pyarrow\")\n    return df\n\n\nif os.path.exists(\"lgb_preds.parquet\"):\n    df_test_preds = read_parquet_files(\"lgb_preds.parquet\")\nelse:\n    s3_file = mlflow.artifacts.download_artifacts(\n        artifact_uri=pred_s3_location, dst_path=\"/tmp\"\n    )  # /tmp is added as lambda gives write access only to that folder. Otherwise use \"./\" .\n    df_test_preds = read_parquet_files(s3_file)\n\n\ndf_items = read_parquet_files(\"items.parquet\")\n\n\ndef predict(find, item_idx: int):\n    \"\"\"\n    Takes the json inputs, processes it and outputs the unit sales\n    \"\"\"\n    try:\n        idx = pd.IndexSlice\n        # df_items.sample(1).index[0]\n        x = df_test_preds.loc[idx[find[\"store_nbr\"], item_idx, find[\"date1\"]]][\n            \"unit_sales\"\n        ]\n    except KeyError:\n        print(\"This item is not present this store. Try some other item\")\n        return -0.0\n    else:\n        return float(round(x, 2))\n\n\ndef lambda_handler(event, context=None) -&gt; dict:\n    \"\"\"\n    lambda handler for predict method\n    \"\"\"\n\n    find = event[\"find\"]\n    item = df_items.sample(1)\n    item_idx, item_family = item.index[0], item[\"family\"].values[0]\n    pred_unit_sales = predict(find, item_idx)\n\n    result = {\n        \" Store\": find[\"store_nbr\"],\n        \" item\": int(item_idx),\n        \"Family\": item_family,\n        \"Prediction date\": find[\"date1\"],\n        \"Unit_sales\": pred_unit_sales,\n    }\n    return result"
  },
  {
    "objectID": "posts/2023-06-04-api-lambda-ecr/index.html#building-container-image",
    "href": "posts/2023-06-04-api-lambda-ecr/index.html#building-container-image",
    "title": "Building Data Pipelines - Part 2 - AWS Cloud",
    "section": "Building container image",
    "text": "Building container image\nBuilding the docker image is same as previous post.\n\n\nTerminal 1\n\ndocker build -t lambda-app:v1 .\n\n\nUsing sensitive/secret access keys.\nSince we are using MLflow’s download artifact functionality directly from the S3, we need to supply our AWS access key to the docker as environment variables. Care needs to be taken when supplying sensitive information. Most hard code these secrets in their code. This is a bad practice as it will make its way to the code repository eventually. Next possible option is to supply as environment variable in the terminal. However, a simple history command will reveal the secrets. So, to avoid that we can set HIST_IGNORE_SPACE and also add keywords to HISTIGNORE ensuring any command with a leading space in front of it will not be stored in history cache.\n\n\nTerminal 1\n\n1set +o history\nexport HISTIGNORE= \\\n2        \"ls*:cat*:*AWS*:*SECRET*:*KEY*:*PASS*:*TOKEN*\"\n export AWS_ACCESS_KEY_ID=xxxx\n export AWS_SECRET_ACCESS_KEY=xxxtt\n\n\n1\n\nDisables history for the current terminal session.\n\n2\n\nHistory doesn’t store any command that has leading space. Highly useful for sensitive environment variables.\n\n\n\n\nRunning the container and testing locally\nUnlike last time to run the docker image as container requires a few arguments. We give them to docker as -e environment variables. Setting environment variables in the terminal is not going to pass over to the docker container. So we need to explicitly supply AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, RUN_ID and S3_BUCKET_NAME while executing docker run command in addition to the regular arguments.\n\n\nTerminal 1\n\ndocker run \\\n    -it --rm \\\n    -p 9000:8080 \\\n    -e AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID} \\\n    -e AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY} \\\n    -e RUN_ID=5651db4644334361b10296c51ba3af3e \\\n    -e S3_BUCKET_NAME=mlops-project-sales-forecast-bucket \\\n    lambda-app:v1\n\nTo test run the command in terminal 2,\n\n\ntest_docker_fn_docker.py in Terminal 2\n\npython test_docker_fn_docker.py\n\nWe will get a prediction as we have done in the previous post."
  },
  {
    "objectID": "posts/2023-06-04-api-lambda-ecr/index.html#uploading-container-image-to-ecr",
    "href": "posts/2023-06-04-api-lambda-ecr/index.html#uploading-container-image-to-ecr",
    "title": "Building Data Pipelines - Part 2 - AWS Cloud",
    "section": "Uploading container image to ECR",
    "text": "Uploading container image to ECR\nFor this section we need awscli package. If don’t have I recommend installing it. sudo apt install awscli. AWS-CLI allows for swifter creation of resources without overly relying on the console.\nWe need our ACCOUNT_ID. It can be found in the AWS console.\n\n\nTerminal\n\n1docker build -t lambda-app:v1 .\n2export ACCOUNT_ID=xxxx\naws ecr create-repository --repository-name lambda-images\ndocker tag lambda-app:v1 ${ACCOUNT_ID}.dkr.ecr.us-east-1.amazonaws.com/lambda-images:app\n3$(aws ecr get-login --no-include-email)\ndocker push ${ACCOUNT_ID}.dkr.ecr.us-east-1.amazonaws.com/lambda-images:app\n\n\n1\n\nOptional as we already built the image.\n\n2\n\nReplace with your account ID.\n\n3\n\nRemember we supplied aws access key and secret before. The session borrows them for ECR login. If it is a new session, those variables have to be given again.\n\n\n  In the screenshots we can see that our container image is uploaded to the registry."
  },
  {
    "objectID": "posts/2023-06-04-api-lambda-ecr/index.html#setup-aws-lambda-function",
    "href": "posts/2023-06-04-api-lambda-ecr/index.html#setup-aws-lambda-function",
    "title": "Building Data Pipelines - Part 2 - AWS Cloud",
    "section": "Setup AWS Lambda function",
    "text": "Setup AWS Lambda function\nThis part is a bit tedious as it is not as straight forward as running commands. I’d like to point out to a detailed guide which has all the steps to create a AWS Lambda function, attach policies, configure and supply environment variable. I urge you to visit that link. Including them here will make it too long, full of screenshots and frankly boring.\n\nTesting the Lambda function\nIn the Test tab, create a test event with our good old sample JSON object. {\"find\":{\"date1\":\"2017-08-26\",\"store_nbr\":20}}.   Hitting the Test button should give us a prediction. as seen in the second image.\nSo our function is able to access S3 artifact bucket to download and give out predictions. Meaning our attached policies are correct. Testing once in the console is fine but the customer wouldn’t want to do that. We need something that triggers the lambda function - we need an API Gateway."
  },
  {
    "objectID": "posts/2023-06-04-api-lambda-ecr/index.html#aws-api-gateway",
    "href": "posts/2023-06-04-api-lambda-ecr/index.html#aws-api-gateway",
    "title": "Building Data Pipelines - Part 2 - AWS Cloud",
    "section": "AWS API Gateway",
    "text": "AWS API Gateway\nA front-end developer, for example, can create a button that calls on the Lambda function. From customer’s viewpoint it is as simple as a click of a button. For us though, it is much more than that.\n\nCreate Rest API, test and deploy it\nAgain I advise you to refer my other guide to create an endpoint. This endpoint is same as the Flask one we encountered in the last posts.\nUpon deploying we get an invoke url. This url is without an endpoint. \nWe need to append predict-sales, our resource endpoint, to the url. The final url would look like https://eecweeeeeg.execute-api.us-east-1.amazonaws.com/stg-lambda-app-for-blog/predict-sales\nUse the following JSON object to test it out in an API platform client such as Thunder Client in VSCode, Postman etc.\n{\"find\": {\"date1\": \"2017-08-26\", \"store_nbr\": 20}}"
  },
  {
    "objectID": "posts/2023-06-04-api-lambda-ecr/index.html#aws-dynamodb",
    "href": "posts/2023-06-04-api-lambda-ecr/index.html#aws-dynamodb",
    "title": "Building Data Pipelines - Part 2 - AWS Cloud",
    "section": "AWS DynamoDB",
    "text": "AWS DynamoDB\nWe want to store these predicted outputs to a DB. With the help of console, create a table sales_predictions. Give store_id as partition_key and item_id as sort_key. Rest of the setting can be default.\nThen add policy that give Lambda permission to write objects into DB. Refer my other notes"
  },
  {
    "objectID": "posts/2023-06-04-api-lambda-ecr/index.html#modify-aws-lambda",
    "href": "posts/2023-06-04-api-lambda-ecr/index.html#modify-aws-lambda",
    "title": "Building Data Pipelines - Part 2 - AWS Cloud",
    "section": "Modify AWS Lambda",
    "text": "Modify AWS Lambda\n\n\nlambda_function.py\n\nimport os\n\nimport mlflow\nimport pandas as pd\nimport boto3\nimport json\n\nRUN_ID = os.getenv(\"RUN_ID\")  # \"5651db4644334361b10296c51ba3af3e\"\nS3_BUCKET_NAME = os.getenv(\n    \"S3_BUCKET_NAME\"\n)  # \"mlops-project-sales-forecast-bucket\"\nEXPERIMENT_ID = 1\nFILE_ADDRESS = \"artifacts/predictions/lgb_preds.parquet\"\npred_s3_location = (\n    f\"s3://{S3_BUCKET_NAME}/{EXPERIMENT_ID}/{RUN_ID}/{FILE_ADDRESS}\"\n)\n\n# Initialize the SNS client object outside of the handler\n1dynamodb = boto3.resource('dynamodb')\n\ndef read_parquet_files(filename: str):\n    \"\"\"\n    Read parquet file format for given filename and returns the contents\n    \"\"\"\n    df = pd.read_parquet(filename, engine=\"pyarrow\")\n    return df\n\n\nif os.path.exists(\"lgb_preds.parquet\"):\n    df_test_preds = read_parquet_files(\"lgb_preds.parquet\")\nelse:\n    s3_file = mlflow.artifacts.download_artifacts(\n        artifact_uri=pred_s3_location, dst_path=\"/tmp\"\n    )  # /tmp is added as lambda gives write access only to that folder. Otherwise use \"./\" .\n    df_test_preds = read_parquet_files(s3_file)\n\n\ndf_items = read_parquet_files(\"items.parquet\")\n\n\ndef predict(find, item_idx: int):\n    \"\"\"\n    Takes the json inputs, processes it and outputs the unit sales\n    \"\"\"\n    try:\n        idx = pd.IndexSlice\n        # df_items.sample(1).index[0]\n        x = df_test_preds.loc[idx[find[\"store_nbr\"], item_idx, find[\"date1\"]]][\n            \"unit_sales\"\n        ]\n    except KeyError:\n        print(\"This item is not present this store. Try some other item\")\n        return -0.0\n    else:\n        return float(round(x, 2))\n\n\ndef lambda_handler(event, context=None) -&gt; dict:\n    \"\"\"\n    lambda handler for predict method\n    \"\"\"\n\n    find = event[\"find\"]\n    item = df_items.sample(1)\n    item_idx, item_family = item.index[0], item[\"family\"].values[0]\n    pred_unit_sales = predict(find, item_idx)\n\n    result = {\n        \"store_id\": find[\"store_nbr\"],\n        \"item_id\": int(item_idx),\n        \"family\": item_family,\n        \"prediction_date\": find[\"date1\"],\n        \"unit_sales\": str(pred_unit_sales),\n    }\n\n2    table = dynamodb.Table(\"sales_preds_for_blog\")\n    table.put_item(\n3        Item = result\n    )\n    return {\n      'statusCode': 200,\n      'body': 'successfully created item!',\n      }\n\n\n1\n\nCreate dynamodb resource\n\n2\n\nAccess already created table sales_preds_for_blog\n\n3\n\nInsert item into the table\n\n\nRebuild the docker image, retag it, publish it to the ECR repo. Then in AWS Lambda, use deploy new image to select the latest build. Test with sample input."
  },
  {
    "objectID": "posts/2023-06-11-terraform-aws/index.html",
    "href": "posts/2023-06-11-terraform-aws/index.html",
    "title": "Building Data Pipelines - Part 3 - Terraform",
    "section": "",
    "text": "In part 2 we saw how to migrate to AWS cloud. In bigger projects where so many resources are present, it is near impossible to remember and manage them using the console. Terraform provides that solution. It allows to track resources just like code. We saw about the use of Terraform in my introduction post on Terraform.\nIn this part, we will take the data pipeline from part 2 and use Terraform to manage the infrastructure."
  },
  {
    "objectID": "posts/2023-06-11-terraform-aws/index.html#module-block-1---ecr",
    "href": "posts/2023-06-11-terraform-aws/index.html#module-block-1---ecr",
    "title": "Building Data Pipelines - Part 3 - Terraform",
    "section": "Module block 1 - ECR",
    "text": "Module block 1 - ECR\n\n\ninfrastructure/modules/ecr/main.tf\n\nmodule \"ecr_image\" {\n1  source = \"./modules/ecr\"\n  ecr_repo_name = \"${var.ecr_repo_name}_${var.project_id}\"\n  account_id = local.account_id\n2  lambda_function_local_path = var.lambda_function_local_path\n3  docker_image_local_path = var.docker_image_local_path\n}\n\n\n1\n\npath to the ecr module\n\n2\n\npath of the lambda_function.py\n\n3\n\npath of dockerfile to create docker image\n\n\nInside the module ecr, we create a main.tf and varible.tf. Variables passed into the module and also ones newly used inside it have to be defined inside variable.tf.\n\nBuilding the docker container image and uploading to ECR\nUsually docker container image building is part of the CI/CD pipeline but since the lambda function requires us to have the image, we build it locally and upload using Terraform’s local-exec provisioner. However, Terraform advises caution with the use of provisioners. Read more on that here.\n\n\ninfrastructure/modules/ecr/main.tf\n\n1resource \"null_resource\" \"ecr_image\" {\n  triggers = {\n    \"python_file\" = md5(file(var.lambda_function_local_path))\n    \"docker_file\" = md5(file(var.docker_image_local_path))\n  }\n\n2  provisioner \"local-exec\" {\n    command = &lt;&lt;EOF\n            aws ecr get-login-password --region ${var.ecr_region} | docker login --username AWS --password-stdin ${var.account_id}.dkr.ecr.${var.ecr_region}.amazonaws.com\n            cd ${path.module}/../..\n            docker build -t ${aws_ecr_repository.repo.repository_url}:${var.ecr_image_tag} .\n            docker push ${aws_ecr_repository.repo.repository_url}:${var.ecr_image_tag}\n        EOF\n  }\n}\n\n\n1\n\nA null_resource block is a feature of Terraform’s. With a help of triggers meta-argument, we can observe any change to lambda_function or dockerfile.\n\n2\n\nWhen there is a change, a trigger condition is active and local-exec is executed. The image is built and uploaded."
  },
  {
    "objectID": "posts/2023-06-11-terraform-aws/index.html#module-block-2---lambda-function",
    "href": "posts/2023-06-11-terraform-aws/index.html#module-block-2---lambda-function",
    "title": "Building Data Pipelines - Part 3 - Terraform",
    "section": "Module block 2 - Lambda Function",
    "text": "Module block 2 - Lambda Function\nOur ECR image is ready to be used as source for Lambda Function. With a depends_on meta-argument, this condition is ensured.\nOur lambda_function inside the container image requires three environment variables: artifact_bucket, run_id, dbtable_name. These variables are passed into the lamda function module as arguments.\n\n\ninfrastructure/modules/lambda/main.tf\n\nresource \"aws_lambda_function\" \"lambda_function\" {\n  function_name = var.lambda_function_name\n  description = \"Sales Forecast lambda function from ECR image from TF\"\n  image_uri = var.image_uri \n  package_type = \"Image\"\n1  role = aws_iam_role.lambda_exec.arn\n  tracing_config {\n    mode = \"Active\"\n  }\n  memory_size = 1024\n  timeout = 30\n2  environment {\n    variables = {\n      S3_BUCKET_NAME = var.artifact_bucket \n      RUN_ID = var.mlflow_run_id \n      DBTABLE_NAME = var.dbtable_name\n    }\n  }\n}\n\n3resource \"aws_cloudwatch_log_group\" \"lambda_log_group\" {\n  name = \"/aws/lambda/${aws_lambda_function.lambda_function.function_name}\"\n  retention_in_days = 30\n}\n\n\n1\n\nIAM Role attached to the Lambda function.\n\n2\n\nEnvironment variables for the lambda function to predict sales.\n\n3\n\nSetting Cloudwatch logs retention period.\n\n\n\nIAM Roles and Polices\nThe AWS Lambda function is the business layer of our app. It plays a crucial role in predicting the sales output. Therefore it needs access to retrieve the trained model from the artifact_bucket and store the predicted results in the DynamoDB table. These operations are only possible if we give AWS Lambda function permission.\nAn IAM role lambda_exec is created.\n\n\nIAM Role-&gt;lambda_exec\n\nresource \"aws_iam_role\" \"lambda_exec\" {\n    name = \"iam_${var.lambda_function_name}\"\n    assume_role_policy = jsonencode({\n        \"Version\": \"2012-10-17\",\n        \"Statement\": [{\n                \"Action\": \"sts:AssumeRole\",\n                \"Principal\": {\n1                    \"Service\": \"lambda.amazonaws.com\"\n                },\n                \"Effect\": \"Allow\",\n                \"Sid\":\"\"\n          }]\n  }) \n}\n\n\n1\n\nRole just for lambda function service\n\n\nTo this roles several policies are added. We need three policies for - Basic lambda execution, Access S3 artifact bucket, Put items into DynamoDB table. These three policies are defined and attached using aws_iam_role_policy_attachment resource block."
  },
  {
    "objectID": "posts/2023-06-11-terraform-aws/index.html#module-block-3---dynamodb",
    "href": "posts/2023-06-11-terraform-aws/index.html#module-block-3---dynamodb",
    "title": "Building Data Pipelines - Part 3 - Terraform",
    "section": "Module block 3 - Dynamodb",
    "text": "Module block 3 - Dynamodb\nSimilar to the previous two blocks, dynamodb module is called with necessary arguments.\nresource \"aws_dynamodb_table\" \"sales_preds_table_fromtf\" {\n  name = var.dynamodb_tablename\n1  billing_mode = \"PAY_PER_REQUEST\"\n  table_class  = \"STANDARD_INFREQUENT_ACCESS\"\n2  hash_key = var.dynamodb_hashkey\n3  range_key = var.dynamodb_rangekey\n  \n  attribute {\n    name = var.dynamodb_hash_key\n    type = \"N\"\n  }\n\n  attribute {\n    name = var.dynamodb_range_key\n    type = \"N\"\n  }\n}\n\n1\n\nBilling mode is set as “On-Demand”\n\n2\n\nHash key is the Partition Key\n\n3\n\nRange Key is the Sort Key"
  },
  {
    "objectID": "posts/2023-06-11-terraform-aws/index.html#module-block-4---api-gateway",
    "href": "posts/2023-06-11-terraform-aws/index.html#module-block-4---api-gateway",
    "title": "Building Data Pipelines - Part 3 - Terraform",
    "section": "Module block 4 - API Gateway",
    "text": "Module block 4 - API Gateway\nAPI Gateway management with Terraform follows all the steps we need manually in the console. The steps are -\n\nCreate rest api with resource aws_api_gateway_rest_api\nCreate gateway resource with aws_api_gateway_resource and give the endpoint path as predict_sales.\nDefine the gateway method with rest_api_post_method as POST.\nSetup POST method’s reponse upon succesfull execution with a code 200.\nIntegrate and deploy the gateway with aws_api_gateway_integration and aws_api_gateway_deployment respectively.\nStage the deployment with rest_api_stage and get the invoke_url. For this we can use the output block.\n\noutput \"rest_api_url\" {\n  value = \"${aws_api_gateway_deployment.sales_pred_deployment.invoke_url}${aws_api_gateway_stage.rest_api_stage.stage_name}${aws_api_gateway_resource.rest_api_predict_resource.path}\"\n}\n\nDefine IAM policy for rest api with aws_api_gateway_rest_api_policy and give access to API gateway to invoke the lambda function with aws_lambda_permission.\n\n\nVariables\nIn Terraform we can pass variable values in many ways. One of those ways is through .tfvars file. This file is exclusively for variables. These variable files are extremely helpful when we need different variable names for development, staging and production. The syntax for supplying the file is like so: -var-file vars/stg.tfvars."
  },
  {
    "objectID": "posts/2023-06-11-terraform-aws/index.html#initialise-backend",
    "href": "posts/2023-06-11-terraform-aws/index.html#initialise-backend",
    "title": "Building Data Pipelines - Part 3 - Terraform",
    "section": "Initialise backend",
    "text": "Initialise backend\nterraform init initialises the Terraform backend, checks where the state file has to be saved and if it is remote, availability of the bucket is validated, installs all the provider plugins."
  },
  {
    "objectID": "posts/2023-06-11-terraform-aws/index.html#plan-and-apply-changes",
    "href": "posts/2023-06-11-terraform-aws/index.html#plan-and-apply-changes",
    "title": "Building Data Pipelines - Part 3 - Terraform",
    "section": "Plan and Apply Changes",
    "text": "Plan and Apply Changes\nWith terraform plan -var-file vars/stg.tfvars we can see what new resources be created/changed/destroyed. This gives as a plan and confirmation of our setup.\nterraform apply -var-file vars/stg.tfvars will apply our configurations. At the end the rest_api_url will be displayed. \nTake that, put it in an api client and supplying our sample JSON input {\"find\": {\"date1\": \"2017-08-28\", \"store_nbr\": 19}}. You should see the status code as 200 and the body containing the predictions with a confirmation saying the item has been successfully created.\n\nWe can confirm it by going to Dynamodb console and checking the items in the table"
  },
  {
    "objectID": "posts/2023-06-11-terraform-aws/index.html#destroying-resources",
    "href": "posts/2023-06-11-terraform-aws/index.html#destroying-resources",
    "title": "Building Data Pipelines - Part 3 - Terraform",
    "section": "Destroying resources",
    "text": "Destroying resources\nUpon completion of the task it is always good practice to destroy the resources to avoid incurring unnecessary costs.\nUse terraform destroy -var-file vars/stg.tfvars to destroy the resources and leave it as we started. Remember in real production environment, destroy command should never be used. Instead delete the resources that are unnecesary and run apply command again."
  },
  {
    "objectID": "posts/2022-09-24-using-json-normalize/index.html",
    "href": "posts/2022-09-24-using-json-normalize/index.html",
    "title": "Using json_normalize Pandas function",
    "section": "",
    "text": "Javascript Object Notation(JSON) is a widely used format for storing and exchanging data. Coming from the relational database, it could be difficult to understand NoSQL databases that use JSON to store data and similarly REST API’s response. JSON is also used in storing football event data. It allows easy addition of features in the future.\nThough JSON format allows for easier exchange of data, for analysis, a tabular form would be appropriate. A JSON structure can be of two forms: a JSON object and list of JSON objects. Since our programming language of choice is Python, those structures can be somewhat called as a dictionary object or list of dicts.\n1\nImporting pandas library,\nimport pandas as pd"
  },
  {
    "objectID": "posts/2022-09-24-using-json-normalize/index.html#flattening-a-simple-json",
    "href": "posts/2022-09-24-using-json-normalize/index.html#flattening-a-simple-json",
    "title": "Using json_normalize Pandas function",
    "section": "1 Flattening a simple JSON",
    "text": "1 Flattening a simple JSON\nA dict\nLet us consider a simple dictionary: 3 keys and their respective values.\n\nviv = {\n    \"player_id\" : 15623, \n    \"player_name\" : \"Vivianne Miedema\", \n    \"jersey_number\" : 11}\nviv\n\n{'player_id': 15623, 'player_name': 'Vivianne Miedema', 'jersey_number': 11}\n\n\nWe use the json_normalize API2 to flatten a JSON dict.\n\ndf = pd.json_normalize(viv);df\n\n\n\n\n\n\n\n\nplayer_id\nplayer_name\njersey_number\n\n\n\n\n0\n15623\nVivianne Miedema\n11\n\n\n\n\n\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1 entries, 0 to 0\nData columns (total 3 columns):\n #   Column         Non-Null Count  Dtype \n---  ------         --------------  ----- \n 0   player_id      1 non-null      int64 \n 1   player_name    1 non-null      object\n 2   jersey_number  1 non-null      int64 \ndtypes: int64(2), object(1)\nmemory usage: 152.0+ bytes\n\n\n\nSide Note: If the data contains something that is not compatible with python, in this case a null variable, there are two choices:\n\n\n\nChange null to None\nPass the data through json.loads function\n\n\nChange null to None\n\nnull = None\nviv1 = { \"player_id\" : 15623, \"player_name\" : \"Vivianne Miedema\", \"jersey_number\" : 11, \"player_nickname\" : null}\nviv1\n\n{'player_id': 15623,\n 'player_name': 'Vivianne Miedema',\n 'jersey_number': 11,\n 'player_nickname': None}\n\n\nMake data as string and pass to json.loads\n\nimport json\nviv1 = '{ \"player_id\" : 15623, \"player_name\" : \"Vivianne Miedema\", \"jersey_number\" : 11, \"player_nickname\" : null}'\nviv1 = json.loads(viv1)\nviv1\n\n{'player_id': 15623,\n 'player_name': 'Vivianne Miedema',\n 'jersey_number': 11,\n 'player_nickname': None}\n\n\n\n1.1 A list of dicts\n\nplayer_list = [\n    { \"player_id\" : 15623, \"player_name\" : \"Vivianne Miedema\", \"jersey_number\" : 11, \"player_nickname\" : null },\n    { \"player_id\" : 10658, \"player_name\" : \"Danielle van de Donk\", \"jersey_number\" : 7, \"player_nickname\" : null }\n]\npd.json_normalize(player_list)\n\n\n\n\n\n\n\n\nplayer_id\nplayer_name\njersey_number\nplayer_nickname\n\n\n\n\n0\n15623\nVivianne Miedema\n11\nNone\n\n\n1\n10658\nDanielle van de Donk\n7\nNone\n\n\n\n\n\n\n\nWe have the JSON list of dicts in a tabular form. All the keys become columns and their values as entries.\nWhen we flattern a list with a key-value pair missing for an entry, instead of an error, NaN(not a number) is stored.\n\nplayer_list = [\n    { \"player_id\" : 15623, \"player_name\" : \"Vivianne Miedema\", \"jersey_number\" : 11, \"player_nickname\" : null },\n    { \"player_id\" : 10658, \"player_name\" : \"Danielle van de Donk\"}\n]\npd.json_normalize(player_list)\n\n\n\n\n\n\n\n\nplayer_id\nplayer_name\njersey_number\nplayer_nickname\n\n\n\n\n0\n15623\nVivianne Miedema\n11.0\nNaN\n\n\n1\n10658\nDanielle van de Donk\nNaN\nNaN\n\n\n\n\n\n\n\nNote: See how player_nickname when not specified also turns to NaN from None."
  },
  {
    "objectID": "posts/2022-09-24-using-json-normalize/index.html#flattening-a-multi-level-json",
    "href": "posts/2022-09-24-using-json-normalize/index.html#flattening-a-multi-level-json",
    "title": "Using json_normalize Pandas function",
    "section": "2 Flattening a multi-level JSON",
    "text": "2 Flattening a multi-level JSON\n\n2.1 A simple dict\n\nat_kick0ff = {\n  \"id\":\"d712fb93-c464-4621-98ba-f2bdcd5641db\",\n  \"timestamp\":\"00:00:00.000\",\n  \"duration\":0.0,\n  \"lineup\":{\n      \"player\":{\n        \"id\":15623,\n        \"name\":\"Vivianne Miedema\"\n      },\n      \"position\":{\n        \"id\":23,\n        \"name\":\"Center Forward\"\n      },\n      \"jersey_number\":11\n    }\n}\nat_kick0ff\n\n{'id': 'd712fb93-c464-4621-98ba-f2bdcd5641db',\n 'timestamp': '00:00:00.000',\n 'duration': 0.0,\n 'lineup': {'player': {'id': 15623, 'name': 'Vivianne Miedema'},\n  'position': {'id': 23, 'name': 'Center Forward'},\n  'jersey_number': 11}}\n\n\n\npd.json_normalize(at_kick0ff)\n\n\n\n\n\n\n\n\nid\ntimestamp\nduration\nlineup.player.id\nlineup.player.name\nlineup.position.id\nlineup.position.name\nlineup.jersey_number\n\n\n\n\n0\nd712fb93-c464-4621-98ba-f2bdcd5641db\n00:00:00.000\n0.0\n15623\nVivianne Miedema\n23\nCenter Forward\n11\n\n\n\n\n\n\n\nYou can see that lineup dictionary key’s nested key-value pairs have been expanded into individual columns. If you feel that is unnecessary, we can restrict expansion by using max_level argument. With max_level=1, the flattening goes one level deeper.\n\npd.json_normalize(at_kick0ff, max_level=1)\n\n\n\n\n\n\n\n\nid\ntimestamp\nduration\nlineup.player\nlineup.position\nlineup.jersey_number\n\n\n\n\n0\nd712fb93-c464-4621-98ba-f2bdcd5641db\n00:00:00.000\n0.0\n{'id': 15623, 'name': 'Vivianne Miedema'}\n{'id': 23, 'name': 'Center Forward'}\n11\n\n\n\n\n\n\n\n\n\n2.2 A list of dicts\n\nfirst_pass = [\n  {\n    \"id\":\"15758edb-58cd-49c4-a817-d2ef48ba3bcf\",\n    \"timestamp\":\"00:00:00.504\",\n    \"type\":{\n      \"id\":30,\n      \"name\":\"Pass\"\n    },\n    \"play_pattern\":{\n      \"id\":9,\n      \"name\":\"From Kick Off\"\n    },\n    \"player\":{\n      \"id\":15623,\n      \"name\":\"Vivianne Miedema\"\n    },\n    \"pass\":{\n      \"recipient\":{\n        \"id\":10666,\n        \"name\":\"Dominique Johanna Anna Bloodworth\"\n      },\n      \"length\":25.455845,\n      \"angle\":-2.3561945,\n      \"height\":{\n        \"id\":1,\n        \"name\":\"Ground Pass\"\n      },\n      \"end_location\":[\n        42.0,\n        22.0\n      ]\n    }\n  }, {\n  \"id\" : \"ab5674a4-e824-4143-9f6f-3f1645557413\",\n  \"timestamp\" : \"00:00:04.201\",\n  \"type\" : {\n    \"id\" : 30,\n    \"name\" : \"Pass\"\n  },\n  \"play_pattern\" : {\n    \"id\" : 9,\n    \"name\" : \"From Kick Off\"\n  },\n  \"player\" : {\n    \"id\" : 10666,\n    \"name\" : \"Dominique Johanna Anna Bloodworth\"\n  },\n  \"location\" : [ 45.0, 29.0 ],\n  \"duration\" : 1.795201,\n  \"pass\" : {\n    \"length\" : 51.62364,\n    \"angle\" : 0.55038595,\n    \"height\" : {\n      \"id\" : 3,\n      \"name\" : \"High Pass\"\n    },\n    \"end_location\" : [ 89.0, 56.0 ]\n  }\n}\n]\n    \npd.json_normalize(first_pass)\n\n\n\n\n\n\n\n\nid\ntimestamp\ntype.id\ntype.name\nplay_pattern.id\nplay_pattern.name\nplayer.id\nplayer.name\npass.recipient.id\npass.recipient.name\npass.length\npass.angle\npass.height.id\npass.height.name\npass.end_location\nlocation\nduration\n\n\n\n\n0\n15758edb-58cd-49c4-a817-d2ef48ba3bcf\n00:00:00.504\n30\nPass\n9\nFrom Kick Off\n15623\nVivianne Miedema\n10666.0\nDominique Johanna Anna Bloodworth\n25.455845\n-2.356194\n1\nGround Pass\n[42.0, 22.0]\nNaN\nNaN\n\n\n1\nab5674a4-e824-4143-9f6f-3f1645557413\n00:00:04.201\n30\nPass\n9\nFrom Kick Off\n10666\nDominique Johanna Anna Bloodworth\nNaN\nNaN\n51.623640\n0.550386\n3\nHigh Pass\n[89.0, 56.0]\n[45.0, 29.0]\n1.795201\n\n\n\n\n\n\n\nLimiting the levels…\n\npd.json_normalize(first_pass, max_level=0)\n\n\n\n\n\n\n\n\nid\ntimestamp\ntype\nplay_pattern\nplayer\npass\nlocation\nduration\n\n\n\n\n0\n15758edb-58cd-49c4-a817-d2ef48ba3bcf\n00:00:00.504\n{'id': 30, 'name': 'Pass'}\n{'id': 9, 'name': 'From Kick Off'}\n{'id': 15623, 'name': 'Vivianne Miedema'}\n{'recipient': {'id': 10666, 'name': 'Dominique...\nNaN\nNaN\n\n\n1\nab5674a4-e824-4143-9f6f-3f1645557413\n00:00:04.201\n{'id': 30, 'name': 'Pass'}\n{'id': 9, 'name': 'From Kick Off'}\n{'id': 10666, 'name': 'Dominique Johanna Anna ...\n{'length': 51.62364, 'angle': 0.55038595, 'hei...\n[45.0, 29.0]\n1.795201"
  },
  {
    "objectID": "posts/2022-09-24-using-json-normalize/index.html#flattening-a-json-nested-list",
    "href": "posts/2022-09-24-using-json-normalize/index.html#flattening-a-json-nested-list",
    "title": "Using json_normalize Pandas function",
    "section": "3 Flattening a JSON nested list",
    "text": "3 Flattening a JSON nested list\n\n3.1 A simple dict\nFor this case, let us consider a simpler example than of football event data. The key info has list of dictionaries inside its structure. We call it nested dict.\n\nawfc = {\n    'team': 'AWFC',\n    'location': 'London',\n    'ranking': 1,\n    'info': {\n        'manager': 'Joe',\n        'contacts': {\n          'email': {\n              'coaching': 'joe@afc.com',\n              'general': 'info@afc.com'\n          },\n          'tel': '123456789',\n      }\n    },\n    'players': [\n      { 'name': 'Viv' },\n      { 'name': 'DvD' },\n      { 'name': 'Kim' }\n    ],\n};awfc\n\n{'team': 'AWFC',\n 'location': 'London',\n 'ranking': 1,\n 'info': {'manager': 'Joe',\n  'contacts': {'email': {'coaching': 'joe@afc.com', 'general': 'info@afc.com'},\n   'tel': '123456789'}},\n 'players': [{'name': 'Viv'}, {'name': 'DvD'}, {'name': 'Kim'}]}\n\n\nThe players column has a list of dicts. So, we can flatten that column using record_path argument.\n\npd.json_normalize(awfc, record_path=['players'])\n\n\n\n\n\n\n\n\nname\n\n\n\n\n0\nViv\n\n\n1\nDvD\n\n\n2\nKim\n\n\n\n\n\n\n\nBut, making a separate table with no reference id has no meaning. To prevent that we can append revelant columns to the new table using meta argument. Here we want their team and Telephone number. The tel key lies within info-&gt;contacts-&gt;tel. So, we need provide that path like so ['info', 'contacts', 'tel'].\n\npd.json_normalize(awfc, record_path=['players'], meta=['team',['info', 'contacts', 'tel']])\n\n\n\n\n\n\n\n\nname\nteam\ninfo.contacts.tel\n\n\n\n\n0\nViv\nAWFC\n123456789\n\n\n1\nDvD\nAWFC\n123456789\n\n\n2\nKim\nAWFC\n123456789\n\n\n\n\n\n\n\nThe order in which those paths are mentioned, the order in which those columns are appended.\n\npd.json_normalize(awfc, record_path=['players'], meta=['team',['info', 'contacts', 'tel'],['info', 'manager']])\n\n\n\n\n\n\n\n\nname\nteam\ninfo.contacts.tel\ninfo.manager\n\n\n\n\n0\nViv\nAWFC\n123456789\nJoe\n\n\n1\nDvD\nAWFC\n123456789\nJoe\n\n\n2\nKim\nAWFC\n123456789\nJoe\n\n\n\n\n\n\n\n\n\n3.2 A list of dicts\n\njson_list = [\n    { \n        'team': 'arsenal', \n        'colour': 'red-white',\n        'info': {\n            'staff': { \n                'physio': 'xxxx', \n                'doctor': 'yyyy' \n            }\n        },\n        'players': [\n            { \n                'name': 'Viv', \n                'sex': 'F', \n                'stats': { 'goals': 101, 'assists': 40 } \n            },\n            { \n                'name': 'Beth', \n                'sex': 'F', \n                'stats': { 'goals': 60, 'assists': 25 } \n            },\n        ]\n    },\n    { \n        'team': 'city', \n        'colour': 'blue',\n        'info': {\n            'staff': { \n                'physio': 'aaaa', \n                'doctor': 'bbbb' \n            }\n        },\n        'players': [\n            { 'name': 'Steph', 'sex': 'F' },\n            { 'name': 'Lucy', 'sex': 'F' },\n        ]\n    },\n]\n\npd.json_normalize(json_list)\n\n\n\n\n\n\n\n\nteam\ncolour\nplayers\ninfo.staff.physio\ninfo.staff.doctor\n\n\n\n\n0\narsenal\nred-white\n[{'name': 'Viv', 'sex': 'F', 'stats': {'goals'...\nxxxx\nyyyy\n\n\n1\ncity\nblue\n[{'name': 'Steph', 'sex': 'F'}, {'name': 'Lucy...\naaaa\nbbbb\n\n\n\n\n\n\n\n\npd.json_normalize(json_list, record_path =['players'])\n\n\n\n\n\n\n\n\nname\nsex\nstats.goals\nstats.assists\n\n\n\n\n0\nViv\nF\n101.0\n40.0\n\n\n1\nBeth\nF\n60.0\n25.0\n\n\n2\nSteph\nF\nNaN\nNaN\n\n\n3\nLucy\nF\nNaN\nNaN\n\n\n\n\n\n\n\nHow about we now append the players’ team, colour, and their physio.\n\npd.json_normalize(\n    json_list, \n    record_path =['players'], \n    meta=['team', 'colour', ['info', 'staff', 'physio']]\n)\n\n\n\n\n\n\n\n\nname\nsex\nstats.goals\nstats.assists\nteam\ncolour\ninfo.staff.physio\n\n\n\n\n0\nViv\nF\n101.0\n40.0\narsenal\nred-white\nxxxx\n\n\n1\nBeth\nF\n60.0\n25.0\narsenal\nred-white\nxxxx\n\n\n2\nSteph\nF\nNaN\nNaN\ncity\nblue\naaaa\n\n\n3\nLucy\nF\nNaN\nNaN\ncity\nblue\naaaa"
  },
  {
    "objectID": "posts/2022-09-24-using-json-normalize/index.html#ignoring-key-errors",
    "href": "posts/2022-09-24-using-json-normalize/index.html#ignoring-key-errors",
    "title": "Using json_normalize Pandas function",
    "section": "4 Ignoring key errors",
    "text": "4 Ignoring key errors\n\njson_list = [\n    { \n        'team': 'arsenal', \n        'colour': 'red-white',\n        'info': {\n            'staff': { \n                'physio': 'xxxx', \n                'doctor': 'yyyy' \n            }\n        },\n        'players': [\n            { \n                'name': 'Viv', \n                'sex': 'F', \n                'stats': { 'goals': 101, 'assists': 40 } \n            },\n            { \n                'name': 'Beth', \n                'sex': 'F', \n                'stats': { 'goals': 60, 'assists': 25 } \n            },\n        ]\n    },\n    { \n        'team': 'city', \n        'colour': 'blue',\n        'info': {\n            'staff': { \n                'doctor': 'bbbb' \n            }\n        },\n        'players': [\n            { 'name': 'Steph', 'sex': 'F' },\n            { 'name': 'Lucy', 'sex': 'F' },\n        ]\n    },\n]\n\nNotice that the key physio is missing from the entry team=city. What happens if we try to access physio key inside meta?\n\npd.json_normalize(\n    json_list, \n    record_path =['players'], \n    meta=['team', 'colour', ['info', 'staff', 'physio']],\n)\n\nKeyError: \"Key 'physio' not found. To replace missing values of 'physio' with np.nan, pass in errors='ignore'\"\n\n\nHow come stats.goals and stats.assists didn’t generate an error but that above does? Because, the meta argument expects values to be present for listed keys in meta by default. We can ignore those errors(as suggested) using errors='ignore'\n\npd.json_normalize(\n    json_list, \n    record_path =['players'], \n    meta=['team', 'colour', ['info', 'staff', 'physio']],\n    errors='ignore'\n)\n\n\n\n\n\n\n\n\nname\nsex\nstats.goals\nstats.assists\nteam\ncolour\ninfo.staff.physio\n\n\n\n\n0\nViv\nF\n101.0\n40.0\narsenal\nred-white\nxxxx\n\n\n1\nBeth\nF\n60.0\n25.0\narsenal\nred-white\nxxxx\n\n\n2\nSteph\nF\nNaN\nNaN\ncity\nblue\nNaN\n\n\n3\nLucy\nF\nNaN\nNaN\ncity\nblue\nNaN"
  },
  {
    "objectID": "posts/2022-09-24-using-json-normalize/index.html#custom-separator-sep",
    "href": "posts/2022-09-24-using-json-normalize/index.html#custom-separator-sep",
    "title": "Using json_normalize Pandas function",
    "section": "5 Custom separator sep",
    "text": "5 Custom separator sep\nWe notice that by default pandas uses . to indicate the direction of the path. We can change that using the sep argument.\n\nTip: Usually an underscore is used instead of .\n\n\njson_list = [\n    { \n        'team': 'arsenal', \n        'colour': 'red-white',\n        'info': {\n            'staff': { \n                'physio': 'xxxx', \n                'doctor': 'yyyy' \n            }\n        },\n        'players': [\n            { \n                'name': 'Viv', \n                'sex': 'F', \n                'stats': { 'goals': 101, 'assists': 40 } \n            },\n            { \n                'name': 'Beth', \n                'sex': 'F', \n                'stats': { 'goals': 60, 'assists': 25 } \n            },\n        ]\n    },\n    { \n        'team': 'city', \n        'colour': 'blue',\n        'info': {\n            'staff': { \n                'physio': 'aaaa', \n                'doctor': 'bbbb' \n            }\n        },\n        'players': [\n            { 'name': 'Steph', 'sex': 'F' },\n            { 'name': 'Lucy', 'sex': 'F' },\n        ]\n    },\n]\n\n\npd.json_normalize(\n    json_list, \n    record_path =['players'], \n    meta=['team', 'colour', ['info', 'staff', 'physio']],\n    sep='-&gt;'\n)\n\n\n\n\n\n\n\n\nname\nsex\nstats-&gt;goals\nstats-&gt;assists\nteam\ncolour\ninfo-&gt;staff-&gt;physio\n\n\n\n\n0\nViv\nF\n101.0\n40.0\narsenal\nred-white\nxxxx\n\n\n1\nBeth\nF\n60.0\n25.0\narsenal\nred-white\nxxxx\n\n\n2\nSteph\nF\nNaN\nNaN\ncity\nblue\naaaa\n\n\n3\nLucy\nF\nNaN\nNaN\ncity\nblue\naaaa"
  },
  {
    "objectID": "posts/2022-09-24-using-json-normalize/index.html#adding-context-to-record-and-meta-data-using-record_prefix-and-meta_prefix",
    "href": "posts/2022-09-24-using-json-normalize/index.html#adding-context-to-record-and-meta-data-using-record_prefix-and-meta_prefix",
    "title": "Using json_normalize Pandas function",
    "section": "6 Adding context to record and meta data using record_prefix and meta_prefix",
    "text": "6 Adding context to record and meta data using record_prefix and meta_prefix\n\npd.json_normalize(\n    json_list, \n    record_path=['players'], \n    meta=['team', 'colour', ['info', 'staff', 'physio']],\n    meta_prefix='meta-',\n    record_prefix='player-',\n    sep='-&gt;'\n)\n\n\n\n\n\n\n\n\nplayer-name\nplayer-sex\nplayer-stats-&gt;goals\nplayer-stats-&gt;assists\nmeta-team\nmeta-colour\nmeta-info-&gt;staff-&gt;physio\n\n\n\n\n0\nViv\nF\n101.0\n40.0\narsenal\nred-white\nxxxx\n\n\n1\nBeth\nF\n60.0\n25.0\narsenal\nred-white\nxxxx\n\n\n2\nSteph\nF\nNaN\nNaN\ncity\nblue\naaaa\n\n\n3\nLucy\nF\nNaN\nNaN\ncity\nblue\naaaa"
  },
  {
    "objectID": "posts/2022-09-24-using-json-normalize/index.html#working-with-a-local-file",
    "href": "posts/2022-09-24-using-json-normalize/index.html#working-with-a-local-file",
    "title": "Using json_normalize Pandas function",
    "section": "7 Working with a local file",
    "text": "7 Working with a local file\nIn most scenarios, we won’t be making new JSON object ourselves instead use JSON formatted files. We make use python’s json module and read the file, then use pandas’ json_normalize to flatten it into a dataframe.\n\nimport json\n# load data using Python JSON module\nwith open('movies.json') as f:\n    data = json.load(f)\n    \n# Normalizing data\npd.json_normalize(data)\n\n\n\n\n\n\n\n\nTitle\nUS Gross\nWorldwide Gross\nUS DVD Sales\nProduction Budget\nRelease Date\nMPAA Rating\nRunning Time min\nDistributor\nSource\nMajor Genre\nCreative Type\nDirector\nRotten Tomatoes Rating\nIMDB Rating\nIMDB Votes\n\n\n\n\n0\nThe Land Girls\n146083\n146083\nNaN\n8000000\nJun 12 1998\nR\nNaN\nGramercy\nNone\nNone\nNone\nNone\nNaN\n6.1\n1071.0\n\n\n1\nFirst Love, Last Rites\n10876\n10876\nNaN\n300000\nAug 07 1998\nR\nNaN\nStrand\nNone\nDrama\nNone\nNone\nNaN\n6.9\n207.0\n\n\n2\nI Married a Strange Person\n203134\n203134\nNaN\n250000\nAug 28 1998\nNone\nNaN\nLionsgate\nNone\nComedy\nNone\nNone\nNaN\n6.8\n865.0\n\n\n3\nFour Rooms\n4301000\n4301000\nNaN\n4000000\nDec 25 1995\nR\nNaN\nMiramax\nOriginal Screenplay\nComedy\nContemporary Fiction\nRobert Rodriguez\n14.0\n6.4\n34328.0\n\n\n4\nThe Four Seasons\n42488161\n42488161\nNaN\n6500000\nMay 22 1981\nNone\nNaN\nUniversal\nOriginal Screenplay\nComedy\nContemporary Fiction\nAlan Alda\n71.0\n7.0\n1814.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n63\nBig Things\n0\n0\nNaN\n50000\nDec 31 2009\nNone\nNaN\nNone\nNone\nNone\nNone\nNone\nNaN\nNaN\nNaN\n\n\n64\nBogus\n4357406\n4357406\nNaN\n32000000\nSep 06 1996\nPG\nNaN\nWarner Bros.\nOriginal Screenplay\nComedy\nFantasy\nNorman Jewison\n40.0\n4.8\n2742.0\n\n\n65\nBeverly Hills Cop\n234760478\n316300000\nNaN\n15000000\nDec 05 1984\nNone\nNaN\nParamount Pictures\nOriginal Screenplay\nAction\nContemporary Fiction\nMartin Brest\n83.0\n7.3\n45065.0\n\n\n66\nBeverly Hills Cop II\n153665036\n276665036\nNaN\n20000000\nMay 20 1987\nR\nNaN\nParamount Pictures\nOriginal Screenplay\nAction\nContemporary Fiction\nTony Scott\n46.0\n6.1\n29712.0\n\n\n67\nBeverly Hills Cop III\n42586861\n119180938\nNaN\n50000000\nMay 25 1994\nR\nNaN\nParamount Pictures\nOriginal Screenplay\nAction\nContemporary Fiction\nJohn Landis\n10.0\n5.0\n21199.0\n\n\n\n\n68 rows × 16 columns"
  },
  {
    "objectID": "posts/2022-09-24-using-json-normalize/index.html#working-with-url",
    "href": "posts/2022-09-24-using-json-normalize/index.html#working-with-url",
    "title": "Using json_normalize Pandas function",
    "section": "8 Working with URL",
    "text": "8 Working with URL\nReading a JSON file from an url needs an extra module in requests as any data from the Internet carries overheads that are necessary for efficient exchange of information(REST API). So, in order to read the file contents, we call upon requests’ text attribute which fetches the contents of the file.\nHere, we use json.loads and not json.load as loads function expects contents(string) rather than a file pointer. If looked closely into the json module, the load calls loads using read() on the file.\n\nimport requests\n\nURL = 'https://vega.github.io/vega-datasets/data/cars.json'\n\ndata = json.loads(requests.get(URL).text)\npd.json_normalize(data)\n\n\n\n\n\n\n\n\nName\nMiles_per_Gallon\nCylinders\nDisplacement\nHorsepower\nWeight_in_lbs\nAcceleration\nYear\nOrigin\n\n\n\n\n0\nchevrolet chevelle malibu\n18.0\n8\n307.0\n130.0\n3504\n12.0\n1970-01-01\nUSA\n\n\n1\nbuick skylark 320\n15.0\n8\n350.0\n165.0\n3693\n11.5\n1970-01-01\nUSA\n\n\n2\nplymouth satellite\n18.0\n8\n318.0\n150.0\n3436\n11.0\n1970-01-01\nUSA\n\n\n3\namc rebel sst\n16.0\n8\n304.0\n150.0\n3433\n12.0\n1970-01-01\nUSA\n\n\n4\nford torino\n17.0\n8\n302.0\n140.0\n3449\n10.5\n1970-01-01\nUSA\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n401\nford mustang gl\n27.0\n4\n140.0\n86.0\n2790\n15.6\n1982-01-01\nUSA\n\n\n402\nvw pickup\n44.0\n4\n97.0\n52.0\n2130\n24.6\n1982-01-01\nEurope\n\n\n403\ndodge rampage\n32.0\n4\n135.0\n84.0\n2295\n11.6\n1982-01-01\nUSA\n\n\n404\nford ranger\n28.0\n4\n120.0\n79.0\n2625\n18.6\n1982-01-01\nUSA\n\n\n405\nchevy s-10\n31.0\n4\n119.0\n82.0\n2720\n19.4\n1982-01-01\nUSA\n\n\n\n\n406 rows × 9 columns"
  },
  {
    "objectID": "posts/2022-09-24-using-json-normalize/index.html#conclusion",
    "href": "posts/2022-09-24-using-json-normalize/index.html#conclusion",
    "title": "Using json_normalize Pandas function",
    "section": "9 Conclusion",
    "text": "9 Conclusion\nWe saw the use of json_normalize function in pandas library. It helps take a JSON data, flatten it, and make it as a dataframe for easier analysis."
  },
  {
    "objectID": "posts/2023-06-02-Terraform-setup/index.html",
    "href": "posts/2023-06-02-Terraform-setup/index.html",
    "title": "Getting started with Terraform",
    "section": "",
    "text": "Terraform from Hasicorp is Infrastructure as Code(IaC) tool that is gaining popularity among the data engineers. It lets us build, change and version infrastructure safely and efficiently. As to why one needs Terraform, I shall direct you to this comprehensive article.\nIn this post, I will write a small guide to get us started with Terraform. This post then can be used as a starting guide for my future posts involving Terraform.\n\n\n\n\n\n\nNote\n\n\n\nNote that this installation guide is only for Mac(M1 silicon) and Ubuntu(GCP E2-medium instance). Also this guide will look similar to the example provided in Hashicorp’s website. The intention is to have a guide for my benefit in one place.\n\n\n\n\nA fully functional Macbook or an Ubuntu GCP/AWS instance with sudo privileges."
  },
  {
    "objectID": "posts/2023-06-02-Terraform-setup/index.html#what-is-main.tf",
    "href": "posts/2023-06-02-Terraform-setup/index.html#what-is-main.tf",
    "title": "Getting started with Terraform",
    "section": "What is main.tf?",
    "text": "What is main.tf?\nIt is a terraform configuration file that describes infrastructure in Terraform. Each terraform config needs a separate directory with a single main.tf.\nThis file contains:\n\nTerraform block\nProviders\nResources\n\n\nTerraform block\nThis block contains Terraform settings for require providers that Terraform will use to provision infrastructure. In our case it is kreuzwerker/docker as source. kreuzwereker is the provider/developer/host and docker is the product we are interested in using. We can specify the minimum version to install. If not mentioned, Terraform will install the lastest version available. Terraform registry is default place to look for popular providers such as AWS, GCP, Azure.\nterraform {\n  required_providers {\n    docker = {\n      source  = \"kreuzwerker/docker\"\n      version = \"~&gt; 3.0.1\"\n    }\n  }\n}\n\n\nProviders\nNow that Terraform knows the “source”, we need to provide a “provider”. Docker is our choice. In this block we can configure docker provider.\nprovider \"docker\" {}\n\n\nResources\nA resource block is used to define components of the provider previously mentioned. So we have a docker provider. We need a docker image and build a container for that image.\nresource \"docker_image\" \"nginx\" {\n  name         = \"nginx\"\n  keep_locally = false\n}\nresource \"docker_container\" \"nginx\" {\n  image = docker_image.nginx.image_id\n  name  = \"tutorial\"\n  ports {\n    internal = 80\n    external = 8000\n  }\n}\nThe resource block takes two strings - resource type and resource name. docker_image is the type and nginx is the name. These two together docker_image.nginx form a unique ID for the resource. For docker_container resource, we use the image from the previous block as reference. So a container is created with that image and ports are also mapped."
  },
  {
    "objectID": "posts/2023-06-02-Terraform-setup/index.html#validate-and-initialise-the-configuration",
    "href": "posts/2023-06-02-Terraform-setup/index.html#validate-and-initialise-the-configuration",
    "title": "Getting started with Terraform",
    "section": "Validate and initialise the configuration",
    "text": "Validate and initialise the configuration\nTerraform has a bunch of CLI commands to make things easier and faster. One needs to remember only 5-6 commands. Rest can be looked up later.\nTo validate the configuration we use\n\n\nTerminal\n\n1terrform validate\n\n\n1\n\nReturns “Success! The configuration is valid.” if syntactically valid.\n\n\nThen we need to initialise our configuration. This process will download and install the providers mentioned. This command will create a hidden directory .terraform and install docker provider. Terraform also creates a lock file named .terraform.lock.hcl which specifies the exact provider versions used, so that you can control when you want to update the providers used for your project."
  },
  {
    "objectID": "posts/2023-06-02-Terraform-setup/index.html#plan-and-create-infrastructure",
    "href": "posts/2023-06-02-Terraform-setup/index.html#plan-and-create-infrastructure",
    "title": "Getting started with Terraform",
    "section": "Plan and create infrastructure",
    "text": "Plan and create infrastructure\nTerraform allows us to see what will be created through terraform plan commands. This command is used to give an overview of the things that will be created. An extra, precautionary step. It looks trivial for our use case but imagine a big organisation with several team members managing infrastructure. Even deleting a resource accidently will create chaos. It is always advised to run terraform plan, overview and then go ahead with creating resources.\nterraform apply is then given to apply the configuration. This is then approved to actually create resources. In our case a nginx docker image is downloaded and container using that image is created with port forwarding."
  },
  {
    "objectID": "posts/2023-06-02-Terraform-setup/index.html#result",
    "href": "posts/2023-06-02-Terraform-setup/index.html#result",
    "title": "Getting started with Terraform",
    "section": "Result",
    "text": "Result\nA nginx server has been started and we can access it through http:127.0.0.1:8000. We will be greeted with a message “Welcome to nginx!”."
  },
  {
    "objectID": "posts/2023-06-02-Terraform-setup/index.html#terraform-state",
    "href": "posts/2023-06-02-Terraform-setup/index.html#terraform-state",
    "title": "Getting started with Terraform",
    "section": "Terraform state",
    "text": "Terraform state\nTerraform has a state(status) file which stores the current state(metadata) of the infrastructure. Any changes to the resource configuration will be reflected into this file. If working in a team, it is advised to store this state file remotely such as s3, gcs etc with versioning and state locking.\n\nTweaking the infrastructure\nTo understand state file better, we shall change the port from 8000 to 8010 in the docker_container resource block.\nresource \"docker_container\" \"nginx\" {\n  image = docker_image.nginx.image_id\n  name  = \"tutorial\"\n  ports {\n    internal = 80\n    external = 8010\n  }\n}\nYou will see that there will be two state files - terraform.tfstate and terraform.tfstate.backup. On the first run both file’s contents will be same. However, now that we changed the port to 8010, terraform.tfstate will change 8010 but the backup file will retain 8000. This is Terraform’s rollback feature.\nApply the changes using terraform apply."
  },
  {
    "objectID": "posts/2023-06-02-Terraform-setup/index.html#destroy-infrastructure",
    "href": "posts/2023-06-02-Terraform-setup/index.html#destroy-infrastructure",
    "title": "Getting started with Terraform",
    "section": "Destroy infrastructure",
    "text": "Destroy infrastructure\nTo destroy resources after use, we can use terraform destroy command. Review the plan and approve it. Be cautious using this command in an organisation."
  },
  {
    "objectID": "posts/2023-02-06-skim-vimtex/index.html",
    "href": "posts/2023-02-06-skim-vimtex/index.html",
    "title": "Setup Skim PDF reader with VimTeX in Mac OS",
    "section": "",
    "text": "VimTeX plugin written by Karl Yngve Lervåg is one of the goto plugins to manage LaTeX files with Vim/Neovim text editors. VimTeX allows integration with several PDF viewers. In Mac OS, Skim and Zathura PDF readers allow easy integration with LaTeX. Since Zathura’s installation in Mac OS involves more steps, we will be using Skim for this post."
  },
  {
    "objectID": "posts/2023-02-06-skim-vimtex/index.html#install-skim",
    "href": "posts/2023-02-06-skim-vimtex/index.html#install-skim",
    "title": "Setup Skim PDF reader with VimTeX in Mac OS",
    "section": "Install Skim",
    "text": "Install Skim\nWith Homebrew\n\n\nTerminal\n\nbrew install --cask skim\n\nOr download the dmg file of the current version(as of writing latest version is v1.6.8) from Skim’s website."
  },
  {
    "objectID": "posts/2023-02-06-skim-vimtex/index.html#install-vimtex",
    "href": "posts/2023-02-06-skim-vimtex/index.html#install-vimtex",
    "title": "Setup Skim PDF reader with VimTeX in Mac OS",
    "section": "Install VimTeX",
    "text": "Install VimTeX\nUsing vim-plug plugin manager we add the following line to .vimrc or init.vim or init.lua\n\n\nInside init.vim\n\nPlug 'lervag/vimtex'"
  },
  {
    "objectID": "posts/2023-02-06-skim-vimtex/index.html#pdf-preview",
    "href": "posts/2023-02-06-skim-vimtex/index.html#pdf-preview",
    "title": "Setup Skim PDF reader with VimTeX in Mac OS",
    "section": "Pdf preview",
    "text": "Pdf preview\nConversion between TeX and PDF is one of the most common operations while writing a scientific document. Though it is possible to open the PDF file in one of the commercially available PDF readers, a seamless integration with neovim(in our case) is appreciated. This is where Skim comes into the picture. By default, Skim allows native, seamless integration with the LaTex editor of choice. In our case, we can make VimTex interact with Skim with just a few lines of config."
  },
  {
    "objectID": "posts/2023-02-06-skim-vimtex/index.html#configurations",
    "href": "posts/2023-02-06-skim-vimtex/index.html#configurations",
    "title": "Setup Skim PDF reader with VimTeX in Mac OS",
    "section": "Configurations",
    "text": "Configurations\n\nMinimal setup and Forward Search\nWe require the following lines to make VimTeX talk to Skim within neovim. This direction of communication, is known as forward search.\n\n\nInside init.vim\n\n# Hover over the number at the end of each line to see its importance\n1let g:vimtex_view_method = 'skim'\n\n\n2let g:vimtex_view_skim_sync = 1\n3let g:vimtex_view_skim_activate = 1\n\n\n1\n\nChoose which program to use to view PDF file.\n\n2\n\nValue 1 allows forward search after every successful compilation.\n\n3\n\nValue 1 allows change focus to skim after command :VimtexView is given.\n\n\nThe forward search allows any change made in the TeX file automatically refreshes Skim to reflect those changes in PDF. One of the other common uses is cursor sync between the TeX file and PDF. Setting let g:vimtex_view_skim_sync allows placing the cursor in some position in the Tex file sync with the same position in the PDF after every successful compilation(:VimtexCompile). Setting let g:vimtex_view_skim_activate allows to shift focus of control from neovim to Skim and bring it to foreground.\n\n\nInverse or Backward Search\nSo far there was only one channel of communication between neovim(editor) and Skim. A backward communication is possible but it took quite bit of hacking to get it to work. More on that read this jdhao’s post. However, with the release of VimTex v2.8, it has become simple to setup.\nConsider a scenario where we are going through a paper and find an error, instead of going back to source TeX file and finding the error location can be cumbersome. Using backward search, we can go to the error location from PDF to TeX. For Skim, to activate backward search press shift and command together and click the position in PDF using the mouse. That location gets reflected in the editor in the background. For more information, see :h :VimtexInverseSearch\nNatively, every instance of neovim starts a server 1. With Skim as client and nvim as server, we can interact in that direction.\nIn order to do so, in the preferances pane of Skim, navigate to Sync tab. There, in the PDF-TeX Sync support, make preset as custom, command as nvim(use vim if you use vim editor), and set arguments as --headless -c \"VimtexInverseSearch %line '%file'\".\n\n\n\n\n\n\n\nImportant\n\n\n\nSkim must be started by VimTeX (either through compiler callback or explicitly via lv) for backward sync to work! (This is how Skim “knows” which neovim instance – terminal or GUI – to sync to.)"
  },
  {
    "objectID": "posts/2023-02-06-skim-vimtex/index.html#conclusion",
    "href": "posts/2023-02-06-skim-vimtex/index.html#conclusion",
    "title": "Setup Skim PDF reader with VimTeX in Mac OS",
    "section": "Conclusion",
    "text": "Conclusion\nWith just four lines of settings in the init.vim file and a line in Skim preferances, we can activate both forward and backward search features with VimTeX."
  },
  {
    "objectID": "posts/2023-04-27-getting-started-with-s3/data/30-days-create-folds.html",
    "href": "posts/2023-04-27-getting-started-with-s3/data/30-days-create-folds.html",
    "title": "Deepak Ramani's blog",
    "section": "",
    "text": "::: {#4e7e904b .cell _cell_guid=‘b1076dfc-b9ad-4769-8c92-a6c4dae69d19’ _uuid=‘8f2839f25d086af736a60e9eeb907d3b93b6e0e5’ execution=‘{“iopub.execute_input”:“2021-08-16T20:16:18.360034Z”,“iopub.status.busy”:“2021-08-16T20:16:18.358906Z”,“iopub.status.idle”:“2021-08-16T20:16:19.347479Z”,“shell.execute_reply”:“2021-08-16T20:16:19.348020Z”,“shell.execute_reply.started”:“2021-08-16T20:05:04.468564Z”}’ papermill=‘{“duration”:0.999913,“end_time”:“2021-08-16T20:16:19.348363”,“exception”:false,“start_time”:“2021-08-16T20:16:18.348450”,“status”:“completed”}’ tags=‘[]’ execution_count=1}\nimport numpy as np\nimport pandas as pd\nfrom sklearn import model_selection\n:::\n\ndf_train = pd.read_csv(\"../input/30-days-of-ml/train.csv\")\n\n\ndf_train[\"kfold\"] = -1\n\n\nkf = model_selection.KFold(n_splits=5, shuffle=True, random_state=42)\nfor fold, (train_indicies, valid_indicies) in enumerate(kf.split(X=df_train)):\n    df_train.loc[valid_indicies, \"kfold\"] = fold\n\n\ndf_train.to_csv(\"train_folds.csv\", index=False)"
  },
  {
    "objectID": "projects/mlops.html",
    "href": "projects/mlops.html",
    "title": "mlops",
    "section": "",
    "text": "You can read about it in github."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Deepak Ramani",
    "section": "",
    "text": "Hello, I’m Deepak! I am a passionate software engineer with a strong interest in data engineering, DevOps, and machine learning. I thrive on tackling complex problems and finding innovative solutions through collaborative teamwork. I spend my free time learning new tools and technologies. Currently learning “effective Pandas” and Kubeflow.\nWith a background in software engineering I am driven by a desire to leverage the power of data to drive impactful results. During my master’s thesis, I developed an end-to-end system with a focus on implementing simpler CNN model architectures to achieve comparable performance to the latest models. This experience allowed me to deepen my understanding of data collection, preprocessing, and the fusion techniques necessary for night-time driving and collision avoidance.\nI am an enthusiastic learner who stays up-to-date with the latest industry trends and technologies. My strong problem-solving skills, combined with my passion for teamwork, enable me to contribute to challenging projects and deliver high-quality results. I am now seeking new opportunities in Germany to apply my skills and knowledge in data engineering, DevOps, and machine learning, making a meaningful impact within a dynamic organization.\nFeel free to connect with me to explore collaboration opportunities, share insights, or discuss exciting projects in these fields. Let’s embark on this journey together to drive innovation and solve complex problems."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Deepak Ramani's blog",
    "section": "",
    "text": "Title\n\n\nDate\n\n\nDescription\n\n\nReading Time\n\n\n\n\n\n\nBuilding Data Pipeline - Part 4 - CI/CD\n\n\nJun 13, 2023\n\n\nBuild CI/CD pipeline with Github actions\n\n\n5 min\n\n\n\n\nTerraform - Attach IAM policies to a role\n\n\nJun 12, 2023\n\n\nA short post on how to attach policies to an IAM role within Terraform\n\n\n4 min\n\n\n\n\nBuilding Data Pipelines - Part 3 - Terraform\n\n\nJun 11, 2023\n\n\nManage AWS hosted web app with Terraform\n\n\n8 min\n\n\n\n\nBuilding Data Pipelines - Part 2 - AWS Cloud\n\n\nJun 4, 2023\n\n\nA post on migrating the web app to AWS cloud\n\n\n10 min\n\n\n\n\nBuilding Data Pipelines - Part 1 - Docker\n\n\nJun 3, 2023\n\n\nA short post on using docker to deploy lambda function locally\n\n\n4 min\n\n\n\n\nDeploy Web Application with Docker\n\n\nJun 3, 2023\n\n\nA short guide on how to deploy a flask application using docker\n\n\n7 min\n\n\n\n\nGetting started with Terraform\n\n\nJun 2, 2023\n\n\nAn introductory post on getting started with Terraform in Mac and Ubuntu and spin up a docker nginx container\n\n\n7 min\n\n\n\n\nGetting started with S3 using boto3\n\n\nApr 27, 2023\n\n\nAn introduction to S3 with boto3 AWS python SDK\n\n\n6 min\n\n\n\n\nSetup Skim PDF reader with VimTeX in Mac OS\n\n\nFeb 6, 2023\n\n\nA short post to setup Skim pdf reader with Vimtex plugin in Mac OS.\n\n\n4 min\n\n\n\n\nUsing json_normalize Pandas function\n\n\nSep 24, 2022\n\n\nA tutorial with examples on flattening JSON object using json_normalize pandas function\n\n\n15 min\n\n\n\n\nManage dotfiles with GNU Stow\n\n\nJan 9, 2022\n\n\nA guide to manage all the dotfiles.\n\n\n4 min\n\n\n\n\nSetting up Kaggle on Linux/Mac\n\n\nSep 18, 2021\n\n\nA guide to setup Kaggle API on Linux/Mac.\n\n\n3 min\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#blog-posts",
    "href": "index.html#blog-posts",
    "title": "Deepak Ramani's blog",
    "section": "",
    "text": "Title\n\n\nDate\n\n\nDescription\n\n\nReading Time\n\n\n\n\n\n\nBuilding Data Pipeline - Part 4 - CI/CD\n\n\nJun 13, 2023\n\n\nBuild CI/CD pipeline with Github actions\n\n\n5 min\n\n\n\n\nTerraform - Attach IAM policies to a role\n\n\nJun 12, 2023\n\n\nA short post on how to attach policies to an IAM role within Terraform\n\n\n4 min\n\n\n\n\nBuilding Data Pipelines - Part 3 - Terraform\n\n\nJun 11, 2023\n\n\nManage AWS hosted web app with Terraform\n\n\n8 min\n\n\n\n\nBuilding Data Pipelines - Part 2 - AWS Cloud\n\n\nJun 4, 2023\n\n\nA post on migrating the web app to AWS cloud\n\n\n10 min\n\n\n\n\nBuilding Data Pipelines - Part 1 - Docker\n\n\nJun 3, 2023\n\n\nA short post on using docker to deploy lambda function locally\n\n\n4 min\n\n\n\n\nDeploy Web Application with Docker\n\n\nJun 3, 2023\n\n\nA short guide on how to deploy a flask application using docker\n\n\n7 min\n\n\n\n\nGetting started with Terraform\n\n\nJun 2, 2023\n\n\nAn introductory post on getting started with Terraform in Mac and Ubuntu and spin up a docker nginx container\n\n\n7 min\n\n\n\n\nGetting started with S3 using boto3\n\n\nApr 27, 2023\n\n\nAn introduction to S3 with boto3 AWS python SDK\n\n\n6 min\n\n\n\n\nSetup Skim PDF reader with VimTeX in Mac OS\n\n\nFeb 6, 2023\n\n\nA short post to setup Skim pdf reader with Vimtex plugin in Mac OS.\n\n\n4 min\n\n\n\n\nUsing json_normalize Pandas function\n\n\nSep 24, 2022\n\n\nA tutorial with examples on flattening JSON object using json_normalize pandas function\n\n\n15 min\n\n\n\n\nManage dotfiles with GNU Stow\n\n\nJan 9, 2022\n\n\nA guide to manage all the dotfiles.\n\n\n4 min\n\n\n\n\nSetting up Kaggle on Linux/Mac\n\n\nSep 18, 2021\n\n\nA guide to setup Kaggle API on Linux/Mac.\n\n\n3 min\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#projects",
    "href": "index.html#projects",
    "title": "Deepak Ramani's blog",
    "section": "Projects",
    "text": "Projects\n\n\n\n\n\nProject\n\n\nDescription\n\n\nRole\n\n\n\n\n\n\nGrocery Unit Sale Predictor - a MLOPS project\n\n\nAn end-to-end ML project that designs and manages a ML model production workflow. \n\n\nCreator\n\n\n\n\nAutonomous driving framework - a DL project\n\n\nAn end-to-end autonomous driving neural network ML model using simpler CNN architecture. \n\n\nCreator\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Archive",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\n\n\n\n\nJun 13, 2023\n\n\nBuilding Data Pipeline - Part 4 - CI/CD\n\n\n\n\nJun 12, 2023\n\n\nTerraform - Attach IAM policies to a role\n\n\n\n\nJun 11, 2023\n\n\nBuilding Data Pipelines - Part 3 - Terraform\n\n\n\n\nJun 4, 2023\n\n\nBuilding Data Pipelines - Part 2 - AWS Cloud\n\n\n\n\nJun 3, 2023\n\n\nDeploy Web Application with Docker\n\n\n\n\nJun 3, 2023\n\n\nBuilding Data Pipelines - Part 1 - Docker\n\n\n\n\nJun 2, 2023\n\n\nGetting started with Terraform\n\n\n\n\nMay 10, 2023\n\n\nNotes - LinkedIn course - Financial data analysis\n\n\n\n\nApr 27, 2023\n\n\nGetting started with S3 using boto3\n\n\n\n\nFeb 6, 2023\n\n\nSetup Skim PDF reader with VimTeX in Mac OS\n\n\n\n\nSep 24, 2022\n\n\nUsing json_normalize Pandas function\n\n\n\n\nSep 7, 2022\n\n\nNotes - Python concepts\n\n\n\n\nJan 9, 2022\n\n\nManage dotfiles with GNU Stow\n\n\n\n\nSep 18, 2021\n\n\nSetting up Kaggle on Linux/Mac\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-04-27-getting-started-with-s3/index.html",
    "href": "posts/2023-04-27-getting-started-with-s3/index.html",
    "title": "Getting started with S3 using boto3",
    "section": "",
    "text": "Boto3 is an AWS python SDK that allows access to AWS services like EC2 and S3. It provides a python object-oriented API and as well as low-level access to AWS services\nimport boto3, botocore\nimport glob\n\nfiles = glob.glob('data/*') #to upload multiple files\nfiles\n\n['data/Player Data.xlsx',\n 'data/30-days-create-folds.ipynb',\n 'data/ARK_GENOMIC_REVOLUTION_ETF_ARKG_HOLDINGS.csv',\n 'data/star_pattern_turtlesim.png']"
  },
  {
    "objectID": "posts/2023-04-27-getting-started-with-s3/index.html#create-a-session-and-client",
    "href": "posts/2023-04-27-getting-started-with-s3/index.html#create-a-session-and-client",
    "title": "Getting started with S3 using boto3",
    "section": "Create a session and client",
    "text": "Create a session and client\nBoto3’s region defaults to N-Virginia. To create buckets in another region, region name has to be explicitly mentioned using session object.\n\nsession = boto3.Session(region_name='us-east-2')\ns3client = session.client('s3')\ns3resource = boto3.resource('s3')\n\nS3 buckets have to follow bucket naming rules.\n\nbucket_names = ['my-s3bucket1-usohio-region', 'my-s3bucket2-usohio-region']\ns3location = {'LocationConstraint': 'us-east-2'}"
  },
  {
    "objectID": "posts/2023-04-27-getting-started-with-s3/index.html#check-if-bucket-exists-in-s3",
    "href": "posts/2023-04-27-getting-started-with-s3/index.html#check-if-bucket-exists-in-s3",
    "title": "Getting started with S3 using boto3",
    "section": "Check if bucket exists in S3",
    "text": "Check if bucket exists in S3\nChecking for something before creation is one of the important tasks to avoid unnecessary errors. Here we check if the buckets already exists.\n\ndef check_bucket(bucket):\n    \"\"\"\n    Checks if a bucket is present in S3\n    args:\n    bucket: takes bucket name\n    \"\"\"\n    try:\n        s3client.head_bucket(Bucket=bucket)\n        print('Bucket exists')\n        return True\n    except botocore.exceptions.ClientError as e:\n        # If a client error is thrown, then check that it was a 404 error.\n        # If it was a 404 error, then the bucket does not exist.\n        error_code = int(e.response['Error']['Code'])\n        if error_code == 403:\n            print(\"Private Bucket. Forbidden Access!\")\n            return True\n        elif error_code == 404:\n            print(\"Bucket Does Not Exist!\")\n            return False\n\n\nfor bucket in bucket_names: \n    print(check_bucket(bucket))\n\nBucket exists\nTrue\nBucket exists\nTrue"
  },
  {
    "objectID": "posts/2023-04-27-getting-started-with-s3/index.html#create-a-bucket-in-s3",
    "href": "posts/2023-04-27-getting-started-with-s3/index.html#create-a-bucket-in-s3",
    "title": "Getting started with S3 using boto3",
    "section": "Create a bucket in S3",
    "text": "Create a bucket in S3\nIf the buckets don’t exist, we create them. We need to supply bucket name, a dictionary specifying in which region the bucket has to be created.\n\nfor bucket_name in bucket_names: \n    if not(check_bucket(bucket_name)):\n        print('Creating a bucket..')\n        s3client.create_bucket(Bucket = bucket_name, CreateBucketConfiguration=s3location)\n\nBucket exists\nBucket exists"
  },
  {
    "objectID": "posts/2023-04-27-getting-started-with-s3/index.html#bucket-versioning",
    "href": "posts/2023-04-27-getting-started-with-s3/index.html#bucket-versioning",
    "title": "Getting started with S3 using boto3",
    "section": "Bucket Versioning",
    "text": "Bucket Versioning\nBucket versioning initial state is not set by default. The response from when not initialised doesn’t carry status information rather status dict is absent. Status expects two return states: enabled, suspended. On first creation, the status is in disabled, an unknown state.\nSo in order to make it appear in the REST response, bucket must be enabled by calling the BucketVersioning() boto3 resource function. If we then check the status, it will be present in the REST response.\n\ndef get_buckets_versioning_client(bucketname):\n    \"\"\"\n    Checks if bucket versioning is enabled/suspended or initialised\n    Args:\n    bucketname: bucket name to check versioning\n    Returns: response status - enabled or suspended\n    \"\"\"\n    response = s3client.get_bucket_versioning(Bucket = bucketname)\n    if 'Status' in response and (response['Status'] == 'Enabled' or response['Status'] == 'Suspended'):\n        print(f'Bucket {bucketname} status: {response[\"Status\"]}')\n        return response['Status']\n    else:\n        print(f'Bucket versioning not initialised for bucket: {bucketname}. Enabling...')\n        s3resource.BucketVersioning(bucket_name=bucketname).enable()\n        enable_response = s3resource.BucketVersioning(bucket_name=bucket_name).status\n        return enable_response\n\n\nfor bucket_name in bucket_names: \n    version_status = get_buckets_versioning_client(bucket_name)\n    print(f'Versioning status: {version_status}')\n\nBucket my-s3bucket1-usohio-region status: Enabled\nVersioning status: Enabled\nBucket my-s3bucket2-usohio-region status: Enabled\nVersioning status: Enabled"
  },
  {
    "objectID": "posts/2023-04-27-getting-started-with-s3/index.html#to-suspend-bucket-versioning",
    "href": "posts/2023-04-27-getting-started-with-s3/index.html#to-suspend-bucket-versioning",
    "title": "Getting started with S3 using boto3",
    "section": "To suspend bucket versioning",
    "text": "To suspend bucket versioning\n\nfor bucket_name in bucket_names:\n    version_status = get_buckets_versioning_client(bucket_name)\n    print(f'Versioning status: {version_status}')\n    if version_status == 'Enabled':\n        print('Disabling again..')\n        s3resource.BucketVersioning(bucket_name=bucket_name).suspend()\n\nBucket my-s3bucket1-usohio-region status: Enabled\nVersioning status: Enabled\nDisabling again..\nBucket my-s3bucket2-usohio-region status: Enabled\nVersioning status: Enabled\nDisabling again.."
  },
  {
    "objectID": "posts/2023-04-27-getting-started-with-s3/index.html#to-enable-bucket-versioning",
    "href": "posts/2023-04-27-getting-started-with-s3/index.html#to-enable-bucket-versioning",
    "title": "Getting started with S3 using boto3",
    "section": "To enable bucket versioning",
    "text": "To enable bucket versioning\n\nfor bucket_name in bucket_names:\n    version_status = get_buckets_versioning_client(bucket_name)\n    print(f'Versioning status: {version_status}')\n    if version_status == 'Suspended':\n        print('Enabling again..')\n        s3resource.BucketVersioning(bucket_name=bucket_name).enable()\n\nBucket my-s3bucket1-usohio-region status: Suspended\nVersioning status: Suspended\nEnabling again..\nBucket my-s3bucket2-usohio-region status: Suspended\nVersioning status: Suspended\nEnabling again.."
  },
  {
    "objectID": "posts/2023-04-27-getting-started-with-s3/index.html#get-bucket-list-from-s3",
    "href": "posts/2023-04-27-getting-started-with-s3/index.html#get-bucket-list-from-s3",
    "title": "Getting started with S3 using boto3",
    "section": "Get bucket list from S3",
    "text": "Get bucket list from S3\nWe can list the buckets in S3 using list_buckets() client function. It return a dict. We can iterate through Buckets key to find the names of the buckets.\n\nbuckets_list = s3client.list_buckets()\nfor bucket in buckets_list['Buckets']:\n    print(bucket['Name'])\n\nmlops-project-sales-forecast-bucket\nmlops-project-sales-forecast-bucket-dr563105-mlops-project\nmy-s3bucket1-usohio-region\nmy-s3bucket2-usohio-region\ns3-for-terraform-state"
  },
  {
    "objectID": "posts/2023-04-27-getting-started-with-s3/index.html#upload-files-to-s3",
    "href": "posts/2023-04-27-getting-started-with-s3/index.html#upload-files-to-s3",
    "title": "Getting started with S3 using boto3",
    "section": "Upload files to S3",
    "text": "Upload files to S3\nBoto3 allows file upload to S3. The upload_file client function requires three mandatory arguments -\n1. filename of the file to be uploaded\n2. bucket_name, Into which bucket the file would be uploaded\n3. key, name of the file in S3\n\ndef upload_files_to_s3(filename, bucket_name, key=None, ExtraArgs=None):\n    \"\"\"\n    Uploads file to S3 bucket\n    Args:\n    filename: takes local filename to be uploaded\n    bucker_name: name of the bucket into which the file is uploaded\n    key: name of the file in the bucket. Default:None\n    ExtraArgs: other arguments. Default:None\n    \"\"\"\n    if key is None:\n        key = filename\n    \n    try:\n        s3client.upload_file(filename,bucket_name,key)\n        print(f'uploaded file:{filename}')\n    except botocore.exceptions.ClientError as e:\n        print(e)\n\nWe can make use of glob module to upload multiple files in a folder\n\nbucket1_files = [files[1],files[2]]\nbucket2_files = [files[0],files[3]]\nbucket1_files, bucket2_files\n\n(['data/30-days-create-folds.ipynb',\n  'data/ARK_GENOMIC_REVOLUTION_ETF_ARKG_HOLDINGS.csv'],\n ['data/Player Data.xlsx', 'data/star_pattern_turtlesim.png'])\n\n\n\nfor file in bucket1_files:\n    upload_files_to_s3(file,bucket_name=bucket_names[0])\n\nuploaded file:data/30-days-create-folds.ipynb\nuploaded file:data/ARK_GENOMIC_REVOLUTION_ETF_ARKG_HOLDINGS.csv\n\n\n\nfor file in bucket2_files:\n    upload_files_to_s3(file,bucket_name=bucket_names[1])\n\nuploaded file:data/Player Data.xlsx\nuploaded file:data/star_pattern_turtlesim.png"
  },
  {
    "objectID": "posts/2023-04-27-getting-started-with-s3/index.html#get-files-list",
    "href": "posts/2023-04-27-getting-started-with-s3/index.html#get-files-list",
    "title": "Getting started with S3 using boto3",
    "section": "Get files list",
    "text": "Get files list\nGetting the files list from each bucket done using list_objects client function. It returns dict and we can iterate through Contents key to retrieve the filenames.\n\nfor bucket in bucket_names:\n    print(f'Listing object inside bucket:{bucket}')\n    list_obj_response = s3client.list_objects(Bucket=bucket)\n    for obj in list_obj_response['Contents']:\n        print(obj['Key'])\n    print()\n\nListing object inside bucket:my-s3bucket1-usohio-region\ndata/30-days-create-folds.ipynb\ndata/ARK_GENOMIC_REVOLUTION_ETF_ARKG_HOLDINGS.csv\n\nListing object inside bucket:my-s3bucket2-usohio-region\ndata/Player Data.xlsx\ndata/star_pattern_turtlesim.png"
  },
  {
    "objectID": "posts/2023-04-27-getting-started-with-s3/index.html#download-files",
    "href": "posts/2023-04-27-getting-started-with-s3/index.html#download-files",
    "title": "Getting started with S3 using boto3",
    "section": "Download files",
    "text": "Download files\nDownloading a file is very similar to uploading one. We need specify bucket name, name of the file to be downloaded, and the destination filename.\n\nprint(f'Downloading files from bucket:{bucket_names[1]}')\ns3client.download_file(Bucket=bucket_names[1],Key='data/star_pattern_turtlesim.png',Filename='downloaded_turtlesim.jpg')\n\nDownloading files from bucket:my-s3bucket2-usohio-region"
  },
  {
    "objectID": "posts/2023-04-27-getting-started-with-s3/index.html#conclusion",
    "href": "posts/2023-04-27-getting-started-with-s3/index.html#conclusion",
    "title": "Getting started with S3 using boto3",
    "section": "Conclusion",
    "text": "Conclusion\nThis blog post shows how to use the boto3 python SDK to manage S3 aws service. With the help of documentation, we can implement require functionalities."
  },
  {
    "objectID": "posts/2023-06-03-deploy-with-docker/index.html",
    "href": "posts/2023-06-03-deploy-with-docker/index.html",
    "title": "Deploy Web Application with Docker",
    "section": "",
    "text": "A customer wants to build a web application on machine learning trained model. They approach us to find a simple solution. They give us a trained model file in parquet format. We need to look up for a potential sale price of a product within the given time period and store number. How can it be achieved?\n\n\nThere are three possibilities to solve this problem:\n\nUse python testing functionality, give the inputs and generate outputs.\nSpin up a Flask application with an endpoint, pass arguments into it and generate outputs.\nBuild a docker image with Flask and its dependencies, run it and pass arguments.\n\nAll the three approaches are technical. In other words, there are chances of failures that may be replicable. To allow for minimal error, we will choose the docker option which packages the second, Flask application."
  },
  {
    "objectID": "posts/2023-06-03-deploy-with-docker/index.html#possible-approaches",
    "href": "posts/2023-06-03-deploy-with-docker/index.html#possible-approaches",
    "title": "Deploy Web Application with Docker",
    "section": "",
    "text": "There are three possibilities to solve this problem:\n\nUse python testing functionality, give the inputs and generate outputs.\nSpin up a Flask application with an endpoint, pass arguments into it and generate outputs.\nBuild a docker image with Flask and its dependencies, run it and pass arguments.\n\nAll the three approaches are technical. In other words, there are chances of failures that may be replicable. To allow for minimal error, we will choose the docker option which packages the second, Flask application."
  },
  {
    "objectID": "posts/2023-06-03-deploy-with-docker/index.html#why-flask",
    "href": "posts/2023-06-03-deploy-with-docker/index.html#why-flask",
    "title": "Deploy Web Application with Docker",
    "section": "Why Flask?",
    "text": "Why Flask?\nFlask is a popular Python microframework for developing web applications. It is also known for its simplicity, flexibility and scalability. Since we want to use Python and make it simple for us to maintain, we will choose Flask."
  },
  {
    "objectID": "posts/2023-06-03-deploy-with-docker/index.html#flask-application",
    "href": "posts/2023-06-03-deploy-with-docker/index.html#flask-application",
    "title": "Deploy Web Application with Docker",
    "section": "Flask application",
    "text": "Flask application\nA web application is similar to visiting a website through an address. Such as visiting your gmail account using mail.google.com. The gmail website is hosted in many servers across the world. A server closest to your location will serve you when you open up the website. Similarly in our case we need to host our application somewhere. We will, for now, choose the local option; meaning in our local machine.\nUsing Flask we host our application in a particular location(endpoint) and through a port locally. I choose this location to be http://localhost:9696/predict-sales.\nBelow is the code for the flask application.\n\n\nflask-app.py\n\nimport pandas as pd\nfrom flask import Flask, jsonify, request\nfrom zipfile import ZipFile as zf\nimport os\n\ndef read_parquet_files(filename: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Read parquet file format for given filename and returns the contents\n    \"\"\"\n    df = pd.read_parquet(filename, engine=\"pyarrow\")\n    return df\n\n\nif os.path.exists(\"lgb_preds.parquet\"):\n    df_test_preds = read_parquet_files(\"lgb_preds.parquet\")\nelse:   \n    with zf(\"lgb_preds.parquet.zip\") as zipfile:\n        zipfile.extract(\"lgb_preds.parquet\")\n1    df_test_preds = read_parquet_files(\"lgb_preds.parquet\")\n\ndf_items = read_parquet_files(\"items.parquet\")\n\n2app = Flask(\"flask-unit-sales-prediction\")\n\n\ndef predict(find: request, item_idx: int) -&gt; float:\n    \"\"\"\n    Takes the json inputs, processes it and outputs the unit sales\n    \"\"\"\n    try:\n        idx = pd.IndexSlice\n        x = df_test_preds.loc[idx[find[\"store_nbr\"], item_idx, find[\"date1\"]]][\n            \"unit_sales\"\n        ]\n    except KeyError:\n        print(\"This item is not present this store. Try some other item\")\n        return -0.0\n    else:\n        return float(round(x, 2))\n\n\n3@app.route(\"/predict-sales\", methods=[\"POST\"])\ndef predict_endpoint():\n    \"\"\"\n    flask predict endpoint\n    \"\"\"\n    find = request.get_json()\n    item = df_items.sample(1)\n    item_idx, item_family = item.index[0], item[\"family\"].values[0]\n    pred_unit_sales = predict(find, item_idx)\n    result = {\n        \" Store\": find[\"store_nbr\"],\n        \" item\": int(item_idx),\n        \"Family\": item_family,\n        \"Prediction date\": find[\"date1\"],\n        \"Unit_sales\": pred_unit_sales,\n    }\n\n4    return jsonify(result)\n\n\nif __name__ == \"__main__\":\n5    app.run(debug=True, host=\"0.0.0.0\", port=9696)\n\n\n1\n\nGetting the trained model table\n\n2\n\nInitialising the Flask app\n\n3\n\nDeclaring the endpoint with “POST” as method\n\n4\n\nPredicted result that converted to JSON format\n\n5\n\nSpin up flask app\n\n\nFlask uses a library called gunicorn to spin up a HTML server. In the code we have dependencies for pandas, pyarrow, numpy. We can’t ask the customer to install these themselves. We intend to use Docker to prevent that criteria. But how to install these dependencies in a docker container?"
  },
  {
    "objectID": "posts/2023-06-03-deploy-with-docker/index.html#dependencies-install",
    "href": "posts/2023-06-03-deploy-with-docker/index.html#dependencies-install",
    "title": "Deploy Web Application with Docker",
    "section": "Dependencies install",
    "text": "Dependencies install\nThere are number of options available for us:\n\nUse pip and requirements.txt. But tracking package versions might get tedious.\nUse Anaconda/Miniconda. For local machine they’re fine but for docker container?they install so many unnecessary libraries making container huge in size.\nUse Poetry. This one is developed to deliver packages. But still it includes libraries unnecessary to us.\nUse Pipenv. It is much smaller, easy to write and use. Hence I will choose this."
  },
  {
    "objectID": "posts/2023-06-03-deploy-with-docker/index.html#pipenv-pipfile-and-pipfile.lock",
    "href": "posts/2023-06-03-deploy-with-docker/index.html#pipenv-pipfile-and-pipfile.lock",
    "title": "Deploy Web Application with Docker",
    "section": "Pipenv, Pipfile and Pipfile.lock",
    "text": "Pipenv, Pipfile and Pipfile.lock\nPipenv is an advanced version of requirements.txt. It uses inline tables and TOML spec. We can easily give the package versions we want to install and pipenv internally creates a Pipfile.lock file that contains hashes to each package installed. A convenient solution for our purpose.\nOur Pipfile looks like this:\n\n\nPipfile\n\n[[source]]\nurl = \"https://pypi.org/simple\"\nverify_ssl = true\nname = \"pypi\"\n\n[packages]\npandas = \"*\"\nflask = \"*\"\npyarrow = \"*\"\ngunicorn = \"*\"\nnumpy = \"*\"\n\n[dev-packages]\nrequests = \"*\"\n\n[requires]\npython_version = \"3.9\"\n\nOn the first run we need to install these packages locally and also to create Pipfile.lock. We can accomplish that using the command pipenv install --dev. I recommend running this inside conda’s base environment. Please don’t use system’s python to manage projects."
  },
  {
    "objectID": "posts/2023-06-03-deploy-with-docker/index.html#dockerfile-and-building-docker-image",
    "href": "posts/2023-06-03-deploy-with-docker/index.html#dockerfile-and-building-docker-image",
    "title": "Deploy Web Application with Docker",
    "section": "Dockerfile and building docker image",
    "text": "Dockerfile and building docker image\nDockerfile is a text file that give instructions to docker engine to build an docker image. We choose python 3.9-slim version, install pip and pipenv, copy all the necessary files, importantly expose the port 9696 and set gunicorn http server as the entrypoint to the image whenever the container is run.\n\n\nDockerfile\n\nFROM python:3.9.14-slim\n\nRUN pip install -U pip\nRUN pip install pipenv\n\nWORKDIR /app\n\nCOPY [ \"Pipfile\", \"Pipfile.lock\", \"./\" ]\n\nRUN pipenv install --system --deploy\n\nCOPY [ \"lgb_preds.parquet.zip\" , \"lgb_preds.parquet.zip\" ]\nCOPY [ \"items.parquet\" , \"items.parquet\" ]\n\nCOPY [\"flask_app.py\", \"flask_app.py\"]\n\nEXPOSE 9696\n\nENTRYPOINT [ \"gunicorn\", \"--bind=0.0.0.0:9696\", \"flask_sales_predictor:app\" ]\n\nTo build a docker image, we use the command docker build -t flask_app:v1 . .\nTo verify it is created we can run docker images and look for the name."
  },
  {
    "objectID": "posts/2023-06-03-deploy-with-docker/index.html#running-the-docker-container",
    "href": "posts/2023-06-03-deploy-with-docker/index.html#running-the-docker-container",
    "title": "Deploy Web Application with Docker",
    "section": "Running the docker container",
    "text": "Running the docker container\nOur software is now packaged as an image. We need to somehow run it to use our application. We can run it through this command\n\n\nTerminal 1\n\ndocker run \\\n    -it \\\n    --rm \\\n    -p 9696:9696 \\\n    flask_app:v1"
  },
  {
    "objectID": "posts/2023-06-03-deploy-with-docker/index.html#testing-the-application",
    "href": "posts/2023-06-03-deploy-with-docker/index.html#testing-the-application",
    "title": "Deploy Web Application with Docker",
    "section": "Testing the application",
    "text": "Testing the application\nThe docker container should be up and running in terminal 1. Now all that is left is test it. For that we need one more file.\n\n\ntest_requests.py in terminal 2\n\nimport requests\n\nfind = {\"date1\": \"2017-08-21\", \"store_nbr\": 19}\n\nurl = \"http://localhost:9696/predict-sales\"\nresponse = requests.post(url, json=find, timeout=10)\nprint(response.json())\n\nThe test_requests.py file has the input in JSON format and the endpoint url http://localhost:9696/predict-sales. We leverage the requests library to make our requests.\nOpen up another terminal or another tab, activate virtual environment and run python test_requests.py. Just python needs to be present inside this environment as everything else is inside the docker container.\nIf everything goes well, we should the predicted product price printed."
  },
  {
    "objectID": "posts/2023-06-13-Github-Actions-CI-CD-TF/index.html",
    "href": "posts/2023-06-13-Github-Actions-CI-CD-TF/index.html",
    "title": "Building Data Pipeline - Part 4 - CI/CD",
    "section": "",
    "text": "The Continuous Integration and Continuous Delivery abbreviated as CI/CD is an important part of software development cycle. This part provides the essential step in checking everything before the final product is ready to delivered to the customer. In our case, this step should create infrastructure to deploy the web application into the cloud. In other words, automate the tasks we did in part 3.\nThere are many tools for CI/CD. We will be using Github Actions as our code is in Github and make sense to automate the build, test and deploy pipeline with it. Our workflow will consist of running CI upon a pull request and CD when this pull request is merged to a particular branch."
  },
  {
    "objectID": "posts/2023-06-13-Github-Actions-CI-CD-TF/index.html#triggers",
    "href": "posts/2023-06-13-Github-Actions-CI-CD-TF/index.html#triggers",
    "title": "Building Data Pipeline - Part 4 - CI/CD",
    "section": "Triggers",
    "text": "Triggers\nWe want to setup event based triggers to activate our workflows. Since we don’t to distrub main branch code base, we create two branches - develop and feature/ci-cd. We switch to the feature/ci-cd branch and define our workflows.\nIn ci-test.yml the trigger is set on pull_request on the branch develop. Meaning we commit our changes to the feature/ci-cd branch and then do a pull_request for develop branch. A pull in Github speak means “pulling/updating” new contents from another remote branch. Pull requests are usually done in UI as it is easy to review, clarify, approve changes and merge request.\nHowever, in cd-deploy.yml we need our infrastructure setup deployed. So it makes sense to put a trigger upon commits being pushed into the develop branch. SO the trigger is push. This action is activated when the pull_request is merged into the develop branch."
  },
  {
    "objectID": "posts/2023-06-13-Github-Actions-CI-CD-TF/index.html#jobs",
    "href": "posts/2023-06-13-Github-Actions-CI-CD-TF/index.html#jobs",
    "title": "Building Data Pipeline - Part 4 - CI/CD",
    "section": "Jobs",
    "text": "Jobs\n\nci-test.yml\nIn CI, it is critical to check if all the parts are integrated together correctly. In our case, we need to confirm whether tf-plan works with our Terraform configuration.\nIn Github Actions, jobs are can be run on several hosted systems through actions. We will use ubuntu-latest with action version 3. That will in run calls aws-actions for credentials. We provide our AWS secrets to our Github secrets page. Github says the secrets are encrypted before reaching them. They also take measures to prevents secrets appearing in logs. 2\nThe actions access the AWS secrets to setup our final task to run our infrastructure plan.\n\n\nGithub Secrets\n\nenv:\n  AWS_DEFAULT_REGION: \"us-east-1\"\n  AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}\n  AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n\njobs:\n  tf-plan:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - name: Configure AWS Credentials\n        uses: aws-actions/configure-aws-credentials@v2\n        with:\n1          aws-access-key-id: ${{ env.AWS_ACCESS_KEY_ID }}\n          aws-secret-access-key: ${{ env.AWS_SECRET_ACCESS_KEY }}\n          aws-region: ${{ env.AWS_DEFAULT_REGION }} \n\n\n1\n\n`${{ secrets.AWS_ACCESS_KEY_ID }} can also be directly used\n\n\nSince the pipeline is used in production environment, we configure our terraform backend with our new production tfstate and use the plan command to check if our plan is valid, error-free and suitable to our needs.\n\n\nTF Plan\n\n- name: TF plan\n   id: plan\n1    working-directory: \"infrastructure\"\n     run: |\n      terraform init -backend-config=\"key=mlops-grocery-sales-prod.tfstate\" --reconfigure && terraform plan --var-file vars/prod.tfvars\n\n\n1\n\nChange to infrastructure directory to execute the terraform command\n\n\n\n\ncd-deploy.yml\nCI in this case does only tf-plan but usually it runs several unit and integration tests. We could combine these two files into one if we want but we will leave things as it is.\nSimilarly, the cd-deply.yml workflow file, check the tf-plan. Upon its success triggers the tf-apply job.\n\n\ntf-apply\n\n- name: TF Apply\n    id: tf-apply\n    working-directory: \"infrastructure\"\n    if: ${{ steps.tf-plan.outcome }} == \"success\"\n    run: |\n      terraform apply -auto-approve -var-file=vars/prod.tfvars\n      echo \"name=rest_api_url::$(terraform output rest_api_url | xargs)\" &gt;&gt; $GITHUB_OUTPUT\n      echo \"name=ecr_repo::$(terraform output run_id | xargs)\" &gt;&gt; $GITHUB_OUTPUT\n      echo \"name=run_id::$(terraform output run_id | xargs)\" &gt;&gt; $GITHUB_OUTPUT\n      echo \"name=lambda_function::$(terraform output lambda_function | xargs)\" &gt;&gt; $GITHUB_OUTPUT\n\n$GITHUB_OUTPUT takes Terraform outputs and displays them in the logs."
  },
  {
    "objectID": "posts/2023-06-13-Github-Actions-CI-CD-TF/index.html#deleting-the-infrastructure",
    "href": "posts/2023-06-13-Github-Actions-CI-CD-TF/index.html#deleting-the-infrastructure",
    "title": "Building Data Pipeline - Part 4 - CI/CD",
    "section": "Deleting the infrastructure",
    "text": "Deleting the infrastructure\nIf for some reason you wish to delete the resources created in the AWS Cloud, please follow these steps -\n\nGoto infrastructure’s main.tf file. Comment out all the modules. Also comment out contents in outputs.tf\nThen commit the changes, push, pull the request and merge the changes.\n\nThis should delete all the resources. Confirm with AWS Console.\nAlternatively we can use terraform destroy ourselves if we know which tfstate and tfvars were used.\n\n\ntf-destroy\n\n1terraform init -backend-config=\"key=mlops-grocery-sales-prod.tfstate\" --reconfigure\n2terraform destroy -var-file=vars/prod.tfvars\n\n\n1\n\nSince we know which Terraform statefile is used for production\n\n2\n\nWhich production variable file is used"
  },
  {
    "objectID": "posts/2023-06-03-lambda-docker/index.html",
    "href": "posts/2023-06-03-lambda-docker/index.html",
    "title": "Building Data Pipelines - Part 1 - Docker",
    "section": "",
    "text": "This post is a follow up to my previous post on deploying with docker. This will be part 1 of the deploy using Docker series. In this part, I will introduce the concept of lambda_function and how it can tested locally.\n\n\nThe customer from my previous post doesn’t want to keep things in their local machine and wish for a cloud option. Also don’t want to reserve resources as they expect only intermittent requests. What could be a viable solution? Enter Lambda_function. Each cloud provider has other own name for lambda function. We will use AWS and it is called Lambda.\nAWS Lambda is a serverless compute service that run code in reponse to an event and automatically manages the underlying compute resources for us. Moreover we pay only for what we use.1\nBefore transitioning into AWS Lambda cloud solution, we can modify our program to AWS Lambda coding standards and test it locally.\n\n\n\nLambda function handler has certain coding standards. 2 For example, the name of the python file must be lambda_function.py and the handler function lambda_handler.\nGoing by that standard, we can modify our flask-app.py from previous post to lambda_function.py.\n\n\nlambda_function.py\n\nimport os\nimport pandas as pd\n\ndef read_parquet_files(filename: str):\n    \"\"\"\n    Read parquet file format for given filename and returns the contents\n    \"\"\"\n    df = pd.read_parquet(filename, engine=\"pyarrow\")\n    return df\n\ndf_test_preds = read_parquet_files(\"lgb_preds.parquet\")\ndf_items = read_parquet_files(\"items.parquet\")\n\ndef predict(find, item_idx: int):\n    \"\"\"\n    Takes the json inputs, processes it and outputs the unit sales\n    \"\"\"\n    try:\n        idx = pd.IndexSlice\n        x = df_test_preds.loc[idx[find[\"store_nbr\"], item_idx, find[\"date1\"]]][\n            \"unit_sales\"\n        ]\n    except KeyError:\n        print(\"This item is not present this store. Try some other item\")\n        return -0.0\n    else:\n        return float(round(x, 2))\n\n1def lambda_handler(event, context=None) -&gt; dict:\n    \"\"\"\n    lambda handler for predict method\n    \"\"\"\n\n    find = event[\"find\"]\n    item = df_items.sample(1)\n    item_idx, item_family = item.index[0], item[\"family\"].values[0]\n    pred_unit_sales = predict(find, item_idx)\n\n    result = {\n        \" Store\": find[\"store_nbr\"],\n        \" item\": int(item_idx),\n        \"Family\": item_family,\n        \"Prediction date\": find[\"date1\"],\n        \"Unit_sales\": pred_unit_sales,\n    }\n    return result\n\n\n1\n\nDefault handler function name\n\n\nLike last time we will use Pipenv to install dependencies. If you’re new, kindly visit deploy with docker post to know why we use Pipenv.\n\n\n\n\n\nDockerfile\n\n1FROM public.ecr.aws/lambda/python:3.9\n\nRUN pip install -U pip\nRUN pip install pipenv\n\nCOPY [ \"Pipfile\", \"Pipfile.lock\", \"./\" ]\n\nRUN pipenv install --system --deploy\n\nCOPY [ \"lgb_preds.parquet\" , \"lgb_preds.parquet\" ]\nCOPY [ \"items.parquet\" , \"lambda_function.py\", \"./\" ]\n\n2CMD [ \"lambda_function.lambda_handler\" ]\n\n\n1\n\nUsing AWS ECR hosted python-3.9 docker image. As to why I will explain in the next part.\n\n2\n\nPay close attention as to how the handler function is invoked. It is not entrypoint like last time.\n\n\n\n\n\nWe use the same build command as last time.\n\n\nTerminal 1\n\ndocker build -t lambda-app:v1 .\n\nThe result if successful would look like this:\n\nRunning the container is different this time as the port exposed is different. AWS allows local testing of Lambda function code through a localhost endpoint. This endpoint is at 9000 port(it can be customised). Inside docker container we make it listen to 8080 port. So we forward 9000 to 8080.\n\n\nTerminal 1\n\ndocker run \\\n    -it \\\n    --rm \\\n1    -p 9000:8080 \\\n    lambda-app:v1\n\n\n1\n\nport forwarding 9000 to 8080\n\n\nResult looks like so:"
  },
  {
    "objectID": "posts/2023-06-03-lambda-docker/index.html#why-lambda_function",
    "href": "posts/2023-06-03-lambda-docker/index.html#why-lambda_function",
    "title": "Building Data Pipelines - Part 1 - Docker",
    "section": "",
    "text": "The customer from my previous post doesn’t want to keep things in their local machine and wish for a cloud option. Also don’t want to reserve resources as they expect only intermittent requests. What could be a viable solution? Enter Lambda_function. Each cloud provider has other own name for lambda function. We will use AWS and it is called Lambda.\nAWS Lambda is a serverless compute service that run code in reponse to an event and automatically manages the underlying compute resources for us. Moreover we pay only for what we use.1\nBefore transitioning into AWS Lambda cloud solution, we can modify our program to AWS Lambda coding standards and test it locally."
  },
  {
    "objectID": "posts/2023-06-03-lambda-docker/index.html#aws-lambda-handler",
    "href": "posts/2023-06-03-lambda-docker/index.html#aws-lambda-handler",
    "title": "Building Data Pipelines - Part 1 - Docker",
    "section": "",
    "text": "Lambda function handler has certain coding standards. 2 For example, the name of the python file must be lambda_function.py and the handler function lambda_handler.\nGoing by that standard, we can modify our flask-app.py from previous post to lambda_function.py.\n\n\nlambda_function.py\n\nimport os\nimport pandas as pd\n\ndef read_parquet_files(filename: str):\n    \"\"\"\n    Read parquet file format for given filename and returns the contents\n    \"\"\"\n    df = pd.read_parquet(filename, engine=\"pyarrow\")\n    return df\n\ndf_test_preds = read_parquet_files(\"lgb_preds.parquet\")\ndf_items = read_parquet_files(\"items.parquet\")\n\ndef predict(find, item_idx: int):\n    \"\"\"\n    Takes the json inputs, processes it and outputs the unit sales\n    \"\"\"\n    try:\n        idx = pd.IndexSlice\n        x = df_test_preds.loc[idx[find[\"store_nbr\"], item_idx, find[\"date1\"]]][\n            \"unit_sales\"\n        ]\n    except KeyError:\n        print(\"This item is not present this store. Try some other item\")\n        return -0.0\n    else:\n        return float(round(x, 2))\n\n1def lambda_handler(event, context=None) -&gt; dict:\n    \"\"\"\n    lambda handler for predict method\n    \"\"\"\n\n    find = event[\"find\"]\n    item = df_items.sample(1)\n    item_idx, item_family = item.index[0], item[\"family\"].values[0]\n    pred_unit_sales = predict(find, item_idx)\n\n    result = {\n        \" Store\": find[\"store_nbr\"],\n        \" item\": int(item_idx),\n        \"Family\": item_family,\n        \"Prediction date\": find[\"date1\"],\n        \"Unit_sales\": pred_unit_sales,\n    }\n    return result\n\n\n1\n\nDefault handler function name\n\n\nLike last time we will use Pipenv to install dependencies. If you’re new, kindly visit deploy with docker post to know why we use Pipenv."
  },
  {
    "objectID": "posts/2023-06-03-lambda-docker/index.html#dockerfile",
    "href": "posts/2023-06-03-lambda-docker/index.html#dockerfile",
    "title": "Building Data Pipelines - Part 1 - Docker",
    "section": "",
    "text": "Dockerfile\n\n1FROM public.ecr.aws/lambda/python:3.9\n\nRUN pip install -U pip\nRUN pip install pipenv\n\nCOPY [ \"Pipfile\", \"Pipfile.lock\", \"./\" ]\n\nRUN pipenv install --system --deploy\n\nCOPY [ \"lgb_preds.parquet\" , \"lgb_preds.parquet\" ]\nCOPY [ \"items.parquet\" , \"lambda_function.py\", \"./\" ]\n\n2CMD [ \"lambda_function.lambda_handler\" ]\n\n\n1\n\nUsing AWS ECR hosted python-3.9 docker image. As to why I will explain in the next part.\n\n2\n\nPay close attention as to how the handler function is invoked. It is not entrypoint like last time."
  },
  {
    "objectID": "posts/2023-06-03-lambda-docker/index.html#docker-image-build-and-running-the-container",
    "href": "posts/2023-06-03-lambda-docker/index.html#docker-image-build-and-running-the-container",
    "title": "Building Data Pipelines - Part 1 - Docker",
    "section": "",
    "text": "We use the same build command as last time.\n\n\nTerminal 1\n\ndocker build -t lambda-app:v1 .\n\nThe result if successful would look like this:\n\nRunning the container is different this time as the port exposed is different. AWS allows local testing of Lambda function code through a localhost endpoint. This endpoint is at 9000 port(it can be customised). Inside docker container we make it listen to 8080 port. So we forward 9000 to 8080.\n\n\nTerminal 1\n\ndocker run \\\n    -it \\\n    --rm \\\n1    -p 9000:8080 \\\n    lambda-app:v1\n\n\n1\n\nport forwarding 9000 to 8080\n\n\nResult looks like so:"
  },
  {
    "objectID": "posts/2021-09-18-kaggle-setup/index.html",
    "href": "posts/2021-09-18-kaggle-setup/index.html",
    "title": "Setting up Kaggle on Linux/Mac",
    "section": "",
    "text": "Most of latest data science innovations happen at Kaggle. Kaggle hosts, in addtion to competitions, a large collection of datasets from various fields. The easiest way to interact with Kaggle is through its public API via command-line tool(CLI). Setting it up outside of Kaggle kernels is one of first tasks. In this post, I will guide you through that process."
  },
  {
    "objectID": "posts/2021-09-18-kaggle-setup/index.html#installation",
    "href": "posts/2021-09-18-kaggle-setup/index.html#installation",
    "title": "Setting up Kaggle on Linux/Mac",
    "section": "Installation",
    "text": "Installation\n\n\nTerminal\n\npip install --user kaggle\n\n\n\n\n\n\n\nTip\n\n\n\nTip: Install kaggle package inside your conda ML development environment rather than outside of it or in base env.\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nDon’t do sudo pip install kaggle as it would require admin privileges for every run."
  },
  {
    "objectID": "posts/2021-09-18-kaggle-setup/index.html#download-api-token",
    "href": "posts/2021-09-18-kaggle-setup/index.html#download-api-token",
    "title": "Setting up Kaggle on Linux/Mac",
    "section": "Download API token",
    "text": "Download API token\n\nCreate/login into your kaggle account.\nFrom the site header, click on your user profile picture and select Account. You will be land on your profile with account tab active.\nScroll down to API section. Click Create New API Token. A json file will be downloaded your default download directory."
  },
  {
    "objectID": "posts/2021-09-18-kaggle-setup/index.html#move-.json-file-to-the-correct-location",
    "href": "posts/2021-09-18-kaggle-setup/index.html#move-.json-file-to-the-correct-location",
    "title": "Setting up Kaggle on Linux/Mac",
    "section": "Move .json file to the correct location",
    "text": "Move .json file to the correct location\n\nMove it to .kaggle in the home directory. Create if absent.\n\n\n\nTerminal\n\ncd\nmkdir ~/.kaggle\nmv &lt;location&gt;/kaggle.json ~/.kaggle/kaggle.json\n\n\nFor your security, ensure that other users of your computer do not have read access to your credentials. On Unix-based systems you can do this with the following command:\n\n\n\nTerminal\n\nchmod 600 ~/.kaggle/kaggle.json\n\n\nRestart the terminal and navigate to the env where kaggle package is installed if necessary."
  },
  {
    "objectID": "posts/2021-09-18-kaggle-setup/index.html#check-if-it-is-properly-installed",
    "href": "posts/2021-09-18-kaggle-setup/index.html#check-if-it-is-properly-installed",
    "title": "Setting up Kaggle on Linux/Mac",
    "section": "Check if it is properly installed",
    "text": "Check if it is properly installed\n\nRun:\n\n\n\nTerminal\n\n$python\n&gt;&gt;&gt;import kaggle\n\nImporting kaggle shouldn’t return an error. If there is error, check whether you’re in the right env where kaggle is installed.\nIf no error, exit the shell and type the following command in the terminal.\n\n\nTerminal\n\nkaggle competitions list\n\nIf installed properly, the command will list all the entered competitions. 1. If not, the binary path may be incorrect. Usually it is installed in ~/.local/bin Try using\n\n\nTerminal\n\n~/.local/bin/kaggle competitions list\n\n\nIf the above command works, export that binary path to the shell environment(bashrc) so that you might use just kaggle next time."
  },
  {
    "objectID": "posts/2021-09-18-kaggle-setup/index.html#api-usage",
    "href": "posts/2021-09-18-kaggle-setup/index.html#api-usage",
    "title": "Setting up Kaggle on Linux/Mac",
    "section": "API usage",
    "text": "API usage\nIt is time to use the Kaggle API. For example, to see what dataset command offers, in the CLI enter\n\n\nTerminal\n\nkaggle dataset --help\n\n\n\n\n\n\n\nTip\n\n\n\nTip: Remember to comply with competition’s terms and conditions before downloading the dataset. You will get an error forbidden if you try to download before agreeing.\n\n\nFor more info on the API, Kaggle’s github page is an excellent resource."
  },
  {
    "objectID": "posts/2023-06-04-api-lambda-ecr/aws-ecr-api.html",
    "href": "posts/2023-06-04-api-lambda-ecr/aws-ecr-api.html",
    "title": "Deepak Ramani's blog",
    "section": "",
    "text": "In this document I’m going to include a guide to integrate/setup AWS API Gateway, AWS Lambda using ECR image as source image.\nIt is long and technical with lots of screenshots and explanations."
  },
  {
    "objectID": "posts/2023-06-04-api-lambda-ecr/aws-ecr-api.html#create-docker-container-image-and-upload-to-ecr",
    "href": "posts/2023-06-04-api-lambda-ecr/aws-ecr-api.html#create-docker-container-image-and-upload-to-ecr",
    "title": "Deepak Ramani's blog",
    "section": "Create docker container image and upload to ECR",
    "text": "Create docker container image and upload to ECR\nWe have the dockerfile, lambda_function, Pipfile, Pipfile.lock and items.parquet. If none of these make any sense, I urge you to go through my posts on lambda and docker on my blog.\n\n\nTerminal\n\ndocker build -t lambda-app:v1 . # build docker image\nexport ACCOUNT_ID=xxxx\naws ecr create-repository --repository-name lambda-images\ndocker tag lambda-app:v1 ${ACCOUNT_ID}.dkr.ecr.us-east-1.amazonaws.com/lambda-images:app\n$(aws ecr get-login --no-include-email)\ndocker push ${ACCOUNT_ID}.dkr.ecr.us-east-1.amazonaws.com/lambda-images:app\n\nNote: Remember we supplied aws access key and secret before and the session borrows them for login. If it is a new session, those variables have to be given again.\n \nIn the screenshots we can see that our container image is uploaded to the registry."
  },
  {
    "objectID": "posts/2023-06-04-api-lambda-ecr/aws-ecr-api.html#aws-lambda-function",
    "href": "posts/2023-06-04-api-lambda-ecr/aws-ecr-api.html#aws-lambda-function",
    "title": "Deepak Ramani's blog",
    "section": "AWS Lambda function",
    "text": "AWS Lambda function\nAWS Lambda allows us to use our container image from ECR to use as source. We need to give the image URI(can be found in ECR console). Below screenshot shows the step to point at the container image and create a function. \nIn cloud environment, security is a massive risk. So AWS insists on policies everywhere. Policy allows only revelant people access. Below policy screenshot shows us giving permission to Cloudwatch to create log groups, put log events whenever the Lambda function is invoked. You can goto “Monitor” tab and browse through the logs.\n\n\nPolicy for S3 access\nSince our artifact after ML training is stored in S3 bucket, we need to give the Lambda function permission to access it. For that click on the role lambda-sales-app-role-qu7xpax1 under Execution role. It will direct to the IAM console. Here click “Add Permission” - “Create Inline Policy” - “JSON” tab and paste the following JSON data:\n{\n    \"Statement\": [\n        {\n            \"Action\": [\n                \"s3:Get*\",\n                \"s3:List*\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": [\n                \"arn:aws:s3:::mlops-project-sales-forecast-bucket\",\n                \"arn:aws:s3:::mlops-project-sales-forecast-bucket/*\"\n            ]\n        }\n    ],\n    \"Version\": \"2012-10-17\"\n}\n mlops-project-sales-forecast-bucket is the name of the S3 bucket where the artifacts are stored. I give restricted permission to the Lambda function. Quite often we see resource will be given * which can led to problems and security threats in the future. Review it. \nNext, create a unique name for it and click create policy. \nIn the overview policy console page, we can see that there are two policies attached. That means the lambda function has permission to download from that specified S3 bucket. \n\n\nEnvironment Variables and Configuration\nIn the configuration tab, environment variable section add in RUN_ID and S3_BUCKET_NAME.  Next change the Max Memory and Timeout values in the General Configuration section. \n\n\nTesting it at Lambda console\nIn the Test tab, we create a test event and give our sample JSON input.  \n{\"find\": {\"date1\": \"2017-08-26\", \"store_nbr\": 20}}\nIf you get a successful execution as in the screenshot, it means the pipeline works and the attached policies are correct. Most time it is the policies that cause annoying issues."
  },
  {
    "objectID": "posts/2023-06-04-api-lambda-ecr/aws-ecr-api.html#api-gateway",
    "href": "posts/2023-06-04-api-lambda-ecr/aws-ecr-api.html#api-gateway",
    "title": "Deepak Ramani's blog",
    "section": "API Gateway",
    "text": "API Gateway\n\nRest API with an endpoint\nWe will create a REST API with an endpoint of predict-sales using POST method. Goto API Gateway console -&gt; Create API -&gt; Rest API -&gt; Click Build. Here give an appropriate name and description. \nThen we create our endpoint predict-sales as a resource and POST method under it. While creating the POST method, we point it to our previously created Lambda function lambda-sales-app.   There will be a pop up window with ARN address. This address will be same as Lambda function. You can verify it by going to the Lambda console.\n\n\nTesting\nBefore deploying the API we can test. Press the Test button, give the sample JSON input in the body section and expect an output similar to Lambda test.  \nNow our API is ready for deployment. ### Deployment From the actions menu choose “Deploy API”. Choose “New Stage”, give a name and then deploy.   We will get an invoke url. However, we need to append our endpoint predict-sales to complete it. So it will look something like https://xxxxxxxx.execute-api.us-east-1.amazonaws.com/stg-lambda-app-for-blog/predict-sales"
  },
  {
    "objectID": "posts/2023-06-04-api-lambda-ecr/aws-ecr-api.html#dynamodb-add-policy",
    "href": "posts/2023-06-04-api-lambda-ecr/aws-ecr-api.html#dynamodb-add-policy",
    "title": "Deepak Ramani's blog",
    "section": "DynamoDB add policy",
    "text": "DynamoDB add policy\nAdd this policy to the previously created role to give lambda access to write to DynamoDB.\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"dynamodb:BatchGetItem\",\n                \"dynamodb:GetItem\",\n                \"dynamodb:Query\",\n                \"dynamodb:Scan\",\n                \"dynamodb:BatchWriteItem\",\n                \"dynamodb:PutItem\",\n                \"dynamodb:UpdateItem\"\n            ],\n            \"Resource\": \"arn:aws:dynamodb:us-east-1:4xxxxxxxx:table/sales_preds_for_blog\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"logs:CreateLogStream\",\n                \"logs:PutLogEvents\"\n            ],\n            \"Resource\": \"arn:aws:dynamodb:us-east-1:4xxxxxxxx:table/sales_preds_for_blog\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"logs:CreateLogGroup\",\n            \"Resource\": \"arn:aws:dynamodb:us-east-1:4xxxxxxxx:table/sales_preds_for_blog\"\n        }\n    ]\n}"
  },
  {
    "objectID": "posts/2022-01-09-dotfiles/index.html",
    "href": "posts/2022-01-09-dotfiles/index.html",
    "title": "Manage dotfiles with GNU Stow",
    "section": "",
    "text": "In this post, I will try to guide in organise your dotfiles in the cloud and manage them using GNU Stow."
  },
  {
    "objectID": "posts/2022-01-09-dotfiles/index.html#what-are-dotfiles",
    "href": "posts/2022-01-09-dotfiles/index.html#what-are-dotfiles",
    "title": "Manage dotfiles with GNU Stow",
    "section": "What are dotfiles?",
    "text": "What are dotfiles?\nFor a casual user, the term dotfiles may sound strange and confusing but it is nothing but application(app) configuration files in developer talk. The apps generally refer to certain files to configure itself.\nPeople usually store these files in a remote location such as a Github repository and retrieve them when needed.\nDotfiles allow personalisation. They can be restored in a new machine saving time. Preparing and organising the dotfiles with some initial effort, help developers save a lot of time later.\nA few examples of dotfiles are .bashrc, .vimrc, .gitignore.\n\n\n\n\n\n\nImportant\n\n\n\nPay attention to personal information inside these files. Never store secure keys, passwords in public domains.\n\n\n\n\nThings to know\n\nWhich app’s config files need to stored.\nWhere do those config files are located.\n\n\n\nCommon config files that need storing\n\n.bashrc or .zshrc\n.vimrc or init.vim(in the case of neovim)\n.gitignore_global and .gitconfig\nTerminal emulator config files\nIDE of choice config files\nAnyother config you want to save\n\nIn fact, if there is an app that you have configured heavily and frequently use, its config files must be stored. In the case the said app doesn’t allow exporting of configurations, it is highly recommended to move onto one that allows it.\n\n\nWhere are most required dotfiles located?\nMost files are present in $HOME or $XDG_CONFIG_HOME directories. $XDG_CONFIG_HOME defines the base directory relative to which user-specific configuration files should be stored. If $XDG_CONFIG_HOME is either not set or empty, a default equal to $HOME/.config should be used."
  },
  {
    "objectID": "posts/2022-01-09-dotfiles/index.html#gnu-stow",
    "href": "posts/2022-01-09-dotfiles/index.html#gnu-stow",
    "title": "Manage dotfiles with GNU Stow",
    "section": "GNU Stow",
    "text": "GNU Stow\nSome prominent results when googled for storing dotfiles are this Atlassian tutorial and using yadm. However, I found those harder to get started.\nGNU Stow on the other hand is an easy-to-use symlink farm manager. As described in their website, it takes distinct packages of software and/or data located in separate directories on the filesystem, and makes them appear to be installed in the same place.\nThis strategy works brilliantly for dotfiles. Borrowing explanation from Brandon Invergo’s article:\n\nThe procedure is simple. I created the ${HOME}/dotfiles directory and then inside it I made subdirectories for all the programs whose configurations I wanted to manage. Inside each of those directories, I moved in all the appropriate files, maintaining the directory structure of my home directory. So, if a file normally resides at the top level of your home directory, it would go into the top level of the program’s subdirectory. If a file normally goes in the default ${XDG_CONFIG_HOME}/${PKGNAME} location (${HOME}/.config/${PKGNAME}), then it would instead go in ${HOME}/dotfiles/${PKGNAME}/.config/${PKGNAME} and so on.\n\n\nInstall Stow\n\n\nTerminal\n\n1sudo apt stow\n\n2brew install stow\n\n\n1\n\nUbuntu\n\n2\n\nHomebrew Mac\n\n\n\n\nPlacing the files\nNow, it might look complex at first. Let me explain with some examples. - .bashrc or .zshrc are present/needed in $HOME directory, so inside $HOME/dotfiles create a subdirectory with bashrc or zshrc and place the original .bashrc or .zshrc file appropriately inside their folder. GNU Stow understands that the dotfile, when symlinked, will create a symlink-copy in the $HOME directory. For future modifications, file in either locations can be edited. But for simplicity, use $HOME/dotfiles directory. - A complicated example would be a config file located deep inside subfolders: nvim’s or neovim’s init.vim or init.lua file. It is present in $HOME/.config/nvim/init.vim. For Stow to understand, it must be placed like this – $HOME/dotfiles/nvim/.config/nvim/init.vim\nFor further reading, I recommend brilliantly written Jake Weisler’s post on GNU Stow.\n\n\nUseful Stow commands\nIf correctly installed, then running the command stow --help should list options to use Stow. Most used commands are\n\n\nTerminal\n\n1stow &lt;packagename&gt;\n2stow -n &lt;packagename&gt;\n3stow -D &lt;packagename&gt;\n4stow -R &lt;packagename&gt;\n\n\n1\n\nactivates symlink\n\n2\n\ntrial runs or simulates symlink generation. Effective for checking for errors\n\n3\n\ndelete stowed package\n\n4\n\nrestows package\n\n\n\n\nActivating Stow\nSo if we have created three subdirectories inside dotfiles say zsh, git, nvim, then\n\n\nTerminal\n\nstow bash git nvim\n\nwill activate their symlinks.\nIf returned to $HOME and $XDG_CONFIG_HOME and verified, then we will see,\n\n\nTerminal\n\n.gitconfig -&gt; .dotfiles/git/.gitconfig\n.zshrc -&gt; .dotfiles/zsh/.zshrc\nnvim -&gt; ../.dotfiles/nvim/.config/nvim\n\nThe most awesome thing in all this is, the directory structure needs to be created only once. For future requirement, one simply clones the dotfiles directory and activates symlinks."
  },
  {
    "objectID": "posts/2022-01-09-dotfiles/index.html#storing-files-in-git",
    "href": "posts/2022-01-09-dotfiles/index.html#storing-files-in-git",
    "title": "Manage dotfiles with GNU Stow",
    "section": "Storing files in Git",
    "text": "Storing files in Git\nThe dotfiles directory now becomes important to store in a remote location for safe keeping. Usually a git repository is the preferred method. For instructions on how to use git, look up various tutorials on Git in the internet.\nIn summary, I have written a short, albeit technical write up on GNU Stow, and its uses for storing dotfiles. Feel free to ask questions in the comments or via various means linked in the blog."
  },
  {
    "objectID": "notes/py_for_finance.html",
    "href": "notes/py_for_finance.html",
    "title": "LinkedIn course - Financial data analysis notes",
    "section": "",
    "text": "For financial data analysis, it is important to know certain terminologies, their purpose, and how to calculate them.\nI’m taking Matt Harrison’s Linkedin course on Python for Finance. Here I note down all the Pandas techniques and concepts explored in the course. As always if you find an error, don’t hesitate to contact me."
  },
  {
    "objectID": "notes/py_for_finance.html#chaining",
    "href": "notes/py_for_finance.html#chaining",
    "title": "LinkedIn course - Financial data analysis notes",
    "section": "Chaining",
    "text": "Chaining\nMatt introduces one of the features in Pandas called chaining. It allows reading the code as a recipe. One can simply go through from top to bottom and understand how the code works. We leverage pipe() pandas function. We can use it call any function.\nFrom the two stocks, PFE and JNJ, we need only PFE. So, we can try to use chaining principle.\n\nWithout chaining\n\ndef fix_cols(df):\n    cols = df.columns\n    outer = [col[0] for col in cols]\n    df.columns = outer\n    return df\n\npfe_df1 = pharma_df.iloc[:,1::2]\npfe_df1 = fix_cols(pfe_df1)\npfe_df1\n\n\n\n\n\n\n\n\nAdj Close\nClose\nHigh\nLow\nOpen\nVolume\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n2015-01-02\n21.793072\n29.724857\n30.151802\n29.620493\n29.667933\n16371571\n\n\n2015-01-05\n21.674822\n29.563566\n29.800758\n29.421251\n29.743834\n24786391\n\n\n2015-01-06\n21.855675\n29.810247\n30.227703\n29.525618\n29.667933\n29468681\n\n\n2015-01-07\n22.154793\n30.218216\n30.237192\n29.962049\n30.094877\n20248816\n\n\n2015-01-08\n22.606922\n30.834915\n30.967743\n30.569260\n30.683111\n49169522\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n2023-04-21\n39.779552\n40.209999\n40.299999\n39.910000\n40.090000\n19227100\n\n\n2023-04-24\n39.482765\n39.910000\n40.200001\n39.709999\n40.189999\n17633700\n\n\n2023-04-25\n38.908978\n39.330002\n39.919998\n39.279999\n39.750000\n24492400\n\n\n2023-04-26\n38.216469\n38.630001\n39.189999\n38.400002\n39.160000\n22401400\n\n\n2023-04-27\n38.325291\n38.740002\n38.830002\n38.310001\n38.619999\n22434000\n\n\n\n\n2094 rows × 6 columns\n\n\n\n\n\nWith chaining\n\npfe_df = (pharma_df\n .iloc[:,1::2] # retrieves only PFE stock data\n .pipe(fix_cols) # Removes the ticker and just shows \n)\npfe_df\n\n\n\n\n\n\n\n\nAdj Close\nClose\nHigh\nLow\nOpen\nVolume\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n2015-01-02\n21.793072\n29.724857\n30.151802\n29.620493\n29.667933\n16371571\n\n\n2015-01-05\n21.674822\n29.563566\n29.800758\n29.421251\n29.743834\n24786391\n\n\n2015-01-06\n21.855675\n29.810247\n30.227703\n29.525618\n29.667933\n29468681\n\n\n2015-01-07\n22.154793\n30.218216\n30.237192\n29.962049\n30.094877\n20248816\n\n\n2015-01-08\n22.606922\n30.834915\n30.967743\n30.569260\n30.683111\n49169522\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n2023-04-21\n39.779552\n40.209999\n40.299999\n39.910000\n40.090000\n19227100\n\n\n2023-04-24\n39.482765\n39.910000\n40.200001\n39.709999\n40.189999\n17633700\n\n\n2023-04-25\n38.908978\n39.330002\n39.919998\n39.279999\n39.750000\n24492400\n\n\n2023-04-26\n38.216469\n38.630001\n39.189999\n38.400002\n39.160000\n22401400\n\n\n2023-04-27\n38.325291\n38.740002\n38.830002\n38.310001\n38.619999\n22434000\n\n\n\n\n2094 rows × 6 columns\n\n\n\nAs you can see this makes an easier reading. We use the pipe() to call our fix_cols function. The resulting dataframe has only the outer level column names. Indeed, I agree that as more analysis are added, it gets complicated and harder to understand. Indeed, the intermediate calculation steps are not shown in the final version which makes it difficult to visualise the operation instantaneously."
  },
  {
    "objectID": "notes/py_for_finance.html#returns",
    "href": "notes/py_for_finance.html#returns",
    "title": "LinkedIn course - Financial data analysis notes",
    "section": "Returns",
    "text": "Returns\nHow much percentage of return can be expected? With pandas, we can simply use .pct_change() function and get the values. Plotting them is as simple as shown previously.\n\n(pfe_df\n .pct_change()\n .Close\n .plot()\n)\n\n\nHistogram can be an option but it doesn’t show negative swing. Somewhat appropriate would be to use bar plot.\n\n(pfe_df\n .pct_change()\n .Close\n .iloc[-100:] #last 100 rows\n .plot.bar()\n)\n\n\nThis plot shows the negative trends but the X-axis is illegible. We don’t know on which date the closing stock prices changed. This is because Pandas converts/groups whatever on the x-axis into categorical variables. For example, for categorical variable such as elephants, dogs and cats this works but for dates that isn’t correct.\nWhat if we explicitly say the x-axis as dates.\n\nfig, ax = plt.subplots(figsize=(10,4))\n(pfe_df\n .pct_change()\n .Close\n .iloc[-100:]\n .plot.bar(ax=ax)\n)\nax.xaxis.set_major_locator(dates.MonthLocator())\nax.xaxis.set_major_formatter(dates.DateFormatter('%b-%y'))\nax.xaxis.set_minor_locator(dates.DayLocator())\n\n\n1970?!? Still Pandas converts dates to categorical variables.\nThe solution Matt suggests is to use matplotlib.\n\ndef my_bar(series, ax):\n    ax.bar(series.index, series)\n    ax.xaxis.set_major_locator(dates.MonthLocator())\n    ax.xaxis.set_major_formatter(dates.DateFormatter('%b-%y'))\n    ax.xaxis.set_minor_locator(dates.DayLocator())\n    return series\n\nfig, ax = plt.subplots(figsize=(10,4))\n_ = ( pfe_df\n .pct_change()\n .Close\n .iloc[-100:]\n .pipe(my_bar, ax)\n)\n\n\nLooks good now."
  },
  {
    "objectID": "notes/py_for_finance.html#cumulative-returns",
    "href": "notes/py_for_finance.html#cumulative-returns",
    "title": "LinkedIn course - Financial data analysis notes",
    "section": "Cumulative returns",
    "text": "Cumulative returns\nCumulative returns shows the investment amount gained or lost over time. The formula is given by \\[\ncumulative\\_return = \\frac{(current\\_price - original\\_price)}{(curent\\_price)}\n\\]\n\n(pfe_df\n .Close\n .sub(pfe_df.Close[0])\n .div(pfe_df.Close[0])\n .plot()\n)\n\n&lt;AxesSubplot:xlabel='Date'&gt;\n\n\n\n\n\nAnother alternate way is to numpy’s cumprod function.\n\n(pfe_df\n .Close\n .add(1)\n .cumprod()\n .sub(1)\n .plot()\n)\n\n/Users/dross/miniforge3/envs/mlops/lib/python3.9/site-packages/numpy/core/fromnumeric.py:57: RuntimeWarning:\n\noverflow encountered in accumulate\n\n\n\n&lt;AxesSubplot:xlabel='Date'&gt;\n\n\n\n\n\nAs you can see both plots give the same result.\nIf we’re to use .pipe here, we can do like so:\n\ndef calc_cumrets(df, col):\n     ser = df[col]\n     return (ser\n             .sub(ser[0])\n             .div(ser[0])\n            )\n(pfe_df\n .pipe(calc_cumrets,'Close')\n .plot()\n)\n\n&lt;AxesSubplot:xlabel='Date'&gt;\n\n\n\n\n\n\nLambda functions or anonymous functions\nUsing lambda functions we can make impropmtu functions and use it with our chaining.\nWe would traditionally call a function like so:\n\ndef get_returns(df):\n    return calc_cumrets(df, 'Close')\n\nget_returns(pfe_df)\n\nDate\n2015-01-02    0.000000\n2015-01-05   -0.005426\n2015-01-06    0.002873\n2015-01-07    0.016598\n2015-01-08    0.037344\n                ...   \n2023-04-21    0.352740\n2023-04-24    0.342647\n2023-04-25    0.323135\n2023-04-26    0.299586\n2023-04-27    0.303286\nName: Close, Length: 2094, dtype: float64\n\n\nHowever, if we are to use lambda, then the above code can be written as:\n\n(lambda df: get_returns(df))(pfe_df)\n\nDate\n2015-01-02    0.000000\n2015-01-05   -0.005426\n2015-01-06    0.002873\n2015-01-07    0.016598\n2015-01-08    0.037344\n                ...   \n2023-04-21    0.352740\n2023-04-24    0.342647\n2023-04-25    0.323135\n2023-04-26    0.299586\n2023-04-27    0.303286\nName: Close, Length: 2094, dtype: float64\n\n\nNow, with cumulative returns calculation, it would be useful if those values can be assigned to a new column in the dataframe. It is here that a Pandas feature in .assign function is helpful. It helps create new columns. We can couple .assign and lambda together.\n\npfe_df = (pfe_df\n .assign(cum_rets=lambda df:calc_cumrets(df, 'Close'))\n)"
  },
  {
    "objectID": "notes/py_for_finance.html#volatility",
    "href": "notes/py_for_finance.html#volatility",
    "title": "LinkedIn course - Financial data analysis notes",
    "section": "Volatility",
    "text": "Volatility\nVolatility is a statistical measure of the dispertion of the returns for a given market index in this case stocks. In most cases, higher the volatility, the riskier the stock. It is often measured from either standard deviation or variance between returns from that stock. Remember standard deviation is the measure of deviation of the data relative to its mean.\nJust like mean(), we can calculate std().\n\n(pfe_df\n .Close\n #.mean()\n .std()\n)\n\n6.69077286484766\n\n\nThe .assign() allows consective chaining methods to use these newly created columns. In the below code block, we can use the pct_change_close created in the first line in to the second line. Then, we can calculate 30 day rolling volatility. Rolling is nothing but a time frame in which the volatility is calculated. We can see that for the first 15 days the volatility is NaN(not a number) and on the 30th day, there is an entry.\n\n(pfe_df\n .assign(pct_change_close=pfe_df.Close.pct_change())\n .pct_change_close\n .rolling(30)\n .std()\n)\n\nDate\n2015-01-02         NaN\n2015-01-05         NaN\n2015-01-06         NaN\n2015-01-07         NaN\n2015-01-08         NaN\n                ...   \n2023-04-21    0.008945\n2023-04-24    0.009058\n2023-04-25    0.009192\n2023-04-26    0.009718\n2023-04-27    0.009543\nName: pct_change_close, Length: 2094, dtype: float64\n\n\n\n#rolling volatility\n(pfe_df\n .assign(close_volatility=pfe_df.rolling(30).Close.std(),\n         percent_volatility=pfe_df.Close.pct_change().rolling(30).std())\n .iloc[:,-2:] # fetch only the last two columns\n .plot(subplots=True)\n\n)\n\narray([&lt;AxesSubplot:xlabel='Date'&gt;, &lt;AxesSubplot:xlabel='Date'&gt;],\n      dtype=object)\n\n\n\n\n\nWe can also use .resample to calculate 15 day volatility as we have date as index.\n\n# 15 day volatility\n(pfe_df\n .assign(pct_change_close=pfe_df.Close.pct_change())\n .resample('15D')\n .std()\n)\n\n\n\n\n\n\n\n\nAdj Close\nClose\nHigh\nLow\nOpen\nVolume\ncum_rets\npct_change_close\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n2015-01-02\n0.423588\n0.577756\n0.554235\n0.536322\n0.565220\n8.716662e+06\n0.019437\n0.009335\n\n\n2015-01-17\n0.408931\n0.557768\n0.552896\n0.473576\n0.428281\n6.992238e+06\n0.018764\n0.011556\n\n\n2015-02-01\n0.864666\n1.092056\n1.124398\n1.137237\n1.027460\n1.347935e+07\n0.036739\n0.013012\n\n\n2015-02-16\n0.103079\n0.139368\n0.153115\n0.158683\n0.142704\n5.436585e+06\n0.004689\n0.006524\n\n\n2015-03-03\n0.232107\n0.313819\n0.310265\n0.285296\n0.329080\n6.907642e+06\n0.010557\n0.008226\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2023-02-19\n0.912909\n0.922787\n0.942723\n1.064260\n1.025597\n4.710137e+06\n0.031044\n0.011505\n\n\n2023-03-06\n0.510382\n0.515904\n0.386680\n0.472090\n0.610293\n1.148802e+07\n0.017356\n0.011168\n\n\n2023-03-21\n0.414519\n0.419005\n0.428108\n0.305953\n0.396622\n3.416600e+06\n0.014096\n0.009084\n\n\n2023-04-05\n0.495242\n0.500601\n0.483455\n0.430794\n0.424291\n4.807599e+06\n0.016841\n0.008860\n\n\n2023-04-20\n0.647205\n0.654208\n0.613951\n0.713320\n0.629458\n2.507162e+06\n0.022009\n0.010312\n\n\n\n\n203 rows × 8 columns\n\n\n\n\n# 15 day rolling volatility\n(pfe_df\n .assign(pct_change_close=pfe_df.Close.pct_change())\n .rolling(window=15, min_periods=15)\n .std()\n)\n\n\n\n\n\n\n\n\nAdj Close\nClose\nHigh\nLow\nOpen\nVolume\ncum_rets\npct_change_close\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n2015-01-02\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2015-01-05\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2015-01-06\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2015-01-07\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2015-01-08\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2023-04-21\n0.591463\n0.597864\n0.638385\n0.562327\n0.596404\n3.967966e+06\n0.020113\n0.009691\n\n\n2023-04-24\n0.657297\n0.664410\n0.695511\n0.616351\n0.626652\n3.915626e+06\n0.022352\n0.009383\n\n\n2023-04-25\n0.771667\n0.780017\n0.784388\n0.725364\n0.709656\n4.093416e+06\n0.026241\n0.008983\n\n\n2023-04-26\n0.957201\n0.967559\n0.939399\n0.930587\n0.843585\n4.133347e+06\n0.032551\n0.009559\n\n\n2023-04-27\n1.047608\n1.058944\n1.047459\n1.068349\n1.011878\n3.287318e+06\n0.035625\n0.008130\n\n\n\n\n2094 rows × 8 columns\n\n\n\nWhat happens if the assinged new column name is same as the pandas function name and we have to use further for our analysis? We can include that ‘assigned’ column within [](square) braces and use it. In the below example, we can see how pct_change conflicts with pandas and is therefore must be put inside [] to access it.\n\n# 15 day rolling volatility\n(pfe_df\n .assign(pct_change=pfe_df.Close.pct_change())\n .rolling(window=15, min_periods=15)\n .std()\n #.pct_change\n [\"pct_change\"]\n .plot()\n)\n\n&lt;AxesSubplot:xlabel='Date'&gt;"
  },
  {
    "objectID": "notes/py_for_finance.html#moving-averages-or-rolling-windows",
    "href": "notes/py_for_finance.html#moving-averages-or-rolling-windows",
    "title": "LinkedIn course - Financial data analysis notes",
    "section": "Moving averages or rolling windows",
    "text": "Moving averages or rolling windows\nMoving average(MA) of a stock is calculated to help smooth out the price data by creating a constantly updated average price. It helps to mitigate the impacts of random, short-term fluctuations on the prices of the stock over a time period. There are two types of moving averages - simple which is just the arithmetic mean of the given prices over a specified number of days and exponential which is the weighted average that gives significance to the recent prices than old ones, making it an indicator that is more responsive to new infotmation.\nMA is used to identify the tread direction of a stock or to determine its support and resistance level as it depends on the past prices. The longer the period for the MA, the greater the lag. A 200-day MA has much greater lag than 20-day MA. The gold standard used by investers are 50-day and 200-day MAs.\nShorter MA for short-term investment and longer MA for long-term. A rising MA means upward trend and declining means downward trend.\n\nWhat is a Golden Cross?\nA golden cross is a chart pattern in which a short-term moving average crosses above a long-term moving average. The golden cross is a bullish breakout pattern formed from a crossover involving a security’s short-term moving average such as the 15-day moving average, breaking above its long-term moving average, such as the 50-day moving average. As long-term indicators carry more weight, the golden cross indicates a bull market on the horizon and is reinforced by high trading volumes.\n\n\nLag\nThese lags can be calculated in Pandas using shift function. shift(1) means shift index one place down. shift(2) means two places down. For example,\n\n(pfe_df\n .assign(s1=pfe_df.Close.shift(1),\n         s2=pfe_df.Close.shift(2))\n [[\"s1\",\"s2\"]]\n)\n\n\n\n\n\n\n\n\ns1\ns2\n\n\nDate\n\n\n\n\n\n\n2015-01-02\nNaN\nNaN\n\n\n2015-01-05\n29.724857\nNaN\n\n\n2015-01-06\n29.563566\n29.724857\n\n\n2015-01-07\n29.810247\n29.563566\n\n\n2015-01-08\n30.218216\n29.810247\n\n\n...\n...\n...\n\n\n2023-04-21\n39.849998\n40.240002\n\n\n2023-04-24\n40.209999\n39.849998\n\n\n2023-04-25\n39.910000\n40.209999\n\n\n2023-04-26\n39.330002\n39.910000\n\n\n2023-04-27\n38.630001\n39.330002\n\n\n\n\n2094 rows × 2 columns\n\n\n\nthe Close value in the first row will be on the second row for shift(1) and two rows down for shift(2).\nNow for simple 3-day moving average, we need to average Close, s1, and s2. We can do it manually using a lambda and use the rolling pandas with window=3 specified.\n\n(pfe_df\n .assign(s1=pfe_df.Close.shift(1),\n         s2=pfe_df.Close.shift(2),\n         ma3=lambda df_:df_.loc[:,[\"Close\", \"s1\", \"s2\"]].mean(axis='columns'),\n         ma3_builtin=pfe_df.Close.rolling(3).mean()\n        )\n[[\"s1\",\"s2\",\"ma3\",\"ma3_builtin\"]]\n)\n\n\n\n\n\n\n\n\ns1\ns2\nma3\nma3_builtin\n\n\nDate\n\n\n\n\n\n\n\n\n2015-01-02\nNaN\nNaN\n29.724857\nNaN\n\n\n2015-01-05\n29.724857\nNaN\n29.644212\nNaN\n\n\n2015-01-06\n29.563566\n29.724857\n29.699557\n29.699557\n\n\n2015-01-07\n29.810247\n29.563566\n29.864010\n29.864010\n\n\n2015-01-08\n30.218216\n29.810247\n30.287793\n30.287793\n\n\n...\n...\n...\n...\n...\n\n\n2023-04-21\n39.849998\n40.240002\n40.100000\n40.100000\n\n\n2023-04-24\n40.209999\n39.849998\n39.989999\n39.989999\n\n\n2023-04-25\n39.910000\n40.209999\n39.816667\n39.816667\n\n\n2023-04-26\n39.330002\n39.910000\n39.290001\n39.290001\n\n\n2023-04-27\n38.630001\n39.330002\n38.900002\n38.900002\n\n\n\n\n2094 rows × 4 columns"
  },
  {
    "objectID": "notes/py_for_finance.html#plotting-mas",
    "href": "notes/py_for_finance.html#plotting-mas",
    "title": "LinkedIn course - Financial data analysis notes",
    "section": "Plotting MAs",
    "text": "Plotting MAs\nWe are getting comfortable with plotting. We select the columns needed to plotted - [‘Close’, ‘ma3_builtin’] for last 200 rows.\n\n(pfe_df\n .assign(s1=pfe_df.Close.shift(1),\n         s2=pfe_df.Close.shift(2),\n         ma3=lambda df_:df_.loc[:,[\"Close\", \"s1\", \"s2\"]].mean(axis='columns'),\n         ma3_builtin=pfe_df.Close.rolling(3).mean()\n        )\n [['Close', 'ma3_builtin']]\n .iloc[-200:]\n .plot()\n)\n\n&lt;AxesSubplot:xlabel='Date'&gt;\n\n\n\n\n\nAs we can see the MA smoothes out the little peaks and troughs.\n\nGolden Cross\nSome experts say if there is a crossover between MA-50 and MA-200, it is an indicator to buy or sell.\n\n(pfe_df\n .assign(ma50=pfe_df.Close.rolling(50).mean(),\n         ma200=pfe_df.Close.rolling(200).mean()\n        )\n [[\"Close\",\"ma50\",\"ma200\"]]\n .iloc[-650:]\n .plot()\n)\n\n&lt;AxesSubplot:xlabel='Date'&gt;"
  },
  {
    "objectID": "notes/py_for_finance.html#obv--on-balance-volume",
    "href": "notes/py_for_finance.html#obv--on-balance-volume",
    "title": "LinkedIn course - Financial data analysis notes",
    "section": "OBV- On-balance Volume",
    "text": "OBV- On-balance Volume\nOBV is one such used for technical analysis. It is a momentum indicator that uses volume to predict changes in stock price.\n\nWhat Does On-Balance Volume Tell You?\nThe actual value of the OBV is unimportant; concentrate on its direction. (source: fidelity)\n\nWhen both price and OBV are making higher peaks and higher troughs, the upward trend is likely to continue.\nWhen both price and OBV are making lower peaks and lower troughs, the downward trend is likely to continue.\nDuring a trading range, if the OBV is rising, accumulation may be taking place—a warning of an upward breakout.\nDuring a trading range, if the OBV is falling, distribution may be taking place—a warning of a downward breakout.\nWhen price continues to make higher peaks and OBV fails to make higher peaks, the upward trend is likely to stall or fail. This is called a negative divergence.\nWhen price continues to make lower troughs and OBV fails to make lower troughs, the downward trend is likely to stall or fail. This is called a positive divergence.\n\n\n\nOBV calculation\nIf today’s close is greater than yesterday’s close then: OBV = Yesterday’s OBV + Today’s Volume\nIf today’s close is less than yesterday’s close then: OBV = Yesterday’s OBV – Today’s Volume\nIf today’s close is equal to yesterday’s close then: OBV = Yesterday’s OBV\n\\[\nOBV = OBV_{prev} + \\begin{cases}\nvolume, \\text{ if close &gt; close}_{prev} \\\\\n0, \\text{ if close = close}_{prev}\\\\\n-volume, \\text{ if close &lt; close}_{prev}\n\\end{cases}\n\\]\nwhere\nOBV = current on-balance volume level\nOBVprev = previous on-balance volume level\nvolume = Latest trading volume amount"
  },
  {
    "objectID": "notes/py_for_finance.html#accumulation-distribution-indicator",
    "href": "notes/py_for_finance.html#accumulation-distribution-indicator",
    "title": "LinkedIn course - Financial data analysis notes",
    "section": "Accumulation distribution indicator",
    "text": "Accumulation distribution indicator"
  },
  {
    "objectID": "notes/py_for_finance.html#rsi---relative-strength-index",
    "href": "notes/py_for_finance.html#rsi---relative-strength-index",
    "title": "LinkedIn course - Financial data analysis notes",
    "section": "RSI - Relative strength index",
    "text": "RSI - Relative strength index"
  }
]