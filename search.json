[
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Archive",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\n\n\n\n\n \n\n\nData Engineering concepts/tools notes\n\n\n\n\n \n\n\n📚 Deepak’s Notes\n\n\n\n\nSep 13, 2023\n\n\nNotes on Data Analytics\n\n\n\n\nSep 13, 2023\n\n\nNotes on Pandas\n\n\n\n\nSep 11, 2023\n\n\nUseful things found while using Linux\n\n\n\n\nSep 8, 2023\n\n\nKafka and Kafka connect command cheatsheet\n\n\n\n\nSep 2, 2023\n\n\nTesting Data Pipelines\n\n\n\n\nAug 30, 2023\n\n\nStoring secrets in JSON files\n\n\n\n\nAug 30, 2023\n\n\nnotes on Kafka Connect\n\n\n\n\nAug 29, 2023\n\n\nChange Data Capture with Debezium, Kafka, S3\n\n\n\n\nAug 24, 2023\n\n\nPostgres - notes\n\n\n\n\nAug 23, 2023\n\n\nApache Kafka notes\n\n\n\n\nAug 20, 2023\n\n\nDebezium\n\n\n\n\nJun 13, 2023\n\n\nBuilding Data Pipeline - Part 4 - CI/CD\n\n\n\n\nJun 12, 2023\n\n\nTerraform - Attach IAM policies to a role\n\n\n\n\nJun 11, 2023\n\n\nBuilding Data Pipelines - Part 3 - Terraform\n\n\n\n\nJun 4, 2023\n\n\nBuilding Data Pipelines - Part 2 - AWS Cloud\n\n\n\n\nJun 3, 2023\n\n\nDeploy Web Application with Docker\n\n\n\n\nJun 3, 2023\n\n\nBuilding Data Pipelines - Part 1 - Docker\n\n\n\n\nJun 2, 2023\n\n\nGetting started with Terraform\n\n\n\n\nMay 10, 2023\n\n\nLinkedIn course - Financial data analysis\n\n\n\n\nApr 27, 2023\n\n\nGetting started with S3 using boto3\n\n\n\n\nFeb 6, 2023\n\n\nSetup Skim PDF reader with VimTeX in Mac OS\n\n\n\n\nSep 24, 2022\n\n\nUsing json_normalize Pandas function\n\n\n\n\nSep 7, 2022\n\n\nPython concepts\n\n\n\n\nJan 9, 2022\n\n\nManage dotfiles with GNU Stow\n\n\n\n\nSep 18, 2021\n\n\nSetting up Kaggle on Linux/Mac\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-08-30-secrets-in-json/index.html",
    "href": "posts/2023-08-30-secrets-in-json/index.html",
    "title": "Storing secrets in JSON files",
    "section": "",
    "text": "Introduction\nUnlike other file formats it is only possible to hardcode sensitive information in JSON files. In this post I explore one of the two ways we can eliminate hardcoded secrets inside JSON.\nHere is how a JSON configuration file looks like:\n\n\npg-src-connections.json\n\n{\n    \"name\": \"pg-src-connector\",\n    \"config\": {\n        \"connector.class\": \"io.debezium.connector.postgresql.PostgresConnector\",\n        \"tasks.max\": \"1\",\n        \"database.hostname\": \"postgres\",\n        \"database.port\": \"5432\",\n        \"database.user\": \"postgres\",\n        \"database.password\": \"postgres\",\n        \"database.dbname\": \"db\",\n        \"database.server.name\": \"postgres\",\n        \"database.include.list\": \"postgres\",\n        \"topic.prefix\": \"debezium\",\n        \"schema.include.list\": \"commerce\"\n    }\n}\n\ndatabase.user, database.password and database.dbname are exposed to others when the author commits this file to a code repository. Anyone with access to the system can use these credentials to enter into the database. This is potentially a security risk considering this JSON object file will be sent as POST request.\nIf proper API security protocols aren’t followed, there are possibilites of sensitive data exposure, injection attacks, session data hijack etc. To prevent potential security breach, we’ve to exercise caution whenever there is login credentials involved.\nAnother example is with a connector connecting Kafka to AWS S3 storage.\n\n\ns3-sink.json\n\n{\n    \"name\": \"s3-sink\",\n    \"config\": {\n        \"connector.class\": \"io.aiven.kafka.connect.s3.AivenKafkaConnectS3SinkConnector\",\n        \"aws.access.key.id\": \"&lt;replace with key id&gt;\",\n        \"aws.secret.access.key\": \"&lt;replace with secret access key&gt;\",\n        \"aws.s3.bucket.name\": \"&lt;replace bucket name&gt;\",\n        \"aws.s3.endpoint\": \"&lt;insert endpoint&gt;\",\n        \"aws.s3.region\": \"us-east-1\",\n        \"format.output.type\": \"jsonl\",\n        \"topics\": \"debezium.commerce.users,debezium.commerce.products\",\n        \"file.compression.type\": \"none\",\n        \"flush.size\": \"20\",\n        \"file.name.template\": \"/{{topic}}/{{timestamp:unit=yyyy}}-{{timestamp:unit=MM}}-{{timestamp:unit=dd}}/{{timestamp:unit=HH}}/{{partition:padding=true}}-{{start_offset:padding=true}}.json\"\n    }\n}\n\nThe configuration contains properties for aws.access.key.id and aws.secret.access.key. Imagine if we hardcode those values into file that has approved access to several AWS services including programmatic access. What would happen if we’re to push this file into a repository that allows a greater number of people to view the file? I leave the next possibly horrorifying consequences for you to decide.\n\n\nMethods used\n\n\nPutting config.json as data argument in a curl command and placing that command inside a shell script.\n\n\nUsing secrets properties file which is accessed by config.json file.\n\n\n\n\n\n\n\n\nNote\n\n\n\nI couldn’t get the second method to work and hence only the first solution is explored. When I figure out the second, I will add it in.\n\n\n\n\nUsing JSON directly in CURL\nIn most of the modern data pipelines, using HTTP/s REST API requests are common. These requests: POST, GET, PUT and DELETE are sent usually with JSON objects as data in request’s body field. There are REST API client available to make our task easier but as always there is good old CURL command.\nIf we want to send s3-sink.json as payload, the curl command will look like this:\ncurl -i -X POST -H \"Accept:application/json\" -H \"Content-Type:application/json\" localhost:8083/connectors/ -d '@./s3-sink.json'\nWe already discussed the disadvantages of sending payload like that. What if we expand/unpack '@./s3-sink.json'? It would look like this:\n\n\nCURL command with unpacked JSON as data payload\n\ncurl --include --request POST --header \"Accept:application/json\" \\\n    --header \"Content-Type:application/json\" \\\n    --url localhost:8083/connectors/ \\\n    --data '{\n        \"name\": \"s3-sink\",\n        \"config\": {\n            \"connector.class\": \"io.aiven.kafka.connect.s3.AivenKafkaConnectS3SinkConnector\",\n            \"aws.access.key.id\": \"&lt; &gt;\",\n            \"aws.secret.access.key\": \"&lt; &gt;\",\n            \"aws.s3.bucket.name\": \"&lt; &gt;\",\n            \"aws.s3.endpoint\": \"&lt; &gt;\",\n            \"aws.s3.region\": \"us-east-1\",\n            \"format.output.type\": \"jsonl\",\n            \"topics\": \"debezium.commerce.users,debezium.commerce.products\",\n            \"file.compression.type\": \"none\",\n            \"flush.size\": \"20\",\n            \"file.name.template\": \"/{{topic}}/{{timestamp:unit=yyyy}}-{{timestamp:unit=MM}}-{{timestamp:unit=dd}}/{{timestamp:unit=HH}}/{{partition:padding=true}}-{{start_offset:padding=true}}.json\"\n        }\n    }'\n\nAny Unix command allows environment variable to imported into the command which gets replaced at runtime.\nIf our env variables are supplied with values in the shell terminal prior to sending the CURL POST request, the command will replace the placeholder with necessary values at runtime.\nLet us see that in action.\n\n\nenv variable in shell terminal\n\nexport POSTGRES_USER=postgres\nexport POSTGRES_PASSWORD=postgres\nexport POSTGRES_DB=cdc-demo-db\nexport POSTGRES_HOST=postgres\nexport AWS_KEY_ID=minio\nexport AWS_SECRET_KEY=minio123\nexport AWS_BUCKET_NAME=commerce\n\nTo use env variables in JSON, the variables are placed inside the quotes in a unique way – \"'\"${AWS_SECRET_KEY}\"'\".\nThe order is important. The variable ${AWS_SECRET_KEY} is first encased in double \", followed by single ' and then ended with double \" quotes.\nThis way the placeholder env variables are replaced with actual values at runtime.\n\n\ncurl command with masked JSON file\n\ncurl --include --request POST --header \"Accept:application/json\" \\\n    --header \"Content-Type:application/json\" \\\n    --url localhost:8083/connectors/ \\\n    --data '{\n        \"name\": \"s3-sink\",\n        \"config\": {\n            \"connector.class\": \"io.aiven.kafka.connect.s3.AivenKafkaConnectS3SinkConnector\",\n            \"aws.access.key.id\": \"'\"${AWS_KEY_ID}\"'\",\n            \"aws.secret.access.key\": \"'\"${AWS_SECRET_KEY}\"'\",\n            \"aws.s3.bucket.name\": \"'\"${AWS_BUCKET_NAME}\"'\",\n            \"aws.s3.endpoint\": \"http://minio:9000\",\n            \"aws.s3.region\": \"us-east-1\",\n            \"format.output.type\": \"jsonl\",\n            \"topics\": \"debezium.commerce.users,debezium.commerce.products\",\n            \"file.compression.type\": \"none\",\n            \"flush.size\": \"20\",\n            \"file.name.template\": \"/{{topic}}/{{timestamp:unit=yyyy}}-{{timestamp:unit=MM}}-{{timestamp:unit=dd}}/{{timestamp:unit=HH}}/{{partition:padding=true}}-{{start_offset:padding=true}}.json\"\n        }\n    }'\n\nThis command can be put into a shell script and the script can be used to execute multiple REST API requests securely.\nThere you have it. A method that allows sensitive variables to masked in JSON payload.\n\n\nConclusion\nWe saw how unpacking a JSON configuration file and using it in CURL command helps avert some of the basic security breaches."
  },
  {
    "objectID": "posts/2023-06-03-lambda-docker/index.html",
    "href": "posts/2023-06-03-lambda-docker/index.html",
    "title": "Building Data Pipelines - Part 1 - Docker",
    "section": "",
    "text": "This post is a follow up to my previous post on deploying with docker. This will be part 1 of the deploy using Docker series. In this part, I will introduce the concept of lambda_function and how it can tested locally.\n\n\nThe customer from my previous post doesn’t want to keep things in their local machine and wish for a cloud option. Also don’t want to reserve resources as they expect only intermittent requests. What could be a viable solution? Enter Lambda_function. Each cloud provider has other own name for lambda function. We will use AWS and it is called Lambda.\nAWS Lambda is a serverless compute service that run code in reponse to an event and automatically manages the underlying compute resources for us. Moreover we pay only for what we use.1\nBefore transitioning into AWS Lambda cloud solution, we can modify our program to AWS Lambda coding standards and test it locally.\n\n\n\nLambda function handler has certain coding standards. 2 For example, the name of the python file must be lambda_function.py and the handler function lambda_handler.\nGoing by that standard, we can modify our flask-app.py from previous post to lambda_function.py.\n\n\nlambda_function.py\n\nimport os\nimport pandas as pd\n\ndef read_parquet_files(filename: str):\n    \"\"\"\n    Read parquet file format for given filename and returns the contents\n    \"\"\"\n    df = pd.read_parquet(filename, engine=\"pyarrow\")\n    return df\n\ndf_test_preds = read_parquet_files(\"lgb_preds.parquet\")\ndf_items = read_parquet_files(\"items.parquet\")\n\ndef predict(find, item_idx: int):\n    \"\"\"\n    Takes the json inputs, processes it and outputs the unit sales\n    \"\"\"\n    try:\n        idx = pd.IndexSlice\n        x = df_test_preds.loc[idx[find[\"store_nbr\"], item_idx, find[\"date1\"]]][\n            \"unit_sales\"\n        ]\n    except KeyError:\n        print(\"This item is not present this store. Try some other item\")\n        return -0.0\n    else:\n        return float(round(x, 2))\n\n1def lambda_handler(event, context=None) -&gt; dict:\n    \"\"\"\n    lambda handler for predict method\n    \"\"\"\n\n    find = event[\"find\"]\n    item = df_items.sample(1)\n    item_idx, item_family = item.index[0], item[\"family\"].values[0]\n    pred_unit_sales = predict(find, item_idx)\n\n    result = {\n        \" Store\": find[\"store_nbr\"],\n        \" item\": int(item_idx),\n        \"Family\": item_family,\n        \"Prediction date\": find[\"date1\"],\n        \"Unit_sales\": pred_unit_sales,\n    }\n    return result\n\n\n1\n\nDefault handler function name\n\n\nLike last time we will use Pipenv to install dependencies. If you’re new, kindly visit deploy with docker post to know why we use Pipenv.\n\n\n\n\n\nDockerfile\n\n1FROM public.ecr.aws/lambda/python:3.9\n\nRUN pip install -U pip\nRUN pip install pipenv\n\nCOPY [ \"Pipfile\", \"Pipfile.lock\", \"./\" ]\n\nRUN pipenv install --system --deploy\n\nCOPY [ \"lgb_preds.parquet\" , \"lgb_preds.parquet\" ]\nCOPY [ \"items.parquet\" , \"lambda_function.py\", \"./\" ]\n\n2CMD [ \"lambda_function.lambda_handler\" ]\n\n\n1\n\nUsing AWS ECR hosted python-3.9 docker image. As to why I will explain in the next part.\n\n2\n\nPay close attention as to how the handler function is invoked. It is not entrypoint like last time.\n\n\n\n\n\nWe use the same build command as last time.\n\n\nTerminal 1\n\ndocker build -t lambda-app:v1 .\n\nThe result if successful would look like this:\n\nRunning the container is different this time as the port exposed is different. AWS allows local testing of Lambda function code through a localhost endpoint. This endpoint is at 9000 port(it can be customised). Inside docker container we make it listen to 8080 port. So we forward 9000 to 8080.\n\n\nTerminal 1\n\ndocker run \\\n    -it \\\n    --rm \\\n1    -p 9000:8080 \\\n    lambda-app:v1\n\n\n1\n\nport forwarding 9000 to 8080\n\n\nResult looks like so:"
  },
  {
    "objectID": "posts/2023-06-03-lambda-docker/index.html#why-lambda_function",
    "href": "posts/2023-06-03-lambda-docker/index.html#why-lambda_function",
    "title": "Building Data Pipelines - Part 1 - Docker",
    "section": "",
    "text": "The customer from my previous post doesn’t want to keep things in their local machine and wish for a cloud option. Also don’t want to reserve resources as they expect only intermittent requests. What could be a viable solution? Enter Lambda_function. Each cloud provider has other own name for lambda function. We will use AWS and it is called Lambda.\nAWS Lambda is a serverless compute service that run code in reponse to an event and automatically manages the underlying compute resources for us. Moreover we pay only for what we use.1\nBefore transitioning into AWS Lambda cloud solution, we can modify our program to AWS Lambda coding standards and test it locally."
  },
  {
    "objectID": "posts/2023-06-03-lambda-docker/index.html#aws-lambda-handler",
    "href": "posts/2023-06-03-lambda-docker/index.html#aws-lambda-handler",
    "title": "Building Data Pipelines - Part 1 - Docker",
    "section": "",
    "text": "Lambda function handler has certain coding standards. 2 For example, the name of the python file must be lambda_function.py and the handler function lambda_handler.\nGoing by that standard, we can modify our flask-app.py from previous post to lambda_function.py.\n\n\nlambda_function.py\n\nimport os\nimport pandas as pd\n\ndef read_parquet_files(filename: str):\n    \"\"\"\n    Read parquet file format for given filename and returns the contents\n    \"\"\"\n    df = pd.read_parquet(filename, engine=\"pyarrow\")\n    return df\n\ndf_test_preds = read_parquet_files(\"lgb_preds.parquet\")\ndf_items = read_parquet_files(\"items.parquet\")\n\ndef predict(find, item_idx: int):\n    \"\"\"\n    Takes the json inputs, processes it and outputs the unit sales\n    \"\"\"\n    try:\n        idx = pd.IndexSlice\n        x = df_test_preds.loc[idx[find[\"store_nbr\"], item_idx, find[\"date1\"]]][\n            \"unit_sales\"\n        ]\n    except KeyError:\n        print(\"This item is not present this store. Try some other item\")\n        return -0.0\n    else:\n        return float(round(x, 2))\n\n1def lambda_handler(event, context=None) -&gt; dict:\n    \"\"\"\n    lambda handler for predict method\n    \"\"\"\n\n    find = event[\"find\"]\n    item = df_items.sample(1)\n    item_idx, item_family = item.index[0], item[\"family\"].values[0]\n    pred_unit_sales = predict(find, item_idx)\n\n    result = {\n        \" Store\": find[\"store_nbr\"],\n        \" item\": int(item_idx),\n        \"Family\": item_family,\n        \"Prediction date\": find[\"date1\"],\n        \"Unit_sales\": pred_unit_sales,\n    }\n    return result\n\n\n1\n\nDefault handler function name\n\n\nLike last time we will use Pipenv to install dependencies. If you’re new, kindly visit deploy with docker post to know why we use Pipenv."
  },
  {
    "objectID": "posts/2023-06-03-lambda-docker/index.html#dockerfile",
    "href": "posts/2023-06-03-lambda-docker/index.html#dockerfile",
    "title": "Building Data Pipelines - Part 1 - Docker",
    "section": "",
    "text": "Dockerfile\n\n1FROM public.ecr.aws/lambda/python:3.9\n\nRUN pip install -U pip\nRUN pip install pipenv\n\nCOPY [ \"Pipfile\", \"Pipfile.lock\", \"./\" ]\n\nRUN pipenv install --system --deploy\n\nCOPY [ \"lgb_preds.parquet\" , \"lgb_preds.parquet\" ]\nCOPY [ \"items.parquet\" , \"lambda_function.py\", \"./\" ]\n\n2CMD [ \"lambda_function.lambda_handler\" ]\n\n\n1\n\nUsing AWS ECR hosted python-3.9 docker image. As to why I will explain in the next part.\n\n2\n\nPay close attention as to how the handler function is invoked. It is not entrypoint like last time."
  },
  {
    "objectID": "posts/2023-06-03-lambda-docker/index.html#docker-image-build-and-running-the-container",
    "href": "posts/2023-06-03-lambda-docker/index.html#docker-image-build-and-running-the-container",
    "title": "Building Data Pipelines - Part 1 - Docker",
    "section": "",
    "text": "We use the same build command as last time.\n\n\nTerminal 1\n\ndocker build -t lambda-app:v1 .\n\nThe result if successful would look like this:\n\nRunning the container is different this time as the port exposed is different. AWS allows local testing of Lambda function code through a localhost endpoint. This endpoint is at 9000 port(it can be customised). Inside docker container we make it listen to 8080 port. So we forward 9000 to 8080.\n\n\nTerminal 1\n\ndocker run \\\n    -it \\\n    --rm \\\n1    -p 9000:8080 \\\n    lambda-app:v1\n\n\n1\n\nport forwarding 9000 to 8080\n\n\nResult looks like so:"
  },
  {
    "objectID": "posts/2023-06-03-lambda-docker/index.html#footnotes",
    "href": "posts/2023-06-03-lambda-docker/index.html#footnotes",
    "title": "Building Data Pipelines - Part 1 - Docker",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://aws.amazon.com/lambda/features/↩︎\nhttps://docs.aws.amazon.com/lambda/latest/dg/python-handler.html↩︎"
  },
  {
    "objectID": "posts/2023-04-27-getting-started-with-s3/index.html",
    "href": "posts/2023-04-27-getting-started-with-s3/index.html",
    "title": "Getting started with S3 using boto3",
    "section": "",
    "text": "Boto3 is an AWS python SDK that allows access to AWS services like EC2 and S3. It provides a python object-oriented API and as well as low-level access to AWS services\nimport boto3, botocore\nimport glob\n\nfiles = glob.glob('data/*') #to upload multiple files\nfiles\n\n['data/Player Data.xlsx',\n 'data/30-days-create-folds.ipynb',\n 'data/ARK_GENOMIC_REVOLUTION_ETF_ARKG_HOLDINGS.csv',\n 'data/star_pattern_turtlesim.png']"
  },
  {
    "objectID": "posts/2023-04-27-getting-started-with-s3/index.html#create-a-session-and-client",
    "href": "posts/2023-04-27-getting-started-with-s3/index.html#create-a-session-and-client",
    "title": "Getting started with S3 using boto3",
    "section": "Create a session and client",
    "text": "Create a session and client\nBoto3’s region defaults to N-Virginia. To create buckets in another region, region name has to be explicitly mentioned using session object.\n\nsession = boto3.Session(region_name='us-east-2')\ns3client = session.client('s3')\ns3resource = boto3.resource('s3')\n\nS3 buckets have to follow bucket naming rules.\n\nbucket_names = ['my-s3bucket1-usohio-region', 'my-s3bucket2-usohio-region']\ns3location = {'LocationConstraint': 'us-east-2'}"
  },
  {
    "objectID": "posts/2023-04-27-getting-started-with-s3/index.html#check-if-bucket-exists-in-s3",
    "href": "posts/2023-04-27-getting-started-with-s3/index.html#check-if-bucket-exists-in-s3",
    "title": "Getting started with S3 using boto3",
    "section": "Check if bucket exists in S3",
    "text": "Check if bucket exists in S3\nChecking for something before creation is one of the important tasks to avoid unnecessary errors. Here we check if the buckets already exists.\n\ndef check_bucket(bucket):\n    \"\"\"\n    Checks if a bucket is present in S3\n    args:\n    bucket: takes bucket name\n    \"\"\"\n    try:\n        s3client.head_bucket(Bucket=bucket)\n        print('Bucket exists')\n        return True\n    except botocore.exceptions.ClientError as e:\n        # If a client error is thrown, then check that it was a 404 error.\n        # If it was a 404 error, then the bucket does not exist.\n        error_code = int(e.response['Error']['Code'])\n        if error_code == 403:\n            print(\"Private Bucket. Forbidden Access!\")\n            return True\n        elif error_code == 404:\n            print(\"Bucket Does Not Exist!\")\n            return False\n\n\nfor bucket in bucket_names: \n    print(check_bucket(bucket))\n\nBucket exists\nTrue\nBucket exists\nTrue"
  },
  {
    "objectID": "posts/2023-04-27-getting-started-with-s3/index.html#create-a-bucket-in-s3",
    "href": "posts/2023-04-27-getting-started-with-s3/index.html#create-a-bucket-in-s3",
    "title": "Getting started with S3 using boto3",
    "section": "Create a bucket in S3",
    "text": "Create a bucket in S3\nIf the buckets don’t exist, we create them. We need to supply bucket name, a dictionary specifying in which region the bucket has to be created.\n\nfor bucket_name in bucket_names: \n    if not(check_bucket(bucket_name)):\n        print('Creating a bucket..')\n        s3client.create_bucket(Bucket = bucket_name, CreateBucketConfiguration=s3location)\n\nBucket exists\nBucket exists"
  },
  {
    "objectID": "posts/2023-04-27-getting-started-with-s3/index.html#bucket-versioning",
    "href": "posts/2023-04-27-getting-started-with-s3/index.html#bucket-versioning",
    "title": "Getting started with S3 using boto3",
    "section": "Bucket Versioning",
    "text": "Bucket Versioning\nBucket versioning initial state is not set by default. The response from when not initialised doesn’t carry status information rather status dict is absent. Status expects two return states: enabled, suspended. On first creation, the status is in disabled, an unknown state.\nSo in order to make it appear in the REST response, bucket must be enabled by calling the BucketVersioning() boto3 resource function. If we then check the status, it will be present in the REST response.\n\ndef get_buckets_versioning_client(bucketname):\n    \"\"\"\n    Checks if bucket versioning is enabled/suspended or initialised\n    Args:\n    bucketname: bucket name to check versioning\n    Returns: response status - enabled or suspended\n    \"\"\"\n    response = s3client.get_bucket_versioning(Bucket = bucketname)\n    if 'Status' in response and (response['Status'] == 'Enabled' or response['Status'] == 'Suspended'):\n        print(f'Bucket {bucketname} status: {response[\"Status\"]}')\n        return response['Status']\n    else:\n        print(f'Bucket versioning not initialised for bucket: {bucketname}. Enabling...')\n        s3resource.BucketVersioning(bucket_name=bucketname).enable()\n        enable_response = s3resource.BucketVersioning(bucket_name=bucket_name).status\n        return enable_response\n\n\nfor bucket_name in bucket_names: \n    version_status = get_buckets_versioning_client(bucket_name)\n    print(f'Versioning status: {version_status}')\n\nBucket my-s3bucket1-usohio-region status: Enabled\nVersioning status: Enabled\nBucket my-s3bucket2-usohio-region status: Enabled\nVersioning status: Enabled"
  },
  {
    "objectID": "posts/2023-04-27-getting-started-with-s3/index.html#to-suspend-bucket-versioning",
    "href": "posts/2023-04-27-getting-started-with-s3/index.html#to-suspend-bucket-versioning",
    "title": "Getting started with S3 using boto3",
    "section": "To suspend bucket versioning",
    "text": "To suspend bucket versioning\n\nfor bucket_name in bucket_names:\n    version_status = get_buckets_versioning_client(bucket_name)\n    print(f'Versioning status: {version_status}')\n    if version_status == 'Enabled':\n        print('Disabling again..')\n        s3resource.BucketVersioning(bucket_name=bucket_name).suspend()\n\nBucket my-s3bucket1-usohio-region status: Enabled\nVersioning status: Enabled\nDisabling again..\nBucket my-s3bucket2-usohio-region status: Enabled\nVersioning status: Enabled\nDisabling again.."
  },
  {
    "objectID": "posts/2023-04-27-getting-started-with-s3/index.html#to-enable-bucket-versioning",
    "href": "posts/2023-04-27-getting-started-with-s3/index.html#to-enable-bucket-versioning",
    "title": "Getting started with S3 using boto3",
    "section": "To enable bucket versioning",
    "text": "To enable bucket versioning\n\nfor bucket_name in bucket_names:\n    version_status = get_buckets_versioning_client(bucket_name)\n    print(f'Versioning status: {version_status}')\n    if version_status == 'Suspended':\n        print('Enabling again..')\n        s3resource.BucketVersioning(bucket_name=bucket_name).enable()\n\nBucket my-s3bucket1-usohio-region status: Suspended\nVersioning status: Suspended\nEnabling again..\nBucket my-s3bucket2-usohio-region status: Suspended\nVersioning status: Suspended\nEnabling again.."
  },
  {
    "objectID": "posts/2023-04-27-getting-started-with-s3/index.html#get-bucket-list-from-s3",
    "href": "posts/2023-04-27-getting-started-with-s3/index.html#get-bucket-list-from-s3",
    "title": "Getting started with S3 using boto3",
    "section": "Get bucket list from S3",
    "text": "Get bucket list from S3\nWe can list the buckets in S3 using list_buckets() client function. It return a dict. We can iterate through Buckets key to find the names of the buckets.\n\nbuckets_list = s3client.list_buckets()\nfor bucket in buckets_list['Buckets']:\n    print(bucket['Name'])\n\nmlops-project-sales-forecast-bucket\nmlops-project-sales-forecast-bucket-dr563105-mlops-project\nmy-s3bucket1-usohio-region\nmy-s3bucket2-usohio-region\ns3-for-terraform-state"
  },
  {
    "objectID": "posts/2023-04-27-getting-started-with-s3/index.html#upload-files-to-s3",
    "href": "posts/2023-04-27-getting-started-with-s3/index.html#upload-files-to-s3",
    "title": "Getting started with S3 using boto3",
    "section": "Upload files to S3",
    "text": "Upload files to S3\nBoto3 allows file upload to S3. The upload_file client function requires three mandatory arguments -\n1. filename of the file to be uploaded\n2. bucket_name, Into which bucket the file would be uploaded\n3. key, name of the file in S3\n\ndef upload_files_to_s3(filename, bucket_name, key=None, ExtraArgs=None):\n    \"\"\"\n    Uploads file to S3 bucket\n    Args:\n    filename: takes local filename to be uploaded\n    bucker_name: name of the bucket into which the file is uploaded\n    key: name of the file in the bucket. Default:None\n    ExtraArgs: other arguments. Default:None\n    \"\"\"\n    if key is None:\n        key = filename\n    \n    try:\n        s3client.upload_file(filename,bucket_name,key)\n        print(f'uploaded file:{filename}')\n    except botocore.exceptions.ClientError as e:\n        print(e)\n\nWe can make use of glob module to upload multiple files in a folder\n\nbucket1_files = [files[1],files[2]]\nbucket2_files = [files[0],files[3]]\nbucket1_files, bucket2_files\n\n(['data/30-days-create-folds.ipynb',\n  'data/ARK_GENOMIC_REVOLUTION_ETF_ARKG_HOLDINGS.csv'],\n ['data/Player Data.xlsx', 'data/star_pattern_turtlesim.png'])\n\n\n\nfor file in bucket1_files:\n    upload_files_to_s3(file,bucket_name=bucket_names[0])\n\nuploaded file:data/30-days-create-folds.ipynb\nuploaded file:data/ARK_GENOMIC_REVOLUTION_ETF_ARKG_HOLDINGS.csv\n\n\n\nfor file in bucket2_files:\n    upload_files_to_s3(file,bucket_name=bucket_names[1])\n\nuploaded file:data/Player Data.xlsx\nuploaded file:data/star_pattern_turtlesim.png"
  },
  {
    "objectID": "posts/2023-04-27-getting-started-with-s3/index.html#get-files-list",
    "href": "posts/2023-04-27-getting-started-with-s3/index.html#get-files-list",
    "title": "Getting started with S3 using boto3",
    "section": "Get files list",
    "text": "Get files list\nGetting the files list from each bucket done using list_objects client function. It returns dict and we can iterate through Contents key to retrieve the filenames.\n\nfor bucket in bucket_names:\n    print(f'Listing object inside bucket:{bucket}')\n    list_obj_response = s3client.list_objects(Bucket=bucket)\n    for obj in list_obj_response['Contents']:\n        print(obj['Key'])\n    print()\n\nListing object inside bucket:my-s3bucket1-usohio-region\ndata/30-days-create-folds.ipynb\ndata/ARK_GENOMIC_REVOLUTION_ETF_ARKG_HOLDINGS.csv\n\nListing object inside bucket:my-s3bucket2-usohio-region\ndata/Player Data.xlsx\ndata/star_pattern_turtlesim.png"
  },
  {
    "objectID": "posts/2023-04-27-getting-started-with-s3/index.html#download-files",
    "href": "posts/2023-04-27-getting-started-with-s3/index.html#download-files",
    "title": "Getting started with S3 using boto3",
    "section": "Download files",
    "text": "Download files\nDownloading a file is very similar to uploading one. We need specify bucket name, name of the file to be downloaded, and the destination filename.\n\nprint(f'Downloading files from bucket:{bucket_names[1]}')\ns3client.download_file(Bucket=bucket_names[1],Key='data/star_pattern_turtlesim.png',Filename='downloaded_turtlesim.jpg')\n\nDownloading files from bucket:my-s3bucket2-usohio-region"
  },
  {
    "objectID": "posts/2023-04-27-getting-started-with-s3/index.html#conclusion",
    "href": "posts/2023-04-27-getting-started-with-s3/index.html#conclusion",
    "title": "Getting started with S3 using boto3",
    "section": "Conclusion",
    "text": "Conclusion\nThis blog post shows how to use the boto3 python SDK to manage S3 aws service. With the help of documentation, we can implement require functionalities."
  },
  {
    "objectID": "posts/2023-06-04-api-lambda-ecr/aws-ecr-api.html",
    "href": "posts/2023-06-04-api-lambda-ecr/aws-ecr-api.html",
    "title": "Deepak Ramani blog",
    "section": "",
    "text": "In this document I’m going to include a guide to integrate/setup AWS API Gateway, AWS Lambda using ECR image as source image.\nIt is long and technical with lots of screenshots and explanations."
  },
  {
    "objectID": "posts/2023-06-04-api-lambda-ecr/aws-ecr-api.html#create-docker-container-image-and-upload-to-ecr",
    "href": "posts/2023-06-04-api-lambda-ecr/aws-ecr-api.html#create-docker-container-image-and-upload-to-ecr",
    "title": "Deepak Ramani blog",
    "section": "Create docker container image and upload to ECR",
    "text": "Create docker container image and upload to ECR\nWe have the dockerfile, lambda_function, Pipfile, Pipfile.lock and items.parquet. If none of these make any sense, I urge you to go through my posts on lambda and docker on my blog.\n\n\nTerminal\n\ndocker build -t lambda-app:v1 . # build docker image\nexport ACCOUNT_ID=xxxx\naws ecr create-repository --repository-name lambda-images\ndocker tag lambda-app:v1 ${ACCOUNT_ID}.dkr.ecr.us-east-1.amazonaws.com/lambda-images:app\n$(aws ecr get-login --no-include-email)\ndocker push ${ACCOUNT_ID}.dkr.ecr.us-east-1.amazonaws.com/lambda-images:app\n\nNote: Remember we supplied aws access key and secret before and the session borrows them for login. If it is a new session, those variables have to be given again.\n \nIn the screenshots we can see that our container image is uploaded to the registry."
  },
  {
    "objectID": "posts/2023-06-04-api-lambda-ecr/aws-ecr-api.html#aws-lambda-function",
    "href": "posts/2023-06-04-api-lambda-ecr/aws-ecr-api.html#aws-lambda-function",
    "title": "Deepak Ramani blog",
    "section": "AWS Lambda function",
    "text": "AWS Lambda function\nAWS Lambda allows us to use our container image from ECR to use as source. We need to give the image URI(can be found in ECR console). Below screenshot shows the step to point at the container image and create a function. \nIn cloud environment, security is a massive risk. So AWS insists on policies everywhere. Policy allows only revelant people access. Below policy screenshot shows us giving permission to Cloudwatch to create log groups, put log events whenever the Lambda function is invoked. You can goto “Monitor” tab and browse through the logs.\n\n\nPolicy for S3 access\nSince our artifact after ML training is stored in S3 bucket, we need to give the Lambda function permission to access it. For that click on the role lambda-sales-app-role-qu7xpax1 under Execution role. It will direct to the IAM console. Here click “Add Permission” - “Create Inline Policy” - “JSON” tab and paste the following JSON data:\n{\n    \"Statement\": [\n        {\n            \"Action\": [\n                \"s3:Get*\",\n                \"s3:List*\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": [\n                \"arn:aws:s3:::mlops-project-sales-forecast-bucket\",\n                \"arn:aws:s3:::mlops-project-sales-forecast-bucket/*\"\n            ]\n        }\n    ],\n    \"Version\": \"2012-10-17\"\n}\n mlops-project-sales-forecast-bucket is the name of the S3 bucket where the artifacts are stored. I give restricted permission to the Lambda function. Quite often we see resource will be given * which can led to problems and security threats in the future. Review it. \nNext, create a unique name for it and click create policy. \nIn the overview policy console page, we can see that there are two policies attached. That means the lambda function has permission to download from that specified S3 bucket. \n\n\nEnvironment Variables and Configuration\nIn the configuration tab, environment variable section add in RUN_ID and S3_BUCKET_NAME.  Next change the Max Memory and Timeout values in the General Configuration section. \n\n\nTesting it at Lambda console\nIn the Test tab, we create a test event and give our sample JSON input.  \n{\"find\": {\"date1\": \"2017-08-26\", \"store_nbr\": 20}}\nIf you get a successful execution as in the screenshot, it means the pipeline works and the attached policies are correct. Most time it is the policies that cause annoying issues."
  },
  {
    "objectID": "posts/2023-06-04-api-lambda-ecr/aws-ecr-api.html#api-gateway",
    "href": "posts/2023-06-04-api-lambda-ecr/aws-ecr-api.html#api-gateway",
    "title": "Deepak Ramani blog",
    "section": "API Gateway",
    "text": "API Gateway\n\nRest API with an endpoint\nWe will create a REST API with an endpoint of predict-sales using POST method. Goto API Gateway console -&gt; Create API -&gt; Rest API -&gt; Click Build. Here give an appropriate name and description. \nThen we create our endpoint predict-sales as a resource and POST method under it. While creating the POST method, we point it to our previously created Lambda function lambda-sales-app.   There will be a pop up window with ARN address. This address will be same as Lambda function. You can verify it by going to the Lambda console.\n\n\nTesting\nBefore deploying the API we can test. Press the Test button, give the sample JSON input in the body section and expect an output similar to Lambda test.  \nNow our API is ready for deployment. ### Deployment From the actions menu choose “Deploy API”. Choose “New Stage”, give a name and then deploy.   We will get an invoke url. However, we need to append our endpoint predict-sales to complete it. So it will look something like https://xxxxxxxx.execute-api.us-east-1.amazonaws.com/stg-lambda-app-for-blog/predict-sales"
  },
  {
    "objectID": "posts/2023-06-04-api-lambda-ecr/aws-ecr-api.html#dynamodb-add-policy",
    "href": "posts/2023-06-04-api-lambda-ecr/aws-ecr-api.html#dynamodb-add-policy",
    "title": "Deepak Ramani blog",
    "section": "DynamoDB add policy",
    "text": "DynamoDB add policy\nAdd this policy to the previously created role to give lambda access to write to DynamoDB.\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"dynamodb:BatchGetItem\",\n                \"dynamodb:GetItem\",\n                \"dynamodb:Query\",\n                \"dynamodb:Scan\",\n                \"dynamodb:BatchWriteItem\",\n                \"dynamodb:PutItem\",\n                \"dynamodb:UpdateItem\"\n            ],\n            \"Resource\": \"arn:aws:dynamodb:us-east-1:4xxxxxxxx:table/sales_preds_for_blog\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"logs:CreateLogStream\",\n                \"logs:PutLogEvents\"\n            ],\n            \"Resource\": \"arn:aws:dynamodb:us-east-1:4xxxxxxxx:table/sales_preds_for_blog\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"logs:CreateLogGroup\",\n            \"Resource\": \"arn:aws:dynamodb:us-east-1:4xxxxxxxx:table/sales_preds_for_blog\"\n        }\n    ]\n}"
  },
  {
    "objectID": "posts/2021-09-18-kaggle-setup/index.html",
    "href": "posts/2021-09-18-kaggle-setup/index.html",
    "title": "Setting up Kaggle on Linux/Mac",
    "section": "",
    "text": "Most of latest data science innovations happen at Kaggle. Kaggle hosts, in addtion to competitions, a large collection of datasets from various fields. The easiest way to interact with Kaggle is through its public API via command-line tool(CLI). Setting it up outside of Kaggle kernels is one of first tasks. In this post, I will guide you through that process."
  },
  {
    "objectID": "posts/2021-09-18-kaggle-setup/index.html#installation",
    "href": "posts/2021-09-18-kaggle-setup/index.html#installation",
    "title": "Setting up Kaggle on Linux/Mac",
    "section": "Installation",
    "text": "Installation\n\n\nTerminal\n\npip install --user kaggle\n\n\n\n\n\n\n\nTip\n\n\n\nTip: Install kaggle package inside your conda ML development environment rather than outside of it or in base env.\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nDon’t do sudo pip install kaggle as it would require admin privileges for every run."
  },
  {
    "objectID": "posts/2021-09-18-kaggle-setup/index.html#download-api-token",
    "href": "posts/2021-09-18-kaggle-setup/index.html#download-api-token",
    "title": "Setting up Kaggle on Linux/Mac",
    "section": "Download API token",
    "text": "Download API token\n\nCreate/login into your kaggle account.\nFrom the site header, click on your user profile picture and select Account. You will be land on your profile with account tab active.\nScroll down to API section. Click Create New API Token. A json file will be downloaded your default download directory."
  },
  {
    "objectID": "posts/2021-09-18-kaggle-setup/index.html#move-.json-file-to-the-correct-location",
    "href": "posts/2021-09-18-kaggle-setup/index.html#move-.json-file-to-the-correct-location",
    "title": "Setting up Kaggle on Linux/Mac",
    "section": "Move .json file to the correct location",
    "text": "Move .json file to the correct location\n\nMove it to .kaggle in the home directory. Create if absent.\n\n\n\nTerminal\n\ncd\nmkdir ~/.kaggle\nmv &lt;location&gt;/kaggle.json ~/.kaggle/kaggle.json\n\n\nFor your security, ensure that other users of your computer do not have read access to your credentials. On Unix-based systems you can do this with the following command:\n\n\n\nTerminal\n\nchmod 600 ~/.kaggle/kaggle.json\n\n\nRestart the terminal and navigate to the env where kaggle package is installed if necessary."
  },
  {
    "objectID": "posts/2021-09-18-kaggle-setup/index.html#check-if-it-is-properly-installed",
    "href": "posts/2021-09-18-kaggle-setup/index.html#check-if-it-is-properly-installed",
    "title": "Setting up Kaggle on Linux/Mac",
    "section": "Check if it is properly installed",
    "text": "Check if it is properly installed\n\nRun:\n\n\n\nTerminal\n\n$python\n&gt;&gt;&gt;import kaggle\n\nImporting kaggle shouldn’t return an error. If there is error, check whether you’re in the right env where kaggle is installed.\nIf no error, exit the shell and type the following command in the terminal.\n\n\nTerminal\n\nkaggle competitions list\n\nIf installed properly, the command will list all the entered competitions. 1. If not, the binary path may be incorrect. Usually it is installed in ~/.local/bin Try using\n\n\nTerminal\n\n~/.local/bin/kaggle competitions list\n\n\nIf the above command works, export that binary path to the shell environment(bashrc) so that you might use just kaggle next time."
  },
  {
    "objectID": "posts/2021-09-18-kaggle-setup/index.html#api-usage",
    "href": "posts/2021-09-18-kaggle-setup/index.html#api-usage",
    "title": "Setting up Kaggle on Linux/Mac",
    "section": "API usage",
    "text": "API usage\nIt is time to use the Kaggle API. For example, to see what dataset command offers, in the CLI enter\n\n\nTerminal\n\nkaggle dataset --help\n\n\n\n\n\n\n\nTip\n\n\n\nTip: Remember to comply with competition’s terms and conditions before downloading the dataset. You will get an error forbidden if you try to download before agreeing.\n\n\nFor more info on the API, Kaggle’s github page is an excellent resource."
  },
  {
    "objectID": "posts/2023-08-29-cdc-debezium-kafka-pg/index.html",
    "href": "posts/2023-08-29-cdc-debezium-kafka-pg/index.html",
    "title": "Change Data Capture with Debezium, Kafka, S3",
    "section": "",
    "text": "Change Data Capture(CDC) is a mechanism in which every change committed to a row in a table is captured and sent to downstream applications. This CDC post answers some of the questions:\n\nHow to transfer change data from source database downstream through Kafka?\n\n\nHow does Debezium help with CDC?\n\n\nHow to ingest data from Kafka topics into a warehouse for analytics?\n\nWe will use a simple CDC project to explore the data pipeline to get data from upstream to downstream consumers.\nFull disclosure - this post and project is inspired from Start Data Engineering’s blog post. I highly recommend reading his posts. It is highly informative. Although, I use his template, the data, data pipeline setup, this blog post are entirely my work."
  },
  {
    "objectID": "posts/2023-08-29-cdc-debezium-kafka-pg/index.html#objective",
    "href": "posts/2023-08-29-cdc-debezium-kafka-pg/index.html#objective",
    "title": "Change Data Capture with Debezium, Kafka, S3",
    "section": "Objective",
    "text": "Objective\nObserve and monitor a postgres DB source with a schema commerce and tables users and products. When there is a change in their rows, we will capture those, send it downstream through Apache Kafka, store it in a storage service such as MinIO and make it available for analytics through duckDB.\nBefore I begin explaining the project’s data pipeline in detail, let me quickly give you the steps to reproduce it. This way you can get started already and read the details later."
  },
  {
    "objectID": "posts/2023-08-29-cdc-debezium-kafka-pg/index.html#challenges-faced",
    "href": "posts/2023-08-29-cdc-debezium-kafka-pg/index.html#challenges-faced",
    "title": "Change Data Capture with Debezium, Kafka, S3",
    "section": "Challenges faced",
    "text": "Challenges faced\nIf you’re curious like me, it is always the challenges that make a good story to tell others. Instead of going in length about each challenge, I will just list a few that I found harder.\n\nFiguring out why decimal/numeric data type variables from source don’t reach the destination with same values. I went from source to sink to Kafka to Debezium to finally discover that it is an issue with Debezium Kafka Connect.\nUnderstanding various configurations for Apache Kafka and which ones are needed while starting its Docker container.\nLearning how WAL works and the significance of CDC"
  },
  {
    "objectID": "posts/2023-08-29-cdc-debezium-kafka-pg/index.html#architecture",
    "href": "posts/2023-08-29-cdc-debezium-kafka-pg/index.html#architecture",
    "title": "Change Data Capture with Debezium, Kafka, S3",
    "section": "Architecture",
    "text": "Architecture"
  },
  {
    "objectID": "posts/2023-08-29-cdc-debezium-kafka-pg/index.html#prerequisites-and-setup",
    "href": "posts/2023-08-29-cdc-debezium-kafka-pg/index.html#prerequisites-and-setup",
    "title": "Change Data Capture with Debezium, Kafka, S3",
    "section": "Prerequisites and Setup",
    "text": "Prerequisites and Setup\nWe use Ubuntu 20.04 LTS AWS EC2 machine for the project. Full source code here.\nWe need the following:\n\ngit version &gt;= 2.37.1,\nDocker version &gt;= 20.10.17 and Docker compose v2 version &gt;= v2.10.2,\npgcli,\nmake, \noptional, python &gt;= v3.7.\n\nTo make things easier I have scripted these prerequisites. Just clone my repo and run the instructions I provide.\n\n\nclone and install prerequisites\n\nsudo apt update && sudo apt install git make -y\ngit clone https://github.com/deepakramani/cdc-debezium-kafka.git\ncd cdc-debezium-kafka\n1make install_conda\n2make install_docker\nsource ~/.bashrc\n\n\n1\n\nInstalls Miniforge. Optional but recommended.\n\n2\n\nInstalls Docker and Docker-compose.\n\n\nLogout and log in back to the instance. To test docker if it is working, run\n\n\ncheck if docker is installed\n\ndocker run --rm hello-world # should return \"Hello from Docker!\" without errors\n\nNow we’re ready to execute our project. I highly encourage readers to use makefile as it makes setup highly efficient.\n\n\nExecuting CDC project\n\ncd cdc-debezium-kafka\nexport POSTGRES_USER=postgres\nexport POSTGRES_PASSWORD=postgres\nexport POSTGRES_DB=cdc-demo-db\nexport POSTGRES_HOST=postgres\nexport DB_SCHEMA=commerce\nexport AWS_KEY_ID=minio\nexport AWS_SECRET_KEY=minio123\nexport AWS_BUCKET_NAME=commerce\nmake up # runs all docker containers\n#wait for 60 seconds allow all containers to be up and running\nmake connections # setup connectors\n#wait for 100-120 seconds to allow data to be pushed to Minio(S3).\n\nOpen a browser and go to localhost:9001 to open up Minio UI. Login with minio as username and minio123 as password. Then navigate to buckets -&gt; commerce -&gt; debezium.commerce.products and further to get to the json files. Similarly to reach debezium.commerce.users table json files.\nThese json files contain the change data(Upsert and delete) for respective tables. From here we can use duckdb to analyse data. There you have it, a complete data pipeline that fetches change data from the source and brings it to the sink(downstream).\n\nDeleting resources\nTo bring down all container and return to the original state, run the following instructions\n\n\nrestoring to original state\n\nmake down #shuts down all project processes and docker containers\n# To delete minio buckets with json files and DB volume. \n# Docker creates directories with root as owner. \n# So root access is needed to delete them.\n# sudo rm -rf minio/ psql_vol/"
  },
  {
    "objectID": "posts/2023-08-29-cdc-debezium-kafka-pg/index.html#testing",
    "href": "posts/2023-08-29-cdc-debezium-kafka-pg/index.html#testing",
    "title": "Change Data Capture with Debezium, Kafka, S3",
    "section": "Testing",
    "text": "Testing\nTesting the data pipeline is one of the important aspects of data engineering. Read more about it in a detail blog here\n\n\ntesting\n\nexport TEST_POSTGRES_USER=test_postgres\nexport TEST_POSTGRES_PASSWORD=test_postgres\nexport TEST_POSTGRES_DB=test_cdc-demo-db\nexport TEST_POSTGRES_HOST=test_postgres\nexport TEST_DB_SCHEMA=commerce\nexport TEST_AWS_KEY_ID=test_minio\nexport TEST_AWS_SECRET_KEY=test_minio123\nexport TEST_AWS_BUCKET_NAME=commerce\n1make tup\n2make ci\n\n\n1\n\nOpens similar test containers for source DB and datagen.\n\n2\n\nExecutes pytest and displays if passed or failed.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nTest suite can be run only once. Running it again will result in error as the previous run data is still present in the DB.\n\n\nTo shutdown after testing,\nmake tdown\n1sudo rm -rf test_psql_vol/\n\n1\n\nNeed to remove the mounted volume test_psql_vol if you want to run test suite again."
  },
  {
    "objectID": "posts/2023-08-29-cdc-debezium-kafka-pg/index.html#loading-data-into-postgres-db-source",
    "href": "posts/2023-08-29-cdc-debezium-kafka-pg/index.html#loading-data-into-postgres-db-source",
    "title": "Change Data Capture with Debezium, Kafka, S3",
    "section": "Loading data into Postgres DB source",
    "text": "Loading data into Postgres DB source\nWe leverage Postgres docker bootstrap feature to create our table and set permissions for replication. For more on bootstrap feature, go this link, navigate to Initialization scripts.\nIn our case we have a file init.sql mounted to /docker-entrypoint-initdb.d.\n\n\ninit.sql\n\n-- create schema commerce\nCREATE SCHEMA commerce;\n\n-- Use commerce schema\nSET search_path TO commerce;\n\n-- create a users table\nCREATE TABLE IF NOT EXISTS users (\n    id SERIAL PRIMARY KEY,\n    username VARCHAR(255) NOT NULL,\n    email_address TEXT NOT NULL \n);\n\nCREATE TABLE IF NOT EXISTS products (\n   id SERIAL PRIMARY KEY, \n   name VARCHAR(255) NOT NULL,\n   description TEXT,\n1   price REAL NOT NULL\n);\n\n2ALTER TABLE users REPLICA IDENTITY FULL;\n\nALTER TABLE products REPLICA IDENTITY FULL;\n\n\n1\n\nThe price has to be REAL data type as debezium encodes NUMERIC or DECIMAL data types to BigDecimal Binary. For more on that, read here.\n\n2\n\nTo allow for replication. Records the old values of all columns in the row in the WAL.\n\n\n\nWAL - Write Ahead Log\nA Write-Ahead log is a feature especially in Databases to ensure integrity of data and durability in case of system failure. It is a process of recording changes in sequential logs before writing directly into disk(Databases).\nOur intended changes are first written to the log and then passed onto DB. Upon successful transaction, the log is updated with a sequence number. Upon system failure and consective restart, the DB polls the log to resume its operation. This way systems can acheive ACID properties for reliable and robust database management.\nThe SQL command ALTER TABLE users REPLICA IDENTITY FULL; instructs Postgres to use all columns of the users table to identify rows when replicating changes.\n\n\nGenerating data for upload\nWe use faker python library to generate fake data into the tables - users and products. We use the psycopg2 Postgres python library to establish database(DB) connection. And then use it to commit the generated data.\n\n\n\n\n\n\nNote\n\n\n\nUpdate - Sep 5, 2023 - The datagen script has changed to accomodate testing but the below source code logic remains the same.\n\n\n\n\nuser_product_data.py\n\nconn = psycopg2.connect(database=f'{POSTGRES_DB}',\n                        user=f'{POSTGRES_USER}',\n                        password=f'{POSTGRES_PASSWORD}',\n                        host=f'{POSTGRES_HOSTNAME}'\n                       )\n\ncurr = conn.cursor()\ncurr.execute(f\"INSERT INTO \\\n               commerce.users (id, username, email_address) \\\n               VALUES (%s, %s, %s)\", (id, fake.user_name(), fake.email())\n            )\ncurr.execute(f\"INSERT INTO \\\n               commerce.products (id, name, description, price) \\\n               VALUES (%s, %s, %s, %s)\", (id, fake.name(), fake.text(), \n               (fake.random_int(min=1, max=999_999))/100.0)\n      )\nconn.commit()\n\nWe can use the same script to simulate(with sleep) updating and deleting a few rows at random.\n\n\nuser_product_data.py\n\n1sleep(0.3)\n# update 10 % of the time\nif random.randint(1, 100) &gt;= 90:\n  curr.execute(\"UPDATE commerce.users \\\n                SET username = %s\\\n                WHERE id = %s\",(fake.user_name(), id)\n              )\n  curr.execute(\"UPDATE commerce.products \\\n                SET name = %s \\\n                WHERE id = %s\",(fake.user_name(), id)\n              )\n  conn.commit()\n\nsleep(0.4)\n# delete 5 % of the time\nif random.randint(1, 100) &gt;= 95:\n  curr.execute(\"DELETE FROM commerce.users WHERE id = %s\",(id,))\n  curr.execute(\"DELETE FROM commerce.products WHERE id = %s\", (id,))\n\nconn.commit()\ncurr.close()\n\n\n1\n\nsleep allows for data to be inserted and be ready sequential operations.\n\n\n\n\nAssociated Docker configuration\n\n\ndocker-compose.yml for data upload into source DB\n\ncdc_commerce_postgres:\n    container_name: source-db\n    image: debezium/postgres:13-alpine\n1    hostname: ${POSTGRES_HOST}\n    ports:\n      - 5432:5432\n    environment:\n      POSTGRES_USER: ${POSTGRES_USER}\n      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}\n      POSTGRES_DB: ${POSTGRES_DB}\n    volumes:\n2      - \"./postgres/init.sql:/docker-entrypoint-initdb.d/init.sql\"\n      - \"./psql_vol:/var/lib/postgresql/data:rw\"\n    networks:\n      - my_network\n\n  datagen:\n    build: ./generate-data\n    image: cdc-datagen\n    container_name: datagen\n    restart: on-failure\n3    environment:\n      POSTGRES_USER: ${POSTGRES_USER}\n      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}\n      POSTGRES_DB: ${POSTGRES_DB}\n      POSTGRES_HOST: ${POSTGRES_HOST}\n      DB_SCHEMA: ${DB_SCHEMA}\n    depends_on:\n4      - cdc_commerce_postgres\n    networks:\n      - my_network\n\n\n1\n\nEnvironment variables to spin up a Postgres DB server is supplied from the terminal.\n\n2\n\nUtilising Postgres bootstrap feature we can create our schema, define and create tables.\n\n3\n\nFor datagen container DB env variables are supplied. The user_product_data.py uses these variables to populate the tables.\n\n4\n\nOfcourse, to populate the tables, source DB must be ready. depends_on property defines the dependency between two services.\n\n\nNow our source DB is populated."
  },
  {
    "objectID": "posts/2023-08-29-cdc-debezium-kafka-pg/index.html#apache-kafka",
    "href": "posts/2023-08-29-cdc-debezium-kafka-pg/index.html#apache-kafka",
    "title": "Change Data Capture with Debezium, Kafka, S3",
    "section": "Apache Kafka",
    "text": "Apache Kafka\nApache Kafka is a message stream processing framework. The change data from the source DB must be streamed downstream. Kafka facilitates this process.\nKafka uses a meta-data service management tool called Zookeeper. It handles the configurations needed for Kafka brokers to perform their tasks properly.\nUsing docker properties, we invoke kafka and zookeeper services.\n\n\ndocker-compose config for Zookeeper and Kafka\n\nzookeeper:\n    image: debezium/zookeeper:2.4\n    container_name: zookeeper\n    ports:\n1      - \"2181:2181\"\n    networks:\n      - my_network\n    \n  kafka:\n    container_name: kafka\n    image: debezium/kafka:latest\n    ports:\n      - \"9093:9093\" \n    environment:\n2      - ZOOKEEPER_CONNECT=zookeeper:2181\n3      - KAFKA_INTER_BROKER_LISTENER_NAME=LISTENER_INT\n4      - KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=LISTENER_INT:PLAINTEXT,LISTENER_EXT:PLAINTEXT\n5      - KAFKA_ADVERTISED_LISTENERS=LISTENER_INT://kafka:9092,LISTENER_EXT://localhost:9093\n      - KAFKA_LISTENERS=LISTENER_INT://kafka:9092,LISTENER_EXT://0.0.0.0:9093 \n    depends_on:\n      - zookeeper\n    networks:\n      - my_network\n\n\n1\n\nZookeeper utilises port 2181 for its service.\n\n2\n\nKafka communicates with zookeeper through 2181 port.\n\n3\n\nKafka uses this host/ip to interact with themselves. Here it is kafka:9092.\n\n4\n\nDefinition of security protocol between listener names and their protocols. Here it is just plain text.\n\n5\n\nInternally Kafka brokers communicate with themselves using 9092. With others in localhost:9093.\n\n\nRead more on Kafka Listerners here.\nApache Kafka configuration can be confusing. It is advised to start simple and add in as needs demand.\nNext, up is to setup the sink or the final destination."
  },
  {
    "objectID": "posts/2023-08-29-cdc-debezium-kafka-pg/index.html#storage---minio-s3",
    "href": "posts/2023-08-29-cdc-debezium-kafka-pg/index.html#storage---minio-s3",
    "title": "Change Data Capture with Debezium, Kafka, S3",
    "section": "Storage - Minio S3",
    "text": "Storage - Minio S3\nWe want to store our message streams from Kafka into a storage as files. We utlise Minio an object storage service as it mimics AWS S3.\nTo set it up, as usual we configure its properties through docker-compose.\n\n\ndocker-compose for Minio storage\n\nminio:\n    image: minio/minio:RELEASE.2022-05-26T05-48-41Z\n    hostname: minio\n    container_name: minio\n    ports:\n      - '9000:9000' \n      - '9001:9001'\n    volumes:\n1      - \"./minio/data:/data\"\n    environment:\n2      MINIO_ROOT_USER: ${AWS_KEY_ID}\n      MINIO_ROOT_PASSWORD: ${AWS_SECRET_KEY}\n\n\n1\n\nminio directory is created and it is mounted as volume.\n\n2\n\nTo mimic AWS S3, we can pass credentials as env variables.\n\n\nThere is another service that creates buckets inside the mounted volume. I leave that to the reader as a self-exercise."
  },
  {
    "objectID": "posts/2023-08-29-cdc-debezium-kafka-pg/index.html#debezium-kafka-connect",
    "href": "posts/2023-08-29-cdc-debezium-kafka-pg/index.html#debezium-kafka-connect",
    "title": "Change Data Capture with Debezium, Kafka, S3",
    "section": "Debezium Kafka Connect",
    "text": "Debezium Kafka Connect\nAll the individual blocks are now prepared, and the remaining task is to connect them.\nSetting up Debezium involves two parts:\n\nConnect to Source DB\nConnect Kafka topics to Minio S3 storage\n\nDebezium supports Postgres source using Kafka connect natively but as a sink connector to S3 it lacks functionality. So we borrow Kafka connect, add in a plugin that recognises both Kafka topics and S3 storage.\nThere are several services which offers sink connectors but for simplicity sake, I use an open source service in Aiven.\nSo, with the help of docker-compose, we use debezium/connect docker image and install Aiven S3 connector plugin on top of it.\n\n\ndocker-compose for Debezium connect\n\n  debezium-connect:\n    container_name: debezium-connect\n1    build: ./sink-connector\n    ports:\n      - \"8083:8083\"\n    environment:\n2      - BOOTSTRAP_SERVERS=kafka:9092\n      - GROUP_ID=1\n      - CONFIG_STORAGE_TOPIC=my_connect_configs\n      - OFFSET_STORAGE_TOPIC=my_connect_offsets\n      - STATUS_STORAGE_TOPIC=my_connect_statuses\n    depends_on:\n      - zookeeper\n      - kafka\n    networks:\n      - my_network\n\n\n1\n\nInstalls Aiven S3 connector plugin.\n\n2\n\nDebezium connects to Kafka as a producer and Aiven S3 Kafka Connect monitors Kafka topics as a consumer. Importantly this is the bootstrap-server to communicate within the docker network my_network.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThe Debezium(producer) connector still doesn’t know from where it is fetching the data from or the Aiven Connect(consumer) sending the data to."
  },
  {
    "objectID": "posts/2023-08-29-cdc-debezium-kafka-pg/index.html#activating-the-traffic-in-the-pipeline",
    "href": "posts/2023-08-29-cdc-debezium-kafka-pg/index.html#activating-the-traffic-in-the-pipeline",
    "title": "Change Data Capture with Debezium, Kafka, S3",
    "section": "Activating the traffic in the pipeline",
    "text": "Activating the traffic in the pipeline\nTo activate the traffic flow from source to destination, we need to supply source and destination link configurations. This is acheived through json connector configuration.\nApache Kafka uses REST API so that means a JSON data goes into the body of the POST request. Conveniently there is curl command that can be used to request. Though curl commands allows to pass json files to the data option, the JSON format as such require sensitive information properties to be hard-coded. That puts system security at risk.\nFortunately for us, a shell script allows for sensitive information to be masked while being supplied at run time(more on it here). It is indeed that approach we take for our task to connect the blocks in the pipeline.\n\n\nsetup-connections.sh\n\ncurl --include --request POST --header \"Accept:application/json\" \\\n    --header \"Content-Type:application/json\" \\\n    --url localhost:8083/connectors/ \\\n    --data '{    \n        \"name\": \"pg-src-connector\",\n        \"config\": {\n1            \"connector.class\": \"io.debezium.connector.postgresql.PostgresConnector\",\n            \"tasks.max\": \"1\",\n2            \"database.hostname\": \"'\"${POSTGRES_USER}\"'\",\n            \"database.port\": \"5432\",\n            \"database.user\": \"'\"${POSTGRES_USER}\"'\",\n            \"database.password\": \"'\"${POSTGRES_PASSWORD}\"'\",\n            \"database.dbname\": \"'\"${POSTGRES_DB}\"'\",\n            \"database.server.name\": \"postgres\",\n            \"database.include.list\": \"postgres\",\n3            \"topic.prefix\": \"debezium\",\n            \"schema.include.list\": \"commerce\",\n4            \"decimal.handling.mode\": \"precise\"\n        }\n      }'\n\n\n1\n\nDebezium connector standards.\n\n2\n\nSupplying env variables to the properties\n\n3\n\nUsually Kafka topic names are just table names. So in our case, it will be debezium.commerce.products and debezium.commerce.users.\n\n4\n\nThis property can adjusted to double preserve source’s decimal or numeric data types.\n\n\nSimilarly we can setup sink connector.\n\n\nsetup-connections.sh\n\ncurl --include --request POST --header \"Accept:application/json\" \\\n    --header \"Content-Type:application/json\" \\\n    --url localhost:8083/connectors/ \\\n    --data '{\n        \"name\": \"s3-sink\",\n        \"config\": {\n            \"connector.class\": \"io.aiven.kafka.connect.s3.AivenKafkaConnectS3SinkConnector\",\n            \"aws.access.key.id\": \"'\"${AWS_KEY_ID}\"'\",\n            \"aws.secret.access.key\": \"'\"${AWS_SECRET_KEY}\"'\",\n            \"aws.s3.bucket.name\": \"'\"${AWS_BUCKET_NAME}\"'\",\n            \"aws.s3.endpoint\": \"http://minio:9000\",\n            \"aws.s3.region\": \"us-east-1\",\n            \"format.output.type\": \"jsonl\",\n1            \"topics\": \"debezium.commerce.users,debezium.commerce.products\",\n            \"file.compression.type\": \"none\",\n            \"flush.size\": \"20\",\n2            \"file.name.template\": \"/{{topic}}/{{timestamp:unit=yyyy}}-{{timestamp:unit=MM}}-{{timestamp:unit=dd}}/{{timestamp:unit=HH}}/{{partition:padding=true}}-{{start_offset:padding=true}}.json\"\n        }\n    }'\n\n\n1\n\nSink connector actively listens to these topics to receive data.\n\n2\n\nFile generated have templated filename."
  },
  {
    "objectID": "posts/2023-08-29-cdc-debezium-kafka-pg/index.html#deployment",
    "href": "posts/2023-08-29-cdc-debezium-kafka-pg/index.html#deployment",
    "title": "Change Data Capture with Debezium, Kafka, S3",
    "section": "Deployment",
    "text": "Deployment\nTo deploy all the components we can use these commands\n\n\nsetup data pipeline\n\n1make up\n2make connections\n\n\n1\n\nFires up all the services.\n\n2\n\nLinks source to Kafka and Kafka topics to Minio S3.\n\n\n\nOn successful connection establishment, we will 201 created message as shown in the below screenshot.\n\nIf done correctly, we should have successfully connected the source to the destination. We will wait a few minutes to have the files appear in the Minio commerce bucket.\n\nWe can also use the Minio UI available at localhost:9001. Use the Minio’s credentials supplied as env variables to login."
  },
  {
    "objectID": "posts/2023-08-29-cdc-debezium-kafka-pg/index.html#analysis",
    "href": "posts/2023-08-29-cdc-debezium-kafka-pg/index.html#analysis",
    "title": "Change Data Capture with Debezium, Kafka, S3",
    "section": "Analysis",
    "text": "Analysis\nThe next part would be to use an analytics DB as such as DuckDB to perform analytics. A sample query that can used to generate SCD2 table.\n\n\nanalysis.sql\n\nWITH cte AS (\n    SELECT\n        COALESCE(CAST(json-&gt;'value'-&gt;'after'-&gt;&gt;'id' AS INTEGER), CAST(json-&gt;'value'-&gt;'before'-&gt;&gt;'id' AS INTEGER)) AS id,\n        json-&gt;'value'-&gt;&gt;'before' AS before_row_value,\n        json-&gt;'value'-&gt;&gt;'after' AS after_row_value,\n        CASE\n            WHEN json-&gt;'value'-&gt;&gt;'op' = 'c' THEN 'CREATE'\n            WHEN json-&gt;'value'-&gt;&gt;'op' = 'd' THEN 'DELETE'\n            WHEN json-&gt;'value'-&gt;&gt;'op' = 'u' THEN 'UPDATE'\n            WHEN json-&gt;'value'-&gt;&gt;'op' = 'r' THEN 'SNAPSHOT'\n            ELSE 'INVALID'\n        END AS operation_type,\n        json-&gt;'value'-&gt;'source'-&gt;&gt;'lsn' AS log_seq_num,\n        epoch_ms(CAST(json-&gt;'value'-&gt;'source'-&gt;&gt;'ts_ms' AS BIGINT)) AS source_timestamp\n    FROM\n        read_ndjson_objects('minio/data/commerce/debezium.commerce.products/*/*/*.json')\n    WHERE log_seq_num IS NOT NULL\n)\nSELECT\n    id,\n    CAST(after_row_value-&gt;&gt;'name' AS VARCHAR(255)) AS name,\n    CAST(after_row_value-&gt;&gt;'description' AS TEXT) AS description,\n    CAST(after_row_value-&gt;&gt;'price' AS NUMERIC(10, 2)) AS price,\n    source_timestamp AS row_valid_start_timestamp,\n    CASE \n        WHEN LEAD(source_timestamp, 1) OVER w_lead_txn_timestamp IS NULL THEN CAST('9999-01-01' AS TIMESTAMP) \n        ELSE LEAD(source_timestamp, 1) OVER w_lead_txn_timestamp \n    END AS row_valid_expiration_timestamp\nFROM cte\nWHERE id IN (SELECT id FROM cte GROUP BY id HAVING COUNT(*) &gt; 1)\nWINDOW w_lead_txn_timestamp AS (PARTITION BY id ORDER BY log_seq_num)\nORDER BY id, row_valid_start_timestamp\nLIMIT 20;\n\n\nMaking SCD2 table is out of scope for this project but no worries I will go through it another post."
  },
  {
    "objectID": "posts/2022-09-24-using-json-normalize/index.html",
    "href": "posts/2022-09-24-using-json-normalize/index.html",
    "title": "Using json_normalize Pandas function",
    "section": "",
    "text": "Javascript Object Notation(JSON) is a widely used format for storing and exchanging data. Coming from the relational database, it could be difficult to understand NoSQL databases that use JSON to store data and similarly REST API’s response. JSON is also used in storing football event data. It allows easy addition of features in the future.\nThough JSON format allows for easier exchange of data, for analysis, a tabular form would be appropriate. A JSON structure can be of two forms: a JSON object and list of JSON objects. Since our programming language of choice is Python, those structures can be somewhat called as a dictionary object or list of dicts.\n1\nImporting pandas library,\nimport pandas as pd"
  },
  {
    "objectID": "posts/2022-09-24-using-json-normalize/index.html#flattening-a-simple-json",
    "href": "posts/2022-09-24-using-json-normalize/index.html#flattening-a-simple-json",
    "title": "Using json_normalize Pandas function",
    "section": "1 Flattening a simple JSON",
    "text": "1 Flattening a simple JSON\nA dict\nLet us consider a simple dictionary: 3 keys and their respective values.\n\nviv = {\n    \"player_id\" : 15623, \n    \"player_name\" : \"Vivianne Miedema\", \n    \"jersey_number\" : 11}\nviv\n\n{'player_id': 15623, 'player_name': 'Vivianne Miedema', 'jersey_number': 11}\n\n\nWe use the json_normalize API2 to flatten a JSON dict.\n\ndf = pd.json_normalize(viv);df\n\n\n\n\n\n\n\n\nplayer_id\nplayer_name\njersey_number\n\n\n\n\n0\n15623\nVivianne Miedema\n11\n\n\n\n\n\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1 entries, 0 to 0\nData columns (total 3 columns):\n #   Column         Non-Null Count  Dtype \n---  ------         --------------  ----- \n 0   player_id      1 non-null      int64 \n 1   player_name    1 non-null      object\n 2   jersey_number  1 non-null      int64 \ndtypes: int64(2), object(1)\nmemory usage: 152.0+ bytes\n\n\n\nSide Note: If the data contains something that is not compatible with python, in this case a null variable, there are two choices:\n\n\n\nChange null to None\nPass the data through json.loads function\n\n\nChange null to None\n\nnull = None\nviv1 = { \"player_id\" : 15623, \"player_name\" : \"Vivianne Miedema\", \"jersey_number\" : 11, \"player_nickname\" : null}\nviv1\n\n{'player_id': 15623,\n 'player_name': 'Vivianne Miedema',\n 'jersey_number': 11,\n 'player_nickname': None}\n\n\nMake data as string and pass to json.loads\n\nimport json\nviv1 = '{ \"player_id\" : 15623, \"player_name\" : \"Vivianne Miedema\", \"jersey_number\" : 11, \"player_nickname\" : null}'\nviv1 = json.loads(viv1)\nviv1\n\n{'player_id': 15623,\n 'player_name': 'Vivianne Miedema',\n 'jersey_number': 11,\n 'player_nickname': None}\n\n\n\n1.1 A list of dicts\n\nplayer_list = [\n    { \"player_id\" : 15623, \"player_name\" : \"Vivianne Miedema\", \"jersey_number\" : 11, \"player_nickname\" : null },\n    { \"player_id\" : 10658, \"player_name\" : \"Danielle van de Donk\", \"jersey_number\" : 7, \"player_nickname\" : null }\n]\npd.json_normalize(player_list)\n\n\n\n\n\n\n\n\nplayer_id\nplayer_name\njersey_number\nplayer_nickname\n\n\n\n\n0\n15623\nVivianne Miedema\n11\nNone\n\n\n1\n10658\nDanielle van de Donk\n7\nNone\n\n\n\n\n\n\n\nWe have the JSON list of dicts in a tabular form. All the keys become columns and their values as entries.\nWhen we flattern a list with a key-value pair missing for an entry, instead of an error, NaN(not a number) is stored.\n\nplayer_list = [\n    { \"player_id\" : 15623, \"player_name\" : \"Vivianne Miedema\", \"jersey_number\" : 11, \"player_nickname\" : null },\n    { \"player_id\" : 10658, \"player_name\" : \"Danielle van de Donk\"}\n]\npd.json_normalize(player_list)\n\n\n\n\n\n\n\n\nplayer_id\nplayer_name\njersey_number\nplayer_nickname\n\n\n\n\n0\n15623\nVivianne Miedema\n11.0\nNaN\n\n\n1\n10658\nDanielle van de Donk\nNaN\nNaN\n\n\n\n\n\n\n\nNote: See how player_nickname when not specified also turns to NaN from None."
  },
  {
    "objectID": "posts/2022-09-24-using-json-normalize/index.html#flattening-a-multi-level-json",
    "href": "posts/2022-09-24-using-json-normalize/index.html#flattening-a-multi-level-json",
    "title": "Using json_normalize Pandas function",
    "section": "2 Flattening a multi-level JSON",
    "text": "2 Flattening a multi-level JSON\n\n2.1 A simple dict\n\nat_kick0ff = {\n  \"id\":\"d712fb93-c464-4621-98ba-f2bdcd5641db\",\n  \"timestamp\":\"00:00:00.000\",\n  \"duration\":0.0,\n  \"lineup\":{\n      \"player\":{\n        \"id\":15623,\n        \"name\":\"Vivianne Miedema\"\n      },\n      \"position\":{\n        \"id\":23,\n        \"name\":\"Center Forward\"\n      },\n      \"jersey_number\":11\n    }\n}\nat_kick0ff\n\n{'id': 'd712fb93-c464-4621-98ba-f2bdcd5641db',\n 'timestamp': '00:00:00.000',\n 'duration': 0.0,\n 'lineup': {'player': {'id': 15623, 'name': 'Vivianne Miedema'},\n  'position': {'id': 23, 'name': 'Center Forward'},\n  'jersey_number': 11}}\n\n\n\npd.json_normalize(at_kick0ff)\n\n\n\n\n\n\n\n\nid\ntimestamp\nduration\nlineup.player.id\nlineup.player.name\nlineup.position.id\nlineup.position.name\nlineup.jersey_number\n\n\n\n\n0\nd712fb93-c464-4621-98ba-f2bdcd5641db\n00:00:00.000\n0.0\n15623\nVivianne Miedema\n23\nCenter Forward\n11\n\n\n\n\n\n\n\nYou can see that lineup dictionary key’s nested key-value pairs have been expanded into individual columns. If you feel that is unnecessary, we can restrict expansion by using max_level argument. With max_level=1, the flattening goes one level deeper.\n\npd.json_normalize(at_kick0ff, max_level=1)\n\n\n\n\n\n\n\n\nid\ntimestamp\nduration\nlineup.player\nlineup.position\nlineup.jersey_number\n\n\n\n\n0\nd712fb93-c464-4621-98ba-f2bdcd5641db\n00:00:00.000\n0.0\n{'id': 15623, 'name': 'Vivianne Miedema'}\n{'id': 23, 'name': 'Center Forward'}\n11\n\n\n\n\n\n\n\n\n\n2.2 A list of dicts\n\nfirst_pass = [\n  {\n    \"id\":\"15758edb-58cd-49c4-a817-d2ef48ba3bcf\",\n    \"timestamp\":\"00:00:00.504\",\n    \"type\":{\n      \"id\":30,\n      \"name\":\"Pass\"\n    },\n    \"play_pattern\":{\n      \"id\":9,\n      \"name\":\"From Kick Off\"\n    },\n    \"player\":{\n      \"id\":15623,\n      \"name\":\"Vivianne Miedema\"\n    },\n    \"pass\":{\n      \"recipient\":{\n        \"id\":10666,\n        \"name\":\"Dominique Johanna Anna Bloodworth\"\n      },\n      \"length\":25.455845,\n      \"angle\":-2.3561945,\n      \"height\":{\n        \"id\":1,\n        \"name\":\"Ground Pass\"\n      },\n      \"end_location\":[\n        42.0,\n        22.0\n      ]\n    }\n  }, {\n  \"id\" : \"ab5674a4-e824-4143-9f6f-3f1645557413\",\n  \"timestamp\" : \"00:00:04.201\",\n  \"type\" : {\n    \"id\" : 30,\n    \"name\" : \"Pass\"\n  },\n  \"play_pattern\" : {\n    \"id\" : 9,\n    \"name\" : \"From Kick Off\"\n  },\n  \"player\" : {\n    \"id\" : 10666,\n    \"name\" : \"Dominique Johanna Anna Bloodworth\"\n  },\n  \"location\" : [ 45.0, 29.0 ],\n  \"duration\" : 1.795201,\n  \"pass\" : {\n    \"length\" : 51.62364,\n    \"angle\" : 0.55038595,\n    \"height\" : {\n      \"id\" : 3,\n      \"name\" : \"High Pass\"\n    },\n    \"end_location\" : [ 89.0, 56.0 ]\n  }\n}\n]\n    \npd.json_normalize(first_pass)\n\n\n\n\n\n\n\n\nid\ntimestamp\ntype.id\ntype.name\nplay_pattern.id\nplay_pattern.name\nplayer.id\nplayer.name\npass.recipient.id\npass.recipient.name\npass.length\npass.angle\npass.height.id\npass.height.name\npass.end_location\nlocation\nduration\n\n\n\n\n0\n15758edb-58cd-49c4-a817-d2ef48ba3bcf\n00:00:00.504\n30\nPass\n9\nFrom Kick Off\n15623\nVivianne Miedema\n10666.0\nDominique Johanna Anna Bloodworth\n25.455845\n-2.356194\n1\nGround Pass\n[42.0, 22.0]\nNaN\nNaN\n\n\n1\nab5674a4-e824-4143-9f6f-3f1645557413\n00:00:04.201\n30\nPass\n9\nFrom Kick Off\n10666\nDominique Johanna Anna Bloodworth\nNaN\nNaN\n51.623640\n0.550386\n3\nHigh Pass\n[89.0, 56.0]\n[45.0, 29.0]\n1.795201\n\n\n\n\n\n\n\nLimiting the levels…\n\npd.json_normalize(first_pass, max_level=0)\n\n\n\n\n\n\n\n\nid\ntimestamp\ntype\nplay_pattern\nplayer\npass\nlocation\nduration\n\n\n\n\n0\n15758edb-58cd-49c4-a817-d2ef48ba3bcf\n00:00:00.504\n{'id': 30, 'name': 'Pass'}\n{'id': 9, 'name': 'From Kick Off'}\n{'id': 15623, 'name': 'Vivianne Miedema'}\n{'recipient': {'id': 10666, 'name': 'Dominique...\nNaN\nNaN\n\n\n1\nab5674a4-e824-4143-9f6f-3f1645557413\n00:00:04.201\n{'id': 30, 'name': 'Pass'}\n{'id': 9, 'name': 'From Kick Off'}\n{'id': 10666, 'name': 'Dominique Johanna Anna ...\n{'length': 51.62364, 'angle': 0.55038595, 'hei...\n[45.0, 29.0]\n1.795201"
  },
  {
    "objectID": "posts/2022-09-24-using-json-normalize/index.html#flattening-a-json-nested-list",
    "href": "posts/2022-09-24-using-json-normalize/index.html#flattening-a-json-nested-list",
    "title": "Using json_normalize Pandas function",
    "section": "3 Flattening a JSON nested list",
    "text": "3 Flattening a JSON nested list\n\n3.1 A simple dict\nFor this case, let us consider a simpler example than of football event data. The key info has list of dictionaries inside its structure. We call it nested dict.\n\nawfc = {\n    'team': 'AWFC',\n    'location': 'London',\n    'ranking': 1,\n    'info': {\n        'manager': 'Joe',\n        'contacts': {\n          'email': {\n              'coaching': 'joe@afc.com',\n              'general': 'info@afc.com'\n          },\n          'tel': '123456789',\n      }\n    },\n    'players': [\n      { 'name': 'Viv' },\n      { 'name': 'DvD' },\n      { 'name': 'Kim' }\n    ],\n};awfc\n\n{'team': 'AWFC',\n 'location': 'London',\n 'ranking': 1,\n 'info': {'manager': 'Joe',\n  'contacts': {'email': {'coaching': 'joe@afc.com', 'general': 'info@afc.com'},\n   'tel': '123456789'}},\n 'players': [{'name': 'Viv'}, {'name': 'DvD'}, {'name': 'Kim'}]}\n\n\nThe players column has a list of dicts. So, we can flatten that column using record_path argument.\n\npd.json_normalize(awfc, record_path=['players'])\n\n\n\n\n\n\n\n\nname\n\n\n\n\n0\nViv\n\n\n1\nDvD\n\n\n2\nKim\n\n\n\n\n\n\n\nBut, making a separate table with no reference id has no meaning. To prevent that we can append revelant columns to the new table using meta argument. Here we want their team and Telephone number. The tel key lies within info-&gt;contacts-&gt;tel. So, we need provide that path like so ['info', 'contacts', 'tel'].\n\npd.json_normalize(awfc, record_path=['players'], meta=['team',['info', 'contacts', 'tel']])\n\n\n\n\n\n\n\n\nname\nteam\ninfo.contacts.tel\n\n\n\n\n0\nViv\nAWFC\n123456789\n\n\n1\nDvD\nAWFC\n123456789\n\n\n2\nKim\nAWFC\n123456789\n\n\n\n\n\n\n\nThe order in which those paths are mentioned, the order in which those columns are appended.\n\npd.json_normalize(awfc, record_path=['players'], meta=['team',['info', 'contacts', 'tel'],['info', 'manager']])\n\n\n\n\n\n\n\n\nname\nteam\ninfo.contacts.tel\ninfo.manager\n\n\n\n\n0\nViv\nAWFC\n123456789\nJoe\n\n\n1\nDvD\nAWFC\n123456789\nJoe\n\n\n2\nKim\nAWFC\n123456789\nJoe\n\n\n\n\n\n\n\n\n\n3.2 A list of dicts\n\njson_list = [\n    { \n        'team': 'arsenal', \n        'colour': 'red-white',\n        'info': {\n            'staff': { \n                'physio': 'xxxx', \n                'doctor': 'yyyy' \n            }\n        },\n        'players': [\n            { \n                'name': 'Viv', \n                'sex': 'F', \n                'stats': { 'goals': 101, 'assists': 40 } \n            },\n            { \n                'name': 'Beth', \n                'sex': 'F', \n                'stats': { 'goals': 60, 'assists': 25 } \n            },\n        ]\n    },\n    { \n        'team': 'city', \n        'colour': 'blue',\n        'info': {\n            'staff': { \n                'physio': 'aaaa', \n                'doctor': 'bbbb' \n            }\n        },\n        'players': [\n            { 'name': 'Steph', 'sex': 'F' },\n            { 'name': 'Lucy', 'sex': 'F' },\n        ]\n    },\n]\n\npd.json_normalize(json_list)\n\n\n\n\n\n\n\n\nteam\ncolour\nplayers\ninfo.staff.physio\ninfo.staff.doctor\n\n\n\n\n0\narsenal\nred-white\n[{'name': 'Viv', 'sex': 'F', 'stats': {'goals'...\nxxxx\nyyyy\n\n\n1\ncity\nblue\n[{'name': 'Steph', 'sex': 'F'}, {'name': 'Lucy...\naaaa\nbbbb\n\n\n\n\n\n\n\n\npd.json_normalize(json_list, record_path =['players'])\n\n\n\n\n\n\n\n\nname\nsex\nstats.goals\nstats.assists\n\n\n\n\n0\nViv\nF\n101.0\n40.0\n\n\n1\nBeth\nF\n60.0\n25.0\n\n\n2\nSteph\nF\nNaN\nNaN\n\n\n3\nLucy\nF\nNaN\nNaN\n\n\n\n\n\n\n\nHow about we now append the players’ team, colour, and their physio.\n\npd.json_normalize(\n    json_list, \n    record_path =['players'], \n    meta=['team', 'colour', ['info', 'staff', 'physio']]\n)\n\n\n\n\n\n\n\n\nname\nsex\nstats.goals\nstats.assists\nteam\ncolour\ninfo.staff.physio\n\n\n\n\n0\nViv\nF\n101.0\n40.0\narsenal\nred-white\nxxxx\n\n\n1\nBeth\nF\n60.0\n25.0\narsenal\nred-white\nxxxx\n\n\n2\nSteph\nF\nNaN\nNaN\ncity\nblue\naaaa\n\n\n3\nLucy\nF\nNaN\nNaN\ncity\nblue\naaaa"
  },
  {
    "objectID": "posts/2022-09-24-using-json-normalize/index.html#ignoring-key-errors",
    "href": "posts/2022-09-24-using-json-normalize/index.html#ignoring-key-errors",
    "title": "Using json_normalize Pandas function",
    "section": "4 Ignoring key errors",
    "text": "4 Ignoring key errors\n\njson_list = [\n    { \n        'team': 'arsenal', \n        'colour': 'red-white',\n        'info': {\n            'staff': { \n                'physio': 'xxxx', \n                'doctor': 'yyyy' \n            }\n        },\n        'players': [\n            { \n                'name': 'Viv', \n                'sex': 'F', \n                'stats': { 'goals': 101, 'assists': 40 } \n            },\n            { \n                'name': 'Beth', \n                'sex': 'F', \n                'stats': { 'goals': 60, 'assists': 25 } \n            },\n        ]\n    },\n    { \n        'team': 'city', \n        'colour': 'blue',\n        'info': {\n            'staff': { \n                'doctor': 'bbbb' \n            }\n        },\n        'players': [\n            { 'name': 'Steph', 'sex': 'F' },\n            { 'name': 'Lucy', 'sex': 'F' },\n        ]\n    },\n]\n\nNotice that the key physio is missing from the entry team=city. What happens if we try to access physio key inside meta?\n\npd.json_normalize(\n    json_list, \n    record_path =['players'], \n    meta=['team', 'colour', ['info', 'staff', 'physio']],\n)\n\nKeyError: \"Key 'physio' not found. To replace missing values of 'physio' with np.nan, pass in errors='ignore'\"\n\n\nHow come stats.goals and stats.assists didn’t generate an error but that above does? Because, the meta argument expects values to be present for listed keys in meta by default. We can ignore those errors(as suggested) using errors='ignore'\n\npd.json_normalize(\n    json_list, \n    record_path =['players'], \n    meta=['team', 'colour', ['info', 'staff', 'physio']],\n    errors='ignore'\n)\n\n\n\n\n\n\n\n\nname\nsex\nstats.goals\nstats.assists\nteam\ncolour\ninfo.staff.physio\n\n\n\n\n0\nViv\nF\n101.0\n40.0\narsenal\nred-white\nxxxx\n\n\n1\nBeth\nF\n60.0\n25.0\narsenal\nred-white\nxxxx\n\n\n2\nSteph\nF\nNaN\nNaN\ncity\nblue\nNaN\n\n\n3\nLucy\nF\nNaN\nNaN\ncity\nblue\nNaN"
  },
  {
    "objectID": "posts/2022-09-24-using-json-normalize/index.html#custom-separator-sep",
    "href": "posts/2022-09-24-using-json-normalize/index.html#custom-separator-sep",
    "title": "Using json_normalize Pandas function",
    "section": "5 Custom separator sep",
    "text": "5 Custom separator sep\nWe notice that by default pandas uses . to indicate the direction of the path. We can change that using the sep argument.\n\nTip: Usually an underscore is used instead of .\n\n\njson_list = [\n    { \n        'team': 'arsenal', \n        'colour': 'red-white',\n        'info': {\n            'staff': { \n                'physio': 'xxxx', \n                'doctor': 'yyyy' \n            }\n        },\n        'players': [\n            { \n                'name': 'Viv', \n                'sex': 'F', \n                'stats': { 'goals': 101, 'assists': 40 } \n            },\n            { \n                'name': 'Beth', \n                'sex': 'F', \n                'stats': { 'goals': 60, 'assists': 25 } \n            },\n        ]\n    },\n    { \n        'team': 'city', \n        'colour': 'blue',\n        'info': {\n            'staff': { \n                'physio': 'aaaa', \n                'doctor': 'bbbb' \n            }\n        },\n        'players': [\n            { 'name': 'Steph', 'sex': 'F' },\n            { 'name': 'Lucy', 'sex': 'F' },\n        ]\n    },\n]\n\n\npd.json_normalize(\n    json_list, \n    record_path =['players'], \n    meta=['team', 'colour', ['info', 'staff', 'physio']],\n    sep='-&gt;'\n)\n\n\n\n\n\n\n\n\nname\nsex\nstats-&gt;goals\nstats-&gt;assists\nteam\ncolour\ninfo-&gt;staff-&gt;physio\n\n\n\n\n0\nViv\nF\n101.0\n40.0\narsenal\nred-white\nxxxx\n\n\n1\nBeth\nF\n60.0\n25.0\narsenal\nred-white\nxxxx\n\n\n2\nSteph\nF\nNaN\nNaN\ncity\nblue\naaaa\n\n\n3\nLucy\nF\nNaN\nNaN\ncity\nblue\naaaa"
  },
  {
    "objectID": "posts/2022-09-24-using-json-normalize/index.html#adding-context-to-record-and-meta-data-using-record_prefix-and-meta_prefix",
    "href": "posts/2022-09-24-using-json-normalize/index.html#adding-context-to-record-and-meta-data-using-record_prefix-and-meta_prefix",
    "title": "Using json_normalize Pandas function",
    "section": "6 Adding context to record and meta data using record_prefix and meta_prefix",
    "text": "6 Adding context to record and meta data using record_prefix and meta_prefix\n\npd.json_normalize(\n    json_list, \n    record_path=['players'], \n    meta=['team', 'colour', ['info', 'staff', 'physio']],\n    meta_prefix='meta-',\n    record_prefix='player-',\n    sep='-&gt;'\n)\n\n\n\n\n\n\n\n\nplayer-name\nplayer-sex\nplayer-stats-&gt;goals\nplayer-stats-&gt;assists\nmeta-team\nmeta-colour\nmeta-info-&gt;staff-&gt;physio\n\n\n\n\n0\nViv\nF\n101.0\n40.0\narsenal\nred-white\nxxxx\n\n\n1\nBeth\nF\n60.0\n25.0\narsenal\nred-white\nxxxx\n\n\n2\nSteph\nF\nNaN\nNaN\ncity\nblue\naaaa\n\n\n3\nLucy\nF\nNaN\nNaN\ncity\nblue\naaaa"
  },
  {
    "objectID": "posts/2022-09-24-using-json-normalize/index.html#working-with-a-local-file",
    "href": "posts/2022-09-24-using-json-normalize/index.html#working-with-a-local-file",
    "title": "Using json_normalize Pandas function",
    "section": "7 Working with a local file",
    "text": "7 Working with a local file\nIn most scenarios, we won’t be making new JSON object ourselves instead use JSON formatted files. We make use python’s json module and read the file, then use pandas’ json_normalize to flatten it into a dataframe.\n\nimport json\n# load data using Python JSON module\nwith open('movies.json') as f:\n    data = json.load(f)\n    \n# Normalizing data\npd.json_normalize(data)\n\n\n\n\n\n\n\n\nTitle\nUS Gross\nWorldwide Gross\nUS DVD Sales\nProduction Budget\nRelease Date\nMPAA Rating\nRunning Time min\nDistributor\nSource\nMajor Genre\nCreative Type\nDirector\nRotten Tomatoes Rating\nIMDB Rating\nIMDB Votes\n\n\n\n\n0\nThe Land Girls\n146083\n146083\nNaN\n8000000\nJun 12 1998\nR\nNaN\nGramercy\nNone\nNone\nNone\nNone\nNaN\n6.1\n1071.0\n\n\n1\nFirst Love, Last Rites\n10876\n10876\nNaN\n300000\nAug 07 1998\nR\nNaN\nStrand\nNone\nDrama\nNone\nNone\nNaN\n6.9\n207.0\n\n\n2\nI Married a Strange Person\n203134\n203134\nNaN\n250000\nAug 28 1998\nNone\nNaN\nLionsgate\nNone\nComedy\nNone\nNone\nNaN\n6.8\n865.0\n\n\n3\nFour Rooms\n4301000\n4301000\nNaN\n4000000\nDec 25 1995\nR\nNaN\nMiramax\nOriginal Screenplay\nComedy\nContemporary Fiction\nRobert Rodriguez\n14.0\n6.4\n34328.0\n\n\n4\nThe Four Seasons\n42488161\n42488161\nNaN\n6500000\nMay 22 1981\nNone\nNaN\nUniversal\nOriginal Screenplay\nComedy\nContemporary Fiction\nAlan Alda\n71.0\n7.0\n1814.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n63\nBig Things\n0\n0\nNaN\n50000\nDec 31 2009\nNone\nNaN\nNone\nNone\nNone\nNone\nNone\nNaN\nNaN\nNaN\n\n\n64\nBogus\n4357406\n4357406\nNaN\n32000000\nSep 06 1996\nPG\nNaN\nWarner Bros.\nOriginal Screenplay\nComedy\nFantasy\nNorman Jewison\n40.0\n4.8\n2742.0\n\n\n65\nBeverly Hills Cop\n234760478\n316300000\nNaN\n15000000\nDec 05 1984\nNone\nNaN\nParamount Pictures\nOriginal Screenplay\nAction\nContemporary Fiction\nMartin Brest\n83.0\n7.3\n45065.0\n\n\n66\nBeverly Hills Cop II\n153665036\n276665036\nNaN\n20000000\nMay 20 1987\nR\nNaN\nParamount Pictures\nOriginal Screenplay\nAction\nContemporary Fiction\nTony Scott\n46.0\n6.1\n29712.0\n\n\n67\nBeverly Hills Cop III\n42586861\n119180938\nNaN\n50000000\nMay 25 1994\nR\nNaN\nParamount Pictures\nOriginal Screenplay\nAction\nContemporary Fiction\nJohn Landis\n10.0\n5.0\n21199.0\n\n\n\n\n68 rows × 16 columns"
  },
  {
    "objectID": "posts/2022-09-24-using-json-normalize/index.html#working-with-url",
    "href": "posts/2022-09-24-using-json-normalize/index.html#working-with-url",
    "title": "Using json_normalize Pandas function",
    "section": "8 Working with URL",
    "text": "8 Working with URL\nReading a JSON file from an url needs an extra module in requests as any data from the Internet carries overheads that are necessary for efficient exchange of information(REST API). So, in order to read the file contents, we call upon requests’ text attribute which fetches the contents of the file.\nHere, we use json.loads and not json.load as loads function expects contents(string) rather than a file pointer. If looked closely into the json module, the load calls loads using read() on the file.\n\nimport requests\n\nURL = 'https://vega.github.io/vega-datasets/data/cars.json'\n\ndata = json.loads(requests.get(URL).text)\npd.json_normalize(data)\n\n\n\n\n\n\n\n\nName\nMiles_per_Gallon\nCylinders\nDisplacement\nHorsepower\nWeight_in_lbs\nAcceleration\nYear\nOrigin\n\n\n\n\n0\nchevrolet chevelle malibu\n18.0\n8\n307.0\n130.0\n3504\n12.0\n1970-01-01\nUSA\n\n\n1\nbuick skylark 320\n15.0\n8\n350.0\n165.0\n3693\n11.5\n1970-01-01\nUSA\n\n\n2\nplymouth satellite\n18.0\n8\n318.0\n150.0\n3436\n11.0\n1970-01-01\nUSA\n\n\n3\namc rebel sst\n16.0\n8\n304.0\n150.0\n3433\n12.0\n1970-01-01\nUSA\n\n\n4\nford torino\n17.0\n8\n302.0\n140.0\n3449\n10.5\n1970-01-01\nUSA\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n401\nford mustang gl\n27.0\n4\n140.0\n86.0\n2790\n15.6\n1982-01-01\nUSA\n\n\n402\nvw pickup\n44.0\n4\n97.0\n52.0\n2130\n24.6\n1982-01-01\nEurope\n\n\n403\ndodge rampage\n32.0\n4\n135.0\n84.0\n2295\n11.6\n1982-01-01\nUSA\n\n\n404\nford ranger\n28.0\n4\n120.0\n79.0\n2625\n18.6\n1982-01-01\nUSA\n\n\n405\nchevy s-10\n31.0\n4\n119.0\n82.0\n2720\n19.4\n1982-01-01\nUSA\n\n\n\n\n406 rows × 9 columns"
  },
  {
    "objectID": "posts/2022-09-24-using-json-normalize/index.html#conclusion",
    "href": "posts/2022-09-24-using-json-normalize/index.html#conclusion",
    "title": "Using json_normalize Pandas function",
    "section": "9 Conclusion",
    "text": "9 Conclusion\nWe saw the use of json_normalize function in pandas library. It helps take a JSON data, flatten it, and make it as a dataframe for easier analysis."
  },
  {
    "objectID": "posts/2022-09-24-using-json-normalize/index.html#footnotes",
    "href": "posts/2022-09-24-using-json-normalize/index.html#footnotes",
    "title": "Using json_normalize Pandas function",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nB. Chen, https://towardsdatascience.com/all-pandas-json-normalize-you-should-know-for-flattening-json-13eae1dfb7dd↩︎\nPandas documentation, https://pandas.pydata.org/pandas-docs/version/1.2.0/reference/api/pandas.json_normalize.html↩︎"
  },
  {
    "objectID": "posts/2023-06-03-deploy-with-docker/index.html",
    "href": "posts/2023-06-03-deploy-with-docker/index.html",
    "title": "Deploy Web Application with Docker",
    "section": "",
    "text": "A customer wants to build a web application on machine learning trained model. They approach us to find a simple solution. They give us a trained model file in parquet format. We need to look up for a potential sale price of a product within the given time period and store number. How can it be achieved?\n\n\nThere are three possibilities to solve this problem:\n\nUse python testing functionality, give the inputs and generate outputs.\nSpin up a Flask application with an endpoint, pass arguments into it and generate outputs.\nBuild a docker image with Flask and its dependencies, run it and pass arguments.\n\nAll the three approaches are technical. In other words, there are chances of failures that may be replicable. To allow for minimal error, we will choose the docker option which packages the second, Flask application."
  },
  {
    "objectID": "posts/2023-06-03-deploy-with-docker/index.html#possible-approaches",
    "href": "posts/2023-06-03-deploy-with-docker/index.html#possible-approaches",
    "title": "Deploy Web Application with Docker",
    "section": "",
    "text": "There are three possibilities to solve this problem:\n\nUse python testing functionality, give the inputs and generate outputs.\nSpin up a Flask application with an endpoint, pass arguments into it and generate outputs.\nBuild a docker image with Flask and its dependencies, run it and pass arguments.\n\nAll the three approaches are technical. In other words, there are chances of failures that may be replicable. To allow for minimal error, we will choose the docker option which packages the second, Flask application."
  },
  {
    "objectID": "posts/2023-06-03-deploy-with-docker/index.html#why-flask",
    "href": "posts/2023-06-03-deploy-with-docker/index.html#why-flask",
    "title": "Deploy Web Application with Docker",
    "section": "Why Flask?",
    "text": "Why Flask?\nFlask is a popular Python microframework for developing web applications. It is also known for its simplicity, flexibility and scalability. Since we want to use Python and make it simple for us to maintain, we will choose Flask."
  },
  {
    "objectID": "posts/2023-06-03-deploy-with-docker/index.html#flask-application",
    "href": "posts/2023-06-03-deploy-with-docker/index.html#flask-application",
    "title": "Deploy Web Application with Docker",
    "section": "Flask application",
    "text": "Flask application\nA web application is similar to visiting a website through an address. Such as visiting your gmail account using mail.google.com. The gmail website is hosted in many servers across the world. A server closest to your location will serve you when you open up the website. Similarly in our case we need to host our application somewhere. We will, for now, choose the local option; meaning in our local machine.\nUsing Flask we host our application in a particular location(endpoint) and through a port locally. I choose this location to be http://localhost:9696/predict-sales.\nBelow is the code for the flask application.\n\n\nflask-app.py\n\nimport pandas as pd\nfrom flask import Flask, jsonify, request\nfrom zipfile import ZipFile as zf\nimport os\n\ndef read_parquet_files(filename: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Read parquet file format for given filename and returns the contents\n    \"\"\"\n    df = pd.read_parquet(filename, engine=\"pyarrow\")\n    return df\n\n\nif os.path.exists(\"lgb_preds.parquet\"):\n    df_test_preds = read_parquet_files(\"lgb_preds.parquet\")\nelse:   \n    with zf(\"lgb_preds.parquet.zip\") as zipfile:\n        zipfile.extract(\"lgb_preds.parquet\")\n1    df_test_preds = read_parquet_files(\"lgb_preds.parquet\")\n\ndf_items = read_parquet_files(\"items.parquet\")\n\n2app = Flask(\"flask-unit-sales-prediction\")\n\n\ndef predict(find: request, item_idx: int) -&gt; float:\n    \"\"\"\n    Takes the json inputs, processes it and outputs the unit sales\n    \"\"\"\n    try:\n        idx = pd.IndexSlice\n        x = df_test_preds.loc[idx[find[\"store_nbr\"], item_idx, find[\"date1\"]]][\n            \"unit_sales\"\n        ]\n    except KeyError:\n        print(\"This item is not present this store. Try some other item\")\n        return -0.0\n    else:\n        return float(round(x, 2))\n\n\n3@app.route(\"/predict-sales\", methods=[\"POST\"])\ndef predict_endpoint():\n    \"\"\"\n    flask predict endpoint\n    \"\"\"\n    find = request.get_json()\n    item = df_items.sample(1)\n    item_idx, item_family = item.index[0], item[\"family\"].values[0]\n    pred_unit_sales = predict(find, item_idx)\n    result = {\n        \" Store\": find[\"store_nbr\"],\n        \" item\": int(item_idx),\n        \"Family\": item_family,\n        \"Prediction date\": find[\"date1\"],\n        \"Unit_sales\": pred_unit_sales,\n    }\n\n4    return jsonify(result)\n\n\nif __name__ == \"__main__\":\n5    app.run(debug=True, host=\"0.0.0.0\", port=9696)\n\n\n1\n\nGetting the trained model table\n\n2\n\nInitialising the Flask app\n\n3\n\nDeclaring the endpoint with “POST” as method\n\n4\n\nPredicted result that converted to JSON format\n\n5\n\nSpin up flask app\n\n\nFlask uses a library called gunicorn to spin up a HTML server. In the code we have dependencies for pandas, pyarrow, numpy. We can’t ask the customer to install these themselves. We intend to use Docker to prevent that criteria. But how to install these dependencies in a docker container?"
  },
  {
    "objectID": "posts/2023-06-03-deploy-with-docker/index.html#dependencies-install",
    "href": "posts/2023-06-03-deploy-with-docker/index.html#dependencies-install",
    "title": "Deploy Web Application with Docker",
    "section": "Dependencies install",
    "text": "Dependencies install\nThere are number of options available for us:\n\nUse pip and requirements.txt. But tracking package versions might get tedious.\nUse Anaconda/Miniconda. For local machine they’re fine but for docker container?they install so many unnecessary libraries making container huge in size.\nUse Poetry. This one is developed to deliver packages. But still it includes libraries unnecessary to us.\nUse Pipenv. It is much smaller, easy to write and use. Hence I will choose this."
  },
  {
    "objectID": "posts/2023-06-03-deploy-with-docker/index.html#pipenv-pipfile-and-pipfile.lock",
    "href": "posts/2023-06-03-deploy-with-docker/index.html#pipenv-pipfile-and-pipfile.lock",
    "title": "Deploy Web Application with Docker",
    "section": "Pipenv, Pipfile and Pipfile.lock",
    "text": "Pipenv, Pipfile and Pipfile.lock\nPipenv is an advanced version of requirements.txt. It uses inline tables and TOML spec. We can easily give the package versions we want to install and pipenv internally creates a Pipfile.lock file that contains hashes to each package installed. A convenient solution for our purpose.\nOur Pipfile looks like this:\n\n\nPipfile\n\n[[source]]\nurl = \"https://pypi.org/simple\"\nverify_ssl = true\nname = \"pypi\"\n\n[packages]\npandas = \"*\"\nflask = \"*\"\npyarrow = \"*\"\ngunicorn = \"*\"\nnumpy = \"*\"\n\n[dev-packages]\nrequests = \"*\"\n\n[requires]\npython_version = \"3.9\"\n\nOn the first run we need to install these packages locally and also to create Pipfile.lock. We can accomplish that using the command pipenv install --dev. I recommend running this inside conda’s base environment. Please don’t use system’s python to manage projects."
  },
  {
    "objectID": "posts/2023-06-03-deploy-with-docker/index.html#dockerfile-and-building-docker-image",
    "href": "posts/2023-06-03-deploy-with-docker/index.html#dockerfile-and-building-docker-image",
    "title": "Deploy Web Application with Docker",
    "section": "Dockerfile and building docker image",
    "text": "Dockerfile and building docker image\nDockerfile is a text file that give instructions to docker engine to build an docker image. We choose python 3.9-slim version, install pip and pipenv, copy all the necessary files, importantly expose the port 9696 and set gunicorn http server as the entrypoint to the image whenever the container is run.\n\n\nDockerfile\n\nFROM python:3.9.14-slim\n\nRUN pip install -U pip\nRUN pip install pipenv\n\nWORKDIR /app\n\nCOPY [ \"Pipfile\", \"Pipfile.lock\", \"./\" ]\n\nRUN pipenv install --system --deploy\n\nCOPY [ \"lgb_preds.parquet.zip\" , \"lgb_preds.parquet.zip\" ]\nCOPY [ \"items.parquet\" , \"items.parquet\" ]\n\nCOPY [\"flask_app.py\", \"flask_app.py\"]\n\nEXPOSE 9696\n\nENTRYPOINT [ \"gunicorn\", \"--bind=0.0.0.0:9696\", \"flask_sales_predictor:app\" ]\n\nTo build a docker image, we use the command docker build -t flask_app:v1 . .\nTo verify it is created we can run docker images and look for the name."
  },
  {
    "objectID": "posts/2023-06-03-deploy-with-docker/index.html#running-the-docker-container",
    "href": "posts/2023-06-03-deploy-with-docker/index.html#running-the-docker-container",
    "title": "Deploy Web Application with Docker",
    "section": "Running the docker container",
    "text": "Running the docker container\nOur software is now packaged as an image. We need to somehow run it to use our application. We can run it through this command\n\n\nTerminal 1\n\ndocker run \\\n    -it \\\n    --rm \\\n    -p 9696:9696 \\\n    flask_app:v1"
  },
  {
    "objectID": "posts/2023-06-03-deploy-with-docker/index.html#testing-the-application",
    "href": "posts/2023-06-03-deploy-with-docker/index.html#testing-the-application",
    "title": "Deploy Web Application with Docker",
    "section": "Testing the application",
    "text": "Testing the application\nThe docker container should be up and running in terminal 1. Now all that is left is test it. For that we need one more file.\n\n\ntest_requests.py in terminal 2\n\nimport requests\n\nfind = {\"date1\": \"2017-08-21\", \"store_nbr\": 19}\n\nurl = \"http://localhost:9696/predict-sales\"\nresponse = requests.post(url, json=find, timeout=10)\nprint(response.json())\n\nThe test_requests.py file has the input in JSON format and the endpoint url http://localhost:9696/predict-sales. We leverage the requests library to make our requests.\nOpen up another terminal or another tab, activate virtual environment and run python test_requests.py. Just python needs to be present inside this environment as everything else is inside the docker container.\nIf everything goes well, we should the predicted product price printed."
  },
  {
    "objectID": "notes/linux/index.html",
    "href": "notes/linux/index.html",
    "title": "Useful things found while using Linux",
    "section": "",
    "text": "Here I note down all things related to Linux which I find interesting. Those can be just short notes or tips and tricks to future reference.\n\nMakefile\nMakefile is a wonderful utility application that allows to define rules which are then run in the order of their definition.\ntarget ... : prerequisites ...\n        command\n        ...\n        ...\nMostly Makefile makes it easier to execute two or more commands in a single execution or abbreviate a lengthy command.\nFor example, docker-compose up -d can be abbreviated or aliased as make up.\nOr, make pg-src runs\ncurl -i -X POST -H \"Accept:application/json\" -H \"Content-Type:application/json\" localhost:8083/connectors/ -d '@./connectors/pg-src-connector.json'\nOr, make tsetup which expands to tsetup: tup tc ci runs two or more make commands sequentially as those targets mentioned as prerequistes.\ntup:\n    docker-compose -f docker-compose-test.yml up -d\n    @echo -n \"sleeping for 30s to launch all containers...\"\n    @sleep 30\n\ntest-connections:\n    . ./tests/setup-connections.sh\n    @echo -n \"sleeping for 10s to get ready for testing...\"\n    @sleep 10\n\nci:\n    docker exec test_suite pytest -p no:warnings -v\nThe @ flag infront of an echo command or any Linux command, suppresses its output\nUsually each line inside a target gets displayed before executing. Sometimes it is unnecessary. For example, @echo -n \"sleeping for 10s to get ready for testing...\" won’t display echo -n \"sleeping for 10s to get ready for testing...\" but rather just sleeping for 30s to launch all containers....\n@ flag in front of sleep command suppresses sleep 30 being displayed as we already written a longer sentence previously."
  },
  {
    "objectID": "notes/data-engineering/apache-Kafka/index.html",
    "href": "notes/data-engineering/apache-Kafka/index.html",
    "title": "Apache Kafka notes",
    "section": "",
    "text": "Kafka requires Zookeeper(ZK) to handle its configurations. Zookeeper is a meta-data management service tool.\nKafka brokers use ZK to determine which broker is the leader of a given partition and topic and perform leader elections.\nZK stores configurations for topics and permissions.\nZK sends notifications to Kafka in case of changes such as new topic, broker died, broker restarted."
  },
  {
    "objectID": "notes/data-engineering/apache-Kafka/index.html#zookeeper",
    "href": "notes/data-engineering/apache-Kafka/index.html#zookeeper",
    "title": "Apache Kafka notes",
    "section": "",
    "text": "Kafka requires Zookeeper(ZK) to handle its configurations. Zookeeper is a meta-data management service tool.\nKafka brokers use ZK to determine which broker is the leader of a given partition and topic and perform leader elections.\nZK stores configurations for topics and permissions.\nZK sends notifications to Kafka in case of changes such as new topic, broker died, broker restarted."
  },
  {
    "objectID": "notes/data-engineering/apache-Kafka/index.html#kafka-listeners",
    "href": "notes/data-engineering/apache-Kafka/index.html#kafka-listeners",
    "title": "Apache Kafka notes",
    "section": "Kafka Listeners",
    "text": "Kafka Listeners\nThis note is learnings from this blog post on Kafka listeners.\nKafka listeners are important server properties in Kafka. Both KAFKA_ADVERTISED_LISTENERS and KAFKA_LISTENERS configure how Kafka brokers interact with each other and with others. Kafka being a distributed system utilises these properties to manage interactions.\nOn a single machine everything can be localhost but in a distributed system it is not often so. Most often Kafka is run in Kubernetes cloud within docker. To interact with individual brokers, we need to address each broker specifically. The blog details configurations clearly. For simpler cases, these are my takeaways:\n\nA listener is combination of Host/IP, Port and Protocol.\nKAFKA_ADVERTISED_LISTENERS and KAFKA_LISTENERS have two components – one host/ip to interact internally with themselves(defined by KAFKA_INTER_BROKER_LISTENER_NAME) and one to interact with others.\nInternally it is usually docker hostname kafka followed by a port number. Example: kafka:9092.\nExternally, it is usually localhost with a port.\nDebezium which connects to kafka, uses internal listener. kafka:9092 as bootstrap-server."
  },
  {
    "objectID": "notes/data-engineering/apache-Kafka/index.html#schema-registry",
    "href": "notes/data-engineering/apache-Kafka/index.html#schema-registry",
    "title": "Apache Kafka notes",
    "section": "Schema Registry",
    "text": "Schema Registry\nSchema Registry provides a centralized repository for managing and validating schemas for topic message data, and for serialization and deserialization of the data over the network. Producers and consumers to Kafka topics can use schemas to ensure data consistency and compatibility as schemas evolve.\nSchema Registry is a key component for data governance, helping to ensure data quality, adherence to standards, visibility into data lineage, audit capabilities, collaboration across teams, efficient application development protocols, and system performance."
  },
  {
    "objectID": "notes/data-engineering/postgres/index.html",
    "href": "notes/data-engineering/postgres/index.html",
    "title": "Postgres - notes",
    "section": "",
    "text": "An insert, update or delete operation on the table in the DB is called a transaction. When a transaction occurs, the transaction is logged in a log file called Write Ahead Log(WAL) in disk. In case of a database crash we may loose the cache but the database can recover using the logs in WAL in disk. A WAL is append-only log file. More on this log, read its Wikipedia page.\nNow a service has to do is just monitor this file for changes. This process of having backup or replicating data is called replication.\nFor our task we need to make sure WAL level is set correctly. For that we need to check the postgresql.conf file.\n\n\n/var/lib/postgresql/data/postgresql.conf\n\ncat /var/lib/postgresql/data/postgresql.conf | grep -iE \"max_wal|wal_level\"\n#wal_level = replica                    # minimal, replica, or logical\n#max_wal_senders = 10           # max number of walsender processes\n\nUncomment them, change wal_level to logical and leave max_wal_senders as 10.\n\n\nThe minimal WAL does not contain sufficient information for point-in-time recovery, so replica or higher must be used to enable continuous archiving (archive_mode) and streaming binary replication.\nIn logical level, the same information is logged as with replica, plus information needed to extract logical change sets from the WAL. Using a level of logical will increase the WAL volume, particularly if many tables are configured for REPLICA IDENTITY FULL and many UPDATE and DELETE statements are executed.1"
  },
  {
    "objectID": "notes/data-engineering/postgres/index.html#using-postgres-db-as-source-for-streaming-change-data",
    "href": "notes/data-engineering/postgres/index.html#using-postgres-db-as-source-for-streaming-change-data",
    "title": "Postgres - notes",
    "section": "",
    "text": "An insert, update or delete operation on the table in the DB is called a transaction. When a transaction occurs, the transaction is logged in a log file called Write Ahead Log(WAL) in disk. In case of a database crash we may loose the cache but the database can recover using the logs in WAL in disk. A WAL is append-only log file. More on this log, read its Wikipedia page.\nNow a service has to do is just monitor this file for changes. This process of having backup or replicating data is called replication.\nFor our task we need to make sure WAL level is set correctly. For that we need to check the postgresql.conf file.\n\n\n/var/lib/postgresql/data/postgresql.conf\n\ncat /var/lib/postgresql/data/postgresql.conf | grep -iE \"max_wal|wal_level\"\n#wal_level = replica                    # minimal, replica, or logical\n#max_wal_senders = 10           # max number of walsender processes\n\nUncomment them, change wal_level to logical and leave max_wal_senders as 10.\n\n\nThe minimal WAL does not contain sufficient information for point-in-time recovery, so replica or higher must be used to enable continuous archiving (archive_mode) and streaming binary replication.\nIn logical level, the same information is logged as with replica, plus information needed to extract logical change sets from the WAL. Using a level of logical will increase the WAL volume, particularly if many tables are configured for REPLICA IDENTITY FULL and many UPDATE and DELETE statements are executed.1"
  },
  {
    "objectID": "notes/data-engineering/postgres/index.html#replica-identity",
    "href": "notes/data-engineering/postgres/index.html#replica-identity",
    "title": "Postgres - notes",
    "section": "Replica Identity",
    "text": "Replica Identity\nhttps://www.postgresql.org/docs/current/sql-altertable.html#SQL-ALTERTABLE-REPLICA-IDENTITY"
  },
  {
    "objectID": "notes/data-engineering/postgres/index.html#footnotes",
    "href": "notes/data-engineering/postgres/index.html#footnotes",
    "title": "Postgres - notes",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nRuntime WAL config - https://www.postgresql.org/docs/current/runtime-config-wal.html#RUNTIME-CONFIG-WAL-SETTINGS↩︎"
  },
  {
    "objectID": "notes/data-engineering/Kafka-connect/index.html",
    "href": "notes/data-engineering/Kafka-connect/index.html",
    "title": "notes on Kafka Connect",
    "section": "",
    "text": "Use this wonderful video to learn in detail about Kafka Connect."
  },
  {
    "objectID": "notes/python_concepts.html",
    "href": "notes/python_concepts.html",
    "title": "Python concepts notes",
    "section": "",
    "text": "These are some of my notes on python concepts I gathered from various sources."
  },
  {
    "objectID": "notes/python_concepts.html#super",
    "href": "notes/python_concepts.html#super",
    "title": "Python concepts notes",
    "section": "Super()",
    "text": "Super()\nsuper() allows to call the base class implicitly without the need to use the base class name explicitly. The main advantage is seen during multiple inheritance. The child classes that may be using cooperative multiple inheritance will call the correct next parent class function in the Method Resolution Order(MRO).\nSyntax Python3 super().__init__() Python2 and still valid in python3 super(childclassname,self).__init__()\nAvoid using like this – super(self.__class__,self).__init__(). This leads to recursion but fortunately python3 syntax standard side steps this problem.\nMore info in stackoverflow"
  },
  {
    "objectID": "notes/python_concepts.html#pycache__",
    "href": "notes/python_concepts.html#pycache__",
    "title": "Python concepts notes",
    "section": "__Pycache__",
    "text": "__Pycache__\n__pycache__ is a directory containing python3 bytecode compiled and ready to be executed. This is done to speed up loading of the modules. Python caches the compiled version of each module in this directory. More info in the python official docs"
  },
  {
    "objectID": "notes/python_concepts.html#zip-function",
    "href": "notes/python_concepts.html#zip-function",
    "title": "Python concepts notes",
    "section": "Zip function",
    "text": "Zip function\nThis function enables combination of elements in 2 or more lists. If the lists are of unequal lengths then the minimum length is taken.\nFor example - zip([1,2,3],[\"one\", \"two\"]) returns a tuple (1, \"one\"), (2, \"two\"). Notice how 3 is not part of the zip operation."
  },
  {
    "objectID": "notes/python_concepts.html#lambda-function",
    "href": "notes/python_concepts.html#lambda-function",
    "title": "Python concepts notes",
    "section": "Lambda function",
    "text": "Lambda function\nAlso knows as anonymous functions helps reduce the need to define unnecessary custom functions. For example, a function that returns a simple arithmetic operation can be made as a lambda function.\nExample - lambda x,y,...: x+y+... - lambda takes several arguments but returns only one expression."
  },
  {
    "objectID": "notes/python_concepts.html#map-function",
    "href": "notes/python_concepts.html#map-function",
    "title": "Python concepts notes",
    "section": "Map function",
    "text": "Map function\nMap is a function that allows some kind of function upon a sequence. This sequence can be list, tuple, string etc.\nSyntax: map(function, seq)\nlambda functions are commonly used with map functions.\nExample -\n\nx = [1,2,3,4]\ndef square(x):\n    return x*x\nmap(square,x) #--&gt;returns an iterator in python3.\n#To return in the desired sequence, use that as prefix in map. \nlist(map(square,x)) #--&gt; returns as a list.\n\n[1, 4, 9, 16]\n\n\nWith lambda function:\n\na = [1,2,3,4]\nlist(map(lambda x:x*x,a))\n\n[1, 4, 9, 16]\n\n\nWith map more than one sequence can be used -\n\nb = [1, 1, 1, 1]\nlist(map(lambda x,y:x+y, a,b))    \n\n[2, 3, 4, 5]"
  },
  {
    "objectID": "notes/python_concepts.html#filter-function",
    "href": "notes/python_concepts.html#filter-function",
    "title": "Python concepts notes",
    "section": "Filter function",
    "text": "Filter function\nThis function is used to filter the outputs if the sequence satisfies some condition. This can be easily written as a list comprehension or a generator. list(filter(lambda x:x%2==0, range(1,11))"
  },
  {
    "objectID": "notes/python_concepts.html#reduce-function",
    "href": "notes/python_concepts.html#reduce-function",
    "title": "Python concepts notes",
    "section": "Reduce function",
    "text": "Reduce function\nThis was removed from the inbuilt functions in python3 and added to functools. It is similar to map function but unlike map it takes only one iterable. list(reduce(lambda x,y:x+y, a)). Internally assigns x and y and calculates the desired function.\nEach of the above function can be substituted with list comprehension.\n\nnamespace and variable scope\nNamespace is the space occupied by an entity in the program. When two or more programs contain the same name, a method to invoke a unique program is done using its module name.\n\n\nLEGB Rule\nVariable scope has three visibilty levels – builtin, global, enclosed, and local.\nLocal scope - variables defined inside a local function. Their lifecycle ends with the local function. Global scope - variable defined at the top of the module and is available for all the functions underneath it. Enclosed scope - seen in nested function. built-in scope - names within python built-in functionality like print().\nChange a global variable inside a local function? use global keyword. Change a enclosed variable inside an enclosed(nested)-local function? use nonlocal keyword. The order of execution follows local, enclosed, global, and built-in.\n\n\nClosures\nClosure is a concept to invoke an enclosed(nested)-local function outside of its scope. This uses a python’s property – functions are first class object. Example -\n\ndef f():\n    return 2\ng = f\n\ng here gets the function f’s location(reference) or the path of the function till the end of it. This functionality is helpful in accessing an enclosed(nested)-local function beyond its scope.\nexample -\n\ndef f():\n    x = 4\n    def g():\n        y = 3\n        return x+y\n    return g\n\na = f()\nprint(a) #--&gt; returns path till function 'g'\nprint(a()) #--&gt; returns 7\nprint(a.__name__) #--&gt; return function 'g' name.\n\n&lt;function f.&lt;locals&gt;.g at 0x1037b8dc0&gt;\n7\ng\n\n\nWhy closures? * Avoid global values * Data hiding * Implement decorators\n\n\nDecorators\nAny callable object that is used to modify a function or class. Adds additional functionality to existing function or class. Basically a wrapper around the existing function where the existing function code couldn’t be changed but additional features/checks are necessary. It is much easier to use the decorator than writing one. The wrapper becomes complex as the functions it wraps get longer.\nA decorator should: * take a function as parameter * add functionality to the function * function needs to return another function\nTwo types: - Function decorator - Class decorator\n\n\nfunction call as parameter\n\ndef f1():\n    print(\"hello from f1\")\ndef f2(function):\n    print(\"hello from f2\")\n    function()\nf2(f1)\n\nhello from f2\nhello from f1\n\n\n\n\nMultiple decorators\n\ndef f1(func):\n    def inner():\n        return \"first \" + func() +\" first\"\n    return inner\n\ndef f2(func):\n    def wrapper():\n        return \"second \" + func()+\" second\"\n    return wrapper\n\n@f1\n@f2\ndef ordinary():\n    return \"good morning\"\n\nprint(ordinary())\n\nfirst second good morning second first\n\n\n&gt;&gt;&gt;&gt; first second good morning second first\nAt first, the f2 decorator is called and prints second good morning second, then f1 decorator takes that output and prefixes and suffixes with first.\n\n\nDecorators with parameters\nTo pass parameters to a decorator, the nested function in the previous case must be defined inside a function.\n\ndef outer(x):\n    def f1(func):\n        def inner():\n            return func() + x\n        return inner\n    return f1\n\n\n@outer(\" everyone\")\ndef greet():\n    return \"good morning\"\n\nprint(greet)\n\n&lt;function outer.&lt;locals&gt;.f1.&lt;locals&gt;.inner at 0x13f874310&gt;\n\n\n\ndef div1(a,b):\n    return a/b\n\ndef div2(a,b,c):\n    return a/b/c\n\nprint(div1(5,0))\nprint(div2(3,4,5))\n\nZeroDivisionError: division by zero\n\n\nTo protect against division-by-zero error, a decorator is written.\n\ndef div_by_zero_dec(func):\n    def inner(*args):\n        for i in args[1:]:\n            if i == 0:\n               return \"Enter valid non-zero input for denominator\"\n        #gives the general error and not decorator output for div2 function if the input is zero\n        #answer =  [\"Enter valid non-zero input for denominator\" if i == 0 else func(*args) for i in args[1:]]\n        return func(*args)\n        #return answer  \n    return inner\n\n@div_by_zero_dec\ndef div1(a,b):\n    return a/b\n\n@div_by_zero_dec\ndef div2(a,b,c):\n    return a/b/c\n\nprint(div1(5,0))\nprint(div2(3,1,0))\n\nEnter valid non-zero input for denominator\nEnter valid non-zero input for denominator\n\n\n\n\nData hiding\nDecorators inherently hides the original function. To avoid that we can use wraps() method from functools.\n\nfrom functools import wraps\ndef outer(x):\n    def f1(func):\n        @wraps(func)\n        def inner():\n            return func() + x\n        return inner\n    return f1\n\n\n@outer(\" everyone\")\ndef greet():\n    return \"good morning\"\n\nprint(greet.__name__)\n\ngreet\n\n\n\n\nClass decorators\nDecorator function can be applied to class methods also.\n\n#check for name equal to Justin is done with a decorator\ndef check_name(func):\n    def inner(name_ref):\n        if name_ref.name == \"Justin\":\n            return \"There is another Justin\"\n        return func(name_ref)\n    return inner\n\nclass Printing():\n    def __init__(self, name):\n        self.name = name\n    \n    @check_name\n    def print_name(self):\n        print(f\"username is {self.name}\")\n\np = Printing(\"Justin\")\np.print_name()\n#username is Justin\n#There is another Justin\n\n'There is another Justin'\n\n\nTo make a class decorator, we need to know about a special method called __call__. If a __call__ method is defined, the object of a class can be used as function call.\n\nclass Printing():\n    def __init__(self, name):\n        self.name = name\n    \n    def __call__(self):\n        print(f\"username is {self.name}\")\n\np = Printing(\"Lindon\")\np()\n# username is Lindon\n\nusername is Lindon\n\n\n\n\nClass decorator on a function\n\nclass Decorator_greet:\n    def __init__(self, func):\n        self.func = func\n    def __call__(self):\n        return self.func().upper()\n\n@Decorator_greet\ndef greet():\n    return \"good morning\"\n\nprint(greet())\n#GOOD MORNING\n\nGOOD MORNING\n\n\n\n\nBuilt-in decorators\n\n@property\n@classmethod\n@staticmethod\n\n\n\n@property\n\nClass methods as attributes The idea of a wrapper is to make changes to code base without hindering its use for the end user. Using @property decorator, a class variable which is now a class method will give the result if used just like accessing the variable. For example - objectname.function() or objectname.function will give the same result without errors. So the user can access just like they did previously.\n\nBorrowing idea from other programming languages, the private variables are defined with __ prefix. So to access those variables, getter, setter, and deleter methods are necessary. Accessing is done with getter method. So it gets @property decorator. Both setters and deleters get @functionname.setter or @functionname.deleter. (verify this. could be wrong)\n\n@classmethod Insted of self, the classmethod decorator takes cls as first argument for its function. These methods can access and modify class states.\n@staticmethod This is similar to classmethod but takes no predefined argument like instance method(self) or classmethod(cls). These methods can’t access class state. So ideally they are used for checking conditions.\n\nDecorator Template\n\nimport functools\n\ndef my_decorator(func):\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        # Do something before\n        result = func(*args, **kwargs)\n        # Do something after\n        return result\n    return wrapper"
  },
  {
    "objectID": "notes/python_concepts.html#context-managers",
    "href": "notes/python_concepts.html#context-managers",
    "title": "Python concepts notes",
    "section": "Context Managers",
    "text": "Context Managers\nUsually with is used w.r.t to file operations, database connections. In addition to using with and as keywords, we can make custom context managers using @contextlib.contextmanger which is a generator decorator.\n\nfrom contextlib import contextmanager\n\n@contextmanager\ndef opening(filename, method):\n    print(\"Enter\")\n    f = open(filename, method) \n    try:\n        yield f\n    finally:\n        f.close()\n        print(\"Exit\")\n\nwith opening(\"hello.txt\", \"w\") as f:\n    print(\"inside\")\n    f.write(\"hello there\")\n\nEnter\ninside\nExit"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Deepak Ramani",
    "section": "",
    "text": "Hello, I’m Deepak! I am a passionate software engineer with a strong interest in data engineering, DevOps, machine learning and data science in general. I thrive on tackling complex problems and finding innovative solutions through collaborative teamwork. I tend to spend more time on making a workflow simpler and easier to execute. I spend my free time learning new tools and technologies. Currently my focus is on these topics,\n\nEffective data analysis with Pandas. I use this wonderful book,\nDeveloping dashboard with Tableau. I use Rob Caroll’s Tableau for sport Youtube playlists more than other resources.\nLearning to use Pyspark. I use Jigsawlabs free course.\n\nWith a background in software engineering I am driven by a desire to leverage the power of data to drive impactful results. During my master’s thesis, I developed an end-to-end system with a focus on implementing simpler CNN model architectures to achieve comparable performance to the latest models. This experience allowed me to deepen my understanding of data collection, preprocessing, and the fusion techniques necessary for night-time driving and collision avoidance.\nI am an enthusiastic learner who stays up-to-date with the latest industry trends and technologies. My strong problem-solving skills, combined with my passion for teamwork, enable me to contribute to challenging projects and deliver high-quality results. I am now seeking new opportunities to apply my skills and knowledge in data engineering, DevOps, and machine learning, making a meaningful impact within a dynamic organization.\nFeel free to connect with me to explore collaboration opportunities, share insights, or discuss exciting projects in these fields. Let’s embark on this journey together to drive innovation and solve complex problems."
  },
  {
    "objectID": "projects/mlops.html",
    "href": "projects/mlops.html",
    "title": "mlops",
    "section": "",
    "text": "You can read about it in github."
  },
  {
    "objectID": "notes/py_for_finance.html",
    "href": "notes/py_for_finance.html",
    "title": "Notes - LinkedIn course - Financial data analysis",
    "section": "",
    "text": "For financial data analysis, it is important to know certain terminologies, their purpose, and how to calculate them.\nI’m taking Matt Harrison’s Linkedin course on Python for Finance. Here I note down all the Pandas techniques and concepts explored in the course. As always if you find an error, don’t hesitate to contact me."
  },
  {
    "objectID": "notes/py_for_finance.html#chaining",
    "href": "notes/py_for_finance.html#chaining",
    "title": "Notes - LinkedIn course - Financial data analysis",
    "section": "Chaining",
    "text": "Chaining\nMatt introduces one of the features in Pandas called chaining. It allows reading the code as a recipe. One can simply go through from top to bottom and understand how the code works. We leverage pipe() pandas function. We can use it call any function.\nFrom the two stocks, PFE and JNJ, we need only PFE. So, we can try to use chaining principle.\n\nWithout chaining\n\ndef fix_cols(df):\n    cols = df.columns\n    outer = [col[0] for col in cols]\n    df.columns = outer\n    return df\n\npfe_df1 = pharma_df.iloc[:,1::2]\npfe_df1 = fix_cols(pfe_df1)\npfe_df1\n\n\n\n\n\n\n\n\nAdj Close\nClose\nHigh\nLow\nOpen\nVolume\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n2015-01-02\n21.793076\n29.724857\n30.151802\n29.620493\n29.667933\n16371571\n\n\n2015-01-05\n21.674822\n29.563566\n29.800758\n29.421251\n29.743834\n24786391\n\n\n2015-01-06\n21.855684\n29.810247\n30.227703\n29.525618\n29.667933\n29468681\n\n\n2015-01-07\n22.154783\n30.218216\n30.237192\n29.962049\n30.094877\n20248816\n\n\n2015-01-08\n22.606922\n30.834915\n30.967743\n30.569260\n30.683111\n49169522\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n2023-04-21\n39.779552\n40.209999\n40.299999\n39.910000\n40.090000\n19227100\n\n\n2023-04-24\n39.482765\n39.910000\n40.200001\n39.709999\n40.189999\n17633700\n\n\n2023-04-25\n38.908978\n39.330002\n39.919998\n39.279999\n39.750000\n24492400\n\n\n2023-04-26\n38.216469\n38.630001\n39.189999\n38.400002\n39.160000\n22401400\n\n\n2023-04-27\n38.325291\n38.740002\n38.830002\n38.310001\n38.619999\n22434000\n\n\n\n\n2094 rows × 6 columns\n\n\n\n\n\nWith chaining\n\npfe_df = (pharma_df\n1          .iloc[:,1::2]\n2          .pipe(fix_cols)\n         )\npfe_df\n\n\n1\n\nRetrieves only PFE stock data\n\n2\n\nRemoves the ticker and just shows\n\n\n\n\n\n\n\n\n\n\n\nAdj Close\nClose\nHigh\nLow\nOpen\nVolume\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n2015-01-02\n21.793076\n29.724857\n30.151802\n29.620493\n29.667933\n16371571\n\n\n2015-01-05\n21.674822\n29.563566\n29.800758\n29.421251\n29.743834\n24786391\n\n\n2015-01-06\n21.855684\n29.810247\n30.227703\n29.525618\n29.667933\n29468681\n\n\n2015-01-07\n22.154783\n30.218216\n30.237192\n29.962049\n30.094877\n20248816\n\n\n2015-01-08\n22.606922\n30.834915\n30.967743\n30.569260\n30.683111\n49169522\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n2023-04-21\n39.779552\n40.209999\n40.299999\n39.910000\n40.090000\n19227100\n\n\n2023-04-24\n39.482765\n39.910000\n40.200001\n39.709999\n40.189999\n17633700\n\n\n2023-04-25\n38.908978\n39.330002\n39.919998\n39.279999\n39.750000\n24492400\n\n\n2023-04-26\n38.216469\n38.630001\n39.189999\n38.400002\n39.160000\n22401400\n\n\n2023-04-27\n38.325291\n38.740002\n38.830002\n38.310001\n38.619999\n22434000\n\n\n\n\n2094 rows × 6 columns\n\n\n\nAs you can see this makes an easier reading. We use the pipe() to call our fix_cols function. The resulting dataframe has only the outer level column names. Indeed, I agree that as more analysis are added, it gets complicated and harder to understand. Indeed, the intermediate calculation steps are not shown in the final version which makes it difficult to visualise the operation instantaneously."
  },
  {
    "objectID": "notes/py_for_finance.html#returns",
    "href": "notes/py_for_finance.html#returns",
    "title": "Notes - LinkedIn course - Financial data analysis",
    "section": "Returns",
    "text": "Returns\nHow much percentage of return can be expected? With pandas, we can simply use .pct_change() function and get the values. Plotting them is as simple as shown previously.\n\n(pfe_df\n .pct_change()\n .Close\n .plot()\n)\n\n\nHistogram can be an option but it doesn’t show negative swing. Somewhat appropriate would be to use bar plot.\n\n(pfe_df\n .pct_change()\n .Close\n .iloc[-100:] #last 100 rows\n .plot.bar()\n)\n\n\nThis plot shows the negative trends but the X-axis is illegible. We don’t know on which date the closing stock prices changed. This is because Pandas converts/groups whatever on the x-axis into categorical variables. For example, for categorical variable such as elephants, dogs and cats this works but for dates that isn’t correct.\nWhat if we explicitly say the x-axis as dates.\n\nfig, ax = plt.subplots(figsize=(10,4))\n(pfe_df\n .pct_change()\n .Close\n .iloc[-100:]\n .plot.bar(ax=ax)\n)\nax.xaxis.set_major_locator(dates.MonthLocator())\nax.xaxis.set_major_formatter(dates.DateFormatter('%b-%y'))\nax.xaxis.set_minor_locator(dates.DayLocator())\n\n\n1970?!? Still Pandas converts dates to categorical variables.\nThe solution Matt suggests is to use matplotlib.\n\ndef my_bar(series, ax):\n    ax.bar(series.index, series)\n    ax.xaxis.set_major_locator(dates.MonthLocator())\n    ax.xaxis.set_major_formatter(dates.DateFormatter('%b-%y'))\n    ax.xaxis.set_minor_locator(dates.DayLocator())\n    return series\n\nfig, ax = plt.subplots(figsize=(10,4))\n_ = ( pfe_df\n .pct_change()\n .Close\n .iloc[-100:]\n .pipe(my_bar, ax)\n)\n\n\nLooks good now."
  },
  {
    "objectID": "notes/py_for_finance.html#cumulative-returns",
    "href": "notes/py_for_finance.html#cumulative-returns",
    "title": "Notes - LinkedIn course - Financial data analysis",
    "section": "Cumulative returns",
    "text": "Cumulative returns\nCumulative returns shows the investment amount gained or lost over time. The formula is given by \\[\ncumulative\\_return = \\frac{(current\\_price - original\\_price)}{(curent\\_price)}\n\\]\n\n(pfe_df\n .Close\n .sub(pfe_df.Close[0])\n .div(pfe_df.Close[0])\n .plot()\n)\n\n&lt;AxesSubplot:xlabel='Date'&gt;\n\n\n\n\n\n\n\n\n\nAnother alternate way is to numpy’s cumprod function.\n\n(pfe_df\n .Close\n .add(1)\n .cumprod()\n .sub(1)\n .plot()\n)\n\n/Users/dross/miniforge3/envs/mlops/lib/python3.9/site-packages/numpy/core/fromnumeric.py:57: RuntimeWarning:\n\noverflow encountered in accumulate\n\n\n\n&lt;AxesSubplot:xlabel='Date'&gt;\n\n\n\n\n\n\n\n\n\nAs you can see both plots give the same result.\nIf we’re to use .pipe here, we can do like so:\n\ndef calc_cumrets(df, col):\n     ser = df[col]\n     return (ser\n             .sub(ser[0])\n             .div(ser[0])\n            )\n(pfe_df\n .pipe(calc_cumrets,'Close')\n .plot()\n)\n\n&lt;AxesSubplot:xlabel='Date'&gt;\n\n\n\n\n\n\n\n\n\n\nLambda functions or anonymous functions\nUsing lambda functions we can make impropmtu functions and use it with our chaining.\nWe would traditionally call a function like so:\n\ndef get_returns(df):\n    return calc_cumrets(df, 'Close')\n\nget_returns(pfe_df)\n\nDate\n2015-01-02    0.000000\n2015-01-05   -0.005426\n2015-01-06    0.002873\n2015-01-07    0.016598\n2015-01-08    0.037344\n                ...   \n2023-04-21    0.352740\n2023-04-24    0.342647\n2023-04-25    0.323135\n2023-04-26    0.299586\n2023-04-27    0.303286\nName: Close, Length: 2094, dtype: float64\n\n\nHowever, if we are to use lambda, then the above code can be written as:\n\n(lambda df: get_returns(df))(pfe_df)\n\nDate\n2015-01-02    0.000000\n2015-01-05   -0.005426\n2015-01-06    0.002873\n2015-01-07    0.016598\n2015-01-08    0.037344\n                ...   \n2023-04-21    0.352740\n2023-04-24    0.342647\n2023-04-25    0.323135\n2023-04-26    0.299586\n2023-04-27    0.303286\nName: Close, Length: 2094, dtype: float64\n\n\nNow, with cumulative returns calculation, it would be useful if those values can be assigned to a new column in the dataframe. It is here that a Pandas feature in .assign function is helpful. It helps create new columns. We can couple .assign and lambda together.\n\npfe_df = (pfe_df\n .assign(cum_rets=lambda df:calc_cumrets(df, 'Close'))\n)"
  },
  {
    "objectID": "notes/py_for_finance.html#volatility",
    "href": "notes/py_for_finance.html#volatility",
    "title": "Notes - LinkedIn course - Financial data analysis",
    "section": "Volatility",
    "text": "Volatility\nVolatility is a statistical measure of the dispertion of the returns for a given market index in this case stocks. In most cases, higher the volatility, the riskier the stock. It is often measured from either standard deviation or variance between returns from that stock. Remember standard deviation is the measure of deviation of the data relative to its mean.\nJust like mean(), we can calculate std().\n\n(pfe_df\n .Close\n #.mean()\n .std()\n)\n\n6.69077286484766\n\n\nThe .assign() allows consective chaining methods to use these newly created columns. In the below code block, we can use the pct_change_close created in the first line in to the second line. Then, we can calculate 30 day rolling volatility. Rolling is nothing but a time frame in which the volatility is calculated. We can see that for the first 15 days the volatility is NaN(not a number) and on the 30th day, there is an entry.\n\n(pfe_df\n .assign(pct_change_close=pfe_df.Close.pct_change())\n .pct_change_close\n .rolling(30)\n .std()\n)\n\nDate\n2015-01-02         NaN\n2015-01-05         NaN\n2015-01-06         NaN\n2015-01-07         NaN\n2015-01-08         NaN\n                ...   \n2023-04-21    0.008945\n2023-04-24    0.009058\n2023-04-25    0.009192\n2023-04-26    0.009718\n2023-04-27    0.009543\nName: pct_change_close, Length: 2094, dtype: float64\n\n\n\n#rolling volatility\n(pfe_df\n .assign(close_volatility=pfe_df.rolling(30).Close.std(),\n         percent_volatility=pfe_df.Close.pct_change().rolling(30).std())\n .iloc[:,-2:] # fetch only the last two columns\n .plot(subplots=True)\n\n)\n\narray([&lt;AxesSubplot:xlabel='Date'&gt;, &lt;AxesSubplot:xlabel='Date'&gt;],\n      dtype=object)\n\n\n\n\n\n\n\n\n\nWe can also use .resample to calculate 15 day volatility as we have date as index.\n\n# 15 day volatility\n(pfe_df\n .assign(pct_change_close=pfe_df.Close.pct_change())\n .resample('15D')\n .std()\n)\n\n\n\n\n\n\n\n\nAdj Close\nClose\nHigh\nLow\nOpen\nVolume\ncum_rets\npct_change_close\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n2015-01-02\n0.423586\n0.577756\n0.554235\n0.536322\n0.565220\n8.716662e+06\n0.019437\n0.009335\n\n\n2015-01-17\n0.408934\n0.557768\n0.552896\n0.473576\n0.428281\n6.992238e+06\n0.018764\n0.011556\n\n\n2015-02-01\n0.864667\n1.092056\n1.124398\n1.137237\n1.027460\n1.347935e+07\n0.036739\n0.013012\n\n\n2015-02-16\n0.103079\n0.139368\n0.153115\n0.158683\n0.142704\n5.436585e+06\n0.004689\n0.006524\n\n\n2015-03-03\n0.232108\n0.313819\n0.310265\n0.285296\n0.329080\n6.907642e+06\n0.010557\n0.008226\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2023-02-19\n0.912909\n0.922787\n0.942723\n1.064260\n1.025597\n4.710137e+06\n0.031044\n0.011505\n\n\n2023-03-06\n0.510382\n0.515904\n0.386680\n0.472090\n0.610293\n1.148802e+07\n0.017356\n0.011168\n\n\n2023-03-21\n0.414519\n0.419005\n0.428108\n0.305953\n0.396622\n3.416600e+06\n0.014096\n0.009084\n\n\n2023-04-05\n0.495242\n0.500601\n0.483455\n0.430794\n0.424291\n4.807599e+06\n0.016841\n0.008860\n\n\n2023-04-20\n0.647205\n0.654208\n0.613951\n0.713320\n0.629458\n2.507162e+06\n0.022009\n0.010312\n\n\n\n\n203 rows × 8 columns\n\n\n\n\n# 15 day rolling volatility\n(pfe_df\n .assign(pct_change_close=pfe_df.Close.pct_change())\n .rolling(window=15, min_periods=15)\n .std()\n)\n\n\n\n\n\n\n\n\nAdj Close\nClose\nHigh\nLow\nOpen\nVolume\ncum_rets\npct_change_close\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n2015-01-02\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2015-01-05\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2015-01-06\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2015-01-07\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2015-01-08\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2023-04-21\n0.591463\n0.597864\n0.638385\n0.562327\n0.596404\n3.967966e+06\n0.020113\n0.009691\n\n\n2023-04-24\n0.657297\n0.664410\n0.695511\n0.616351\n0.626652\n3.915626e+06\n0.022352\n0.009383\n\n\n2023-04-25\n0.771667\n0.780017\n0.784388\n0.725364\n0.709656\n4.093416e+06\n0.026241\n0.008983\n\n\n2023-04-26\n0.957201\n0.967559\n0.939399\n0.930587\n0.843585\n4.133347e+06\n0.032551\n0.009559\n\n\n2023-04-27\n1.047608\n1.058944\n1.047459\n1.068349\n1.011878\n3.287318e+06\n0.035625\n0.008130\n\n\n\n\n2094 rows × 8 columns\n\n\n\nWhat happens if the assinged new column name is same as the pandas function name and we have to use further for our analysis? We can include that ‘assigned’ column within [](square) braces and use it. In the below example, we can see how pct_change conflicts with pandas and is therefore must be put inside [] to access it.\n\n# 15 day rolling volatility\n(pfe_df\n .assign(pct_change=pfe_df.Close.pct_change())\n .rolling(window=15, min_periods=15)\n .std()\n #.pct_change\n [\"pct_change\"]\n .plot()\n)\n\n&lt;AxesSubplot:xlabel='Date'&gt;"
  },
  {
    "objectID": "notes/py_for_finance.html#moving-averages-or-rolling-windows",
    "href": "notes/py_for_finance.html#moving-averages-or-rolling-windows",
    "title": "Notes - LinkedIn course - Financial data analysis",
    "section": "Moving averages or rolling windows",
    "text": "Moving averages or rolling windows\nMoving average(MA) of a stock is calculated to help smooth out the price data by creating a constantly updated average price. It helps to mitigate the impacts of random, short-term fluctuations on the prices of the stock over a time period. There are two types of moving averages - simple which is just the arithmetic mean of the given prices over a specified number of days and exponential which is the weighted average that gives significance to the recent prices than old ones, making it an indicator that is more responsive to new infotmation.\nMA is used to identify the tread direction of a stock or to determine its support and resistance level as it depends on the past prices. The longer the period for the MA, the greater the lag. A 200-day MA has much greater lag than 20-day MA. The gold standard used by investers are 50-day and 200-day MAs.\nShorter MA for short-term investment and longer MA for long-term. A rising MA means upward trend and declining means downward trend.\n\nWhat is a Golden Cross?\nA golden cross is a chart pattern in which a short-term moving average crosses above a long-term moving average. The golden cross is a bullish breakout pattern formed from a crossover involving a security’s short-term moving average such as the 15-day moving average, breaking above its long-term moving average, such as the 50-day moving average. As long-term indicators carry more weight, the golden cross indicates a bull market on the horizon and is reinforced by high trading volumes.\n\n\nLag\nThese lags can be calculated in Pandas using shift function. shift(1) means shift index one place down. shift(2) means two places down. For example,\n\n(pfe_df\n .assign(s1=pfe_df.Close.shift(1),\n         s2=pfe_df.Close.shift(2))\n [[\"s1\",\"s2\"]]\n)\n\n\n\n\n\n\n\n\ns1\ns2\n\n\nDate\n\n\n\n\n\n\n2015-01-02\nNaN\nNaN\n\n\n2015-01-05\n29.724857\nNaN\n\n\n2015-01-06\n29.563566\n29.724857\n\n\n2015-01-07\n29.810247\n29.563566\n\n\n2015-01-08\n30.218216\n29.810247\n\n\n...\n...\n...\n\n\n2023-04-21\n39.849998\n40.240002\n\n\n2023-04-24\n40.209999\n39.849998\n\n\n2023-04-25\n39.910000\n40.209999\n\n\n2023-04-26\n39.330002\n39.910000\n\n\n2023-04-27\n38.630001\n39.330002\n\n\n\n\n2094 rows × 2 columns\n\n\n\nthe Close value in the first row will be on the second row for shift(1) and two rows down for shift(2).\nNow for simple 3-day moving average, we need to average Close, s1, and s2. We can do it manually using a lambda and use the rolling pandas with window=3 specified.\n\n(pfe_df\n .assign(s1=pfe_df.Close.shift(1),\n         s2=pfe_df.Close.shift(2),\n         ma3=lambda df_:df_.loc[:,[\"Close\", \"s1\", \"s2\"]].mean(axis='columns'),\n         ma3_builtin=pfe_df.Close.rolling(3).mean()\n        )\n[[\"s1\",\"s2\",\"ma3\",\"ma3_builtin\"]]\n)\n\n\n\n\n\n\n\n\ns1\ns2\nma3\nma3_builtin\n\n\nDate\n\n\n\n\n\n\n\n\n2015-01-02\nNaN\nNaN\n29.724857\nNaN\n\n\n2015-01-05\n29.724857\nNaN\n29.644212\nNaN\n\n\n2015-01-06\n29.563566\n29.724857\n29.699557\n29.699557\n\n\n2015-01-07\n29.810247\n29.563566\n29.864010\n29.864010\n\n\n2015-01-08\n30.218216\n29.810247\n30.287793\n30.287793\n\n\n...\n...\n...\n...\n...\n\n\n2023-04-21\n39.849998\n40.240002\n40.100000\n40.100000\n\n\n2023-04-24\n40.209999\n39.849998\n39.989999\n39.989999\n\n\n2023-04-25\n39.910000\n40.209999\n39.816667\n39.816667\n\n\n2023-04-26\n39.330002\n39.910000\n39.290001\n39.290001\n\n\n2023-04-27\n38.630001\n39.330002\n38.900002\n38.900002\n\n\n\n\n2094 rows × 4 columns"
  },
  {
    "objectID": "notes/py_for_finance.html#plotting-mas",
    "href": "notes/py_for_finance.html#plotting-mas",
    "title": "Notes - LinkedIn course - Financial data analysis",
    "section": "Plotting MAs",
    "text": "Plotting MAs\nWe are getting comfortable with plotting. We select the columns needed to plotted - [‘Close’, ‘ma3_builtin’] for last 200 rows.\n\n(pfe_df\n .assign(s1=pfe_df.Close.shift(1),\n         s2=pfe_df.Close.shift(2),\n         ma3=lambda df_:df_.loc[:,[\"Close\", \"s1\", \"s2\"]].mean(axis='columns'),\n         ma3_builtin=pfe_df.Close.rolling(3).mean()\n        )\n [['Close', 'ma3_builtin']]\n .iloc[-200:]\n .plot()\n)\n\n&lt;AxesSubplot:xlabel='Date'&gt;\n\n\n\n\n\n\n\n\n\nAs we can see the MA smoothes out the little peaks and troughs.\n\nGolden Cross\nSome experts say if there is a crossover between MA-50 and MA-200, it is an indicator to buy or sell.\n\n(pfe_df\n .assign(ma50=pfe_df.Close.rolling(50).mean(),\n         ma200=pfe_df.Close.rolling(200).mean()\n        )\n [[\"Close\",\"ma50\",\"ma200\"]]\n .iloc[-650:]\n .plot()\n)\n\n&lt;AxesSubplot:xlabel='Date'&gt;"
  },
  {
    "objectID": "notes/py_for_finance.html#obv--on-balance-volume",
    "href": "notes/py_for_finance.html#obv--on-balance-volume",
    "title": "Notes - LinkedIn course - Financial data analysis",
    "section": "OBV- On-balance Volume",
    "text": "OBV- On-balance Volume\nOBV is one such used for technical analysis. It is a momentum indicator that uses volume to predict changes in stock price.\n\nWhat Does On-Balance Volume Tell You?\nThe actual value of the OBV is unimportant; concentrate on its direction. (source: fidelity)\n\nWhen both price and OBV are making higher peaks and higher troughs, the upward trend is likely to continue.\nWhen both price and OBV are making lower peaks and lower troughs, the downward trend is likely to continue.\nDuring a trading range, if the OBV is rising, accumulation may be taking place—a warning of an upward breakout.\nDuring a trading range, if the OBV is falling, distribution may be taking place—a warning of a downward breakout.\nWhen price continues to make higher peaks and OBV fails to make higher peaks, the upward trend is likely to stall or fail. This is called a negative divergence.\nWhen price continues to make lower troughs and OBV fails to make lower troughs, the downward trend is likely to stall or fail. This is called a positive divergence.\n\n\n\nOBV calculation\nIf today’s close is greater than yesterday’s close then: OBV = Yesterday’s OBV + Today’s Volume\nIf today’s close is less than yesterday’s close then: OBV = Yesterday’s OBV – Today’s Volume\nIf today’s close is equal to yesterday’s close then: OBV = Yesterday’s OBV\n\\[\nOBV = OBV_{prev} + \\begin{cases}\nvolume, \\text{ if close &gt; close}_{prev} \\\\\n0, \\text{ if close = close}_{prev}\\\\\n-volume, \\text{ if close &lt; close}_{prev}\n\\end{cases}\n\\]\nwhere\nOBV = current on-balance volume level\nOBVprev = previous on-balance volume level\nvolume = Latest trading volume amount\n\n\nPython - Naive approach\n\ndef calc_obv(df): \n    df = df.copy() \n    df[\"OBV\"] = 0.0 \n    for i in range(1, len(df)): \n        if df[\"Close\"][i] &gt; df[\"Close\"][i - 1]: \n            df[\"OBV\"][i] = df[\"OBV\"][i-1] + df[\"Volume\"][i] \n        elif df[\"Close\"][i] &lt; df[\"Close\"][i - 1]: \n            df[\"OBV\"][i] = df[\"OBV\"][i-1] - df[\"Volume\"][i] \n        else: \n            df[\"OBV\"][i] = df[\"OBV\"][i-1] \n    return df\n\ncalc_obv(pfe_df)\n\nIt takes a while to complete. That is too long. Another approach would be to use numpy’s where condition. Inserting the conditions needs some thinking and isn’t obvious as seeing the mathematical formula. So we avoid np.where even though it is faster than naive python approach.\n\n\nPreferred choice to calculate OBV - np.select\nThe syntax of np.select is easy to understand - we specify the conditions list, their choices and a default value if the conditions aren’t satisfied.\n\nnp.select(condlist=[pfe_df.Close &lt; 7.5, pfe_df.Close &gt; 70],\n          choicelist=[7.55, 72], default= 34)\n\narray([34., 34., 34., ..., 34., 34., 34.])\n\n\nSo our code will be like so -\n\n(pfe_df\n .assign(vol=np.select(condlist=[pfe_df.Close &gt; pfe_df.Close.shift(1),\n                                 pfe_df.Close == pfe_df.Close.shift(1),\n                                 pfe_df.Close &lt; pfe_df.Close.shift(1)],\n                       choicelist=[pfe_df.Volume, 0, -pfe_df.Volume]),\n        obv=lambda df_:df_.vol.cumsum()\n        )\n)\n\n\n\n\n\n\n\n\nAdj Close\nClose\nHigh\nLow\nOpen\nVolume\ncum_rets\nvol\nobv\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n\n2015-01-02\n21.793076\n29.724857\n30.151802\n29.620493\n29.667933\n16371571\n0.000000\n0\n0\n\n\n2015-01-05\n21.674822\n29.563566\n29.800758\n29.421251\n29.743834\n24786391\n-0.005426\n-24786391\n-24786391\n\n\n2015-01-06\n21.855684\n29.810247\n30.227703\n29.525618\n29.667933\n29468681\n0.002873\n29468681\n4682290\n\n\n2015-01-07\n22.154783\n30.218216\n30.237192\n29.962049\n30.094877\n20248816\n0.016598\n20248816\n24931106\n\n\n2015-01-08\n22.606922\n30.834915\n30.967743\n30.569260\n30.683111\n49169522\n0.037344\n49169522\n74100628\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2023-04-21\n39.779552\n40.209999\n40.299999\n39.910000\n40.090000\n19227100\n0.352740\n19227100\n-538012626\n\n\n2023-04-24\n39.482765\n39.910000\n40.200001\n39.709999\n40.189999\n17633700\n0.342647\n-17633700\n-555646326\n\n\n2023-04-25\n38.908978\n39.330002\n39.919998\n39.279999\n39.750000\n24492400\n0.323135\n-24492400\n-580138726\n\n\n2023-04-26\n38.216469\n38.630001\n39.189999\n38.400002\n39.160000\n22401400\n0.299586\n-22401400\n-602540126\n\n\n2023-04-27\n38.325291\n38.740002\n38.830002\n38.310001\n38.619999\n22434000\n0.303286\n22434000\n-580106126\n\n\n\n\n2094 rows × 9 columns\n\n\n\nPutting this as function,\n\ndef calc_obv(df, close_col=\"Close\", vol_col=\"Volume\"):\n    close = df[close_col]\n    vol = df[vol_col]\n    close_shift = close.shift(1)\n    return (df\n             .assign(vol=np.select(condlist=[close &gt; close.shift(1),\n                                   close == close.shift(1),\n                                   close &lt; close.shift(1)],\n                                  choicelist=[vol, 0, -vol]),\n                     obv=lambda df_:df_.vol.fillna(0).cumsum()\n                    )\n             [\"obv\"]\n    )\n\n(pfe_df\n    .assign(obv=calc_obv)\n    .obv\n    .plot()\n)\n\n&lt;AxesSubplot:xlabel='Date'&gt;"
  },
  {
    "objectID": "notes/py_for_finance.html#accumulation-distribution-indicator",
    "href": "notes/py_for_finance.html#accumulation-distribution-indicator",
    "title": "Notes - LinkedIn course - Financial data analysis",
    "section": "Accumulation distribution indicator",
    "text": "Accumulation distribution indicator\n\nWhat is A/D and why is it needed?\nA/D is an evolution of OBV to find the relationship between the price and volume flow of the stock. OBV, from its formula totally relies on the closing value and the volume is either added or substracted giving total bias either to the buyers or sellers. This is not realistic. So A/D brings proporation to the context. In other words it brings a multipler that ranges from -1 to +1. Realistically the value is between this range and therefore providing realistic volume flow.\nFormula for A/D is given by \\[\nA\\\\/D = A\\\\/D_{prev} + (\\text{money flow  multiplier} * \\text{current volume})\n\\] where Money flow multiplier and Current Volume is given by\n\\(MFM = \\frac{(Close - Low) - (High - Close)}{(High - Low)}\\) \\(CV = \\text{MFM} * \\text{period volume}\\)\n\n\nWhat does A/D indicate?\nIt is a cumulative indicator that uses volume and price to assess whether a stock is accumulating or distributing.\n\n\nCalculation\n\n(pfe_df\n .assign(mfm=((pfe_df.Close-pfe_df.Low) - (pfe_df.High-pfe_df.Close)) / (pfe_df.High-pfe_df.Low),\n         mfv=lambda df_:df_.mfm * df_.Volume,\n         cmfv = lambda df_:df_.mfv.cumsum()\n        )\n)\n\n\n\n\n\n\n\n\nAdj Close\nClose\nHigh\nLow\nOpen\nVolume\ncum_rets\nmfm\nmfv\ncmfv\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2015-01-02\n21.793076\n29.724857\n30.151802\n29.620493\n29.667933\n16371571\n0.000000\n-0.607142\n-9.939876e+06\n-9.939876e+06\n\n\n2015-01-05\n21.674822\n29.563566\n29.800758\n29.421251\n29.743834\n24786391\n-0.005426\n-0.250001\n-6.196629e+06\n-1.613650e+07\n\n\n2015-01-06\n21.855684\n29.810247\n30.227703\n29.525618\n29.667933\n29468681\n0.002873\n-0.189188\n-5.575108e+06\n-2.171161e+07\n\n\n2015-01-07\n22.154783\n30.218216\n30.237192\n29.962049\n30.094877\n20248816\n0.016598\n0.862063\n1.745575e+07\n-4.255863e+06\n\n\n2015-01-08\n22.606922\n30.834915\n30.967743\n30.569260\n30.683111\n49169522\n0.037344\n0.333333\n1.638984e+07\n1.213398e+07\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2023-04-21\n39.779552\n40.209999\n40.299999\n39.910000\n40.090000\n19227100\n0.352740\n0.538460\n1.035302e+07\n6.969870e+08\n\n\n2023-04-24\n39.482765\n39.910000\n40.200001\n39.709999\n40.189999\n17633700\n0.342647\n-0.183673\n-3.238837e+06\n6.937482e+08\n\n\n2023-04-25\n38.908978\n39.330002\n39.919998\n39.279999\n39.750000\n24492400\n0.323135\n-0.843740\n-2.066523e+07\n6.730830e+08\n\n\n2023-04-26\n38.216469\n38.630001\n39.189999\n38.400002\n39.160000\n22401400\n0.299586\n-0.417721\n-9.357525e+06\n6.637254e+08\n\n\n2023-04-27\n38.325291\n38.740002\n38.830002\n38.310001\n38.619999\n22434000\n0.303286\n0.653846\n1.466838e+07\n6.783938e+08\n\n\n\n\n2094 rows × 10 columns\n\n\n\nMaking it as a function,\n\ndef calc_ad(df, close_col=\"Close\",low_col=\"Low\",high_col=\"High\",vol_col=\"Volume\"):\n    close = df[close_col]\n    low = df[low_col]\n    high = df[high_col]\n    vol = df[vol_col]\n    return (df\n             .assign(mfm=((close-low) - (high-close))/ (high-low),\n                     mfv=lambda df_:df_.mfm * vol,\n                     cmfv=lambda df_:df_.mfv.cumsum()       \n                    )  \n             .cmfv\n           )\n(pfe_df\n    .assign(ad=calc_ad)\n    .ad\n    .plot()\n)\n\n&lt;AxesSubplot:xlabel='Date'&gt;\n\n\n\n\n\n\n\n\n\n\n\nComparing OBV and AD\n\n(pfe_df\n    .assign(obv=calc_obv,\n            ad=calc_ad\n           )\n    [[\"obv\",\"ad\"]]\n    .iloc[-650:]\n    .plot()\n\n)\n\n&lt;AxesSubplot:xlabel='Date'&gt;"
  },
  {
    "objectID": "notes/py_for_finance.html#rsi---relative-strength-index",
    "href": "notes/py_for_finance.html#rsi---relative-strength-index",
    "title": "Notes - LinkedIn course - Financial data analysis",
    "section": "RSI - Relative strength index",
    "text": "RSI - Relative strength index"
  },
  {
    "objectID": "notes/data-analysis/pandas/index.html",
    "href": "notes/data-analysis/pandas/index.html",
    "title": "Notes on Pandas",
    "section": "",
    "text": "Pandas is a powerful and versatile tool for data analysis and manipulation. I use it almost every day to analyse something.\nA dataframe(DF) is a table with rows and columns. It has several properties attached to it. Importantly it allows data analysis and manipulation.\n\n1pd.set_option('max_columns', 200)\n\n2df = pd.read_csv('filename')\ndf = pd.read_parquet('filename')\n\n3df.shape\n\n4df.info()\n\n5df.describe()\n\n6df.head()/tail()\n\n1\n\nIncreases the max columns displayed to 200. Default is 20.\n\n2\n\nReads an appropriate file format to a DF.\n\n3\n\nReturns a tuple with the dimensions of a DF.\n\n4\n\nReturns concise summary of a DF.\n\n5\n\nGenerates descriptive statistics of a DF.\n\n6\n\nReturns the first(head) or last(tail) 5 rows of a DF.\n\n\n\n\n1df.dtypes\n\n2df.select_dtypes('#select from prev command#')\n\n3df.select_dtypes('object').columns.tolist()\n\n4f\"categorical columns: {df.select_dtypes('object').columns.tolist()}\"\n\n5df.select_dtypes('object').head()\n\n6df.select_dtypes('int64').head().style.background_gradient(cmap='YlOrRd')\n\n1\n\nProvides data types of each column in a DF.\n\n2\n\nAllows to select a particular data type. int64 picks all the int columns.\n\n3\n\nLists all columns that have object dtype.\n\n4\n\nA way to display categorical columns.\n\n5\n\nOutputs first 10 rows of object columns.\n\n6\n\nColour gradients the int datatype columns.\n\n\n\n\n\n\n1df.columns\n\n2df.drop(['#columntodrop'],axis=1, inplace=True/False)\n\n3df = df[['','']].copy()\n\n#It is generally advised to pick the columns needed than drop from the dataframe. How to list all columns? `df.columns`. Then make a variable and assign the columns needed.\ncols = ['col1', 'col2', 'col3']\ndf = df[cols] \n\n4df.isna()\ndf.isnull()\n\n5df.isna().sum()\n\n6df.nunique()\n\n7df['column_name'].unique()\n\n8df['column_name'].value_counts()\n\n9df.corr()\n\n1\n\nList all the column names.\n\n2\n\nMention the columns to drop/remove from DF.\n\n3\n\nTells pandas this is a new DF and not a reference to the prev DF.\n\n4\n\nDisplays true or false for check if there is a null in values.\n\n5\n\nGives the sum of null values in each columns.\n\n6\n\nReturns the number of unique values in each column.\n\n7\n\nReturns an array of unique values in a specific column.\n\n8\n\nReturns a Series containing counts of unique values in a specific column.\n\n9\n\nCalculates the correlation between numeric columns.\n\n\n\n\n\n1df['cleandt'] = pd.to_datetime(df['ColumnName'])\n\n2df.rename(columns={'':'','':''})\n\n3df.columns = [col.replace(' ', '') for col in df.columns]\n\n1\n\nChange a column to datetime format.\n\n2\n\nRename column names.\n\n3\n\nRemove white spaces in columns names.\n\n\n\n\n\n1df.duplicated()\n\n2df.loc[df.duplicated()]\n\n3df.loc[df.duplicated(subset=['ColumnName'])]\ndf.loc[df.duplicated(df.query('column_name==\"\"'))]\n\n4df_new = df.loc[~df.duplicated(subset=['ColumnName','ColumnName','ColumnName'])].reset_index(drop=True)\n\n1\n\nTrue or False for duplicated rows.\n\n2\n\nSelect rows that are duplicated.\n\n3\n\nFinds duplicate rows based on the ‘ColumnName’ column. The result will be a DF containing the duplicate rows.\n\n4\n\nInverse of duplicated values. This way only non-duplicated rows can be selected and assigned to new DF. We reset_index to reset the index.\n\n\n\n\n\n1df.groupby('column_name').agg({'column_to_aggregate': 'aggregation_function'})\n\n2df['column_name'].sum() .mean() .median() .min() .max() .std() .var() .count()\n\n3df['column_name'].quantile(q) .cumsum() .cumprod()\n\n4df.agg({'column_name_1': 'mean', 'column_name_2': 'sum'})\n\n5df.resample('D').sum()\n\n1\n\nGroups data by a specific column and applies an aggregation function to another column.\n\n2\n\nAggregate functions that return a single value.\n\n3\n\nCalculates quantile(0.25), cumulative sum and product.\n\n4\n\nCalculates a separate aggregate for each column.\n\n5\n\nIf the date is the index, then the DF can be resampled by specified time frequency and aggregated.\n\n\n\n\n\nPivot table is allows to reorganize and aggregate data based on one or more columns. It provides a way to create a multidimensional view of your data. It is a powerful reporting tool.\nimport pandas as pd\n\n# Create a sample DataFrame\ndata = {'category': ['A', 'B', 'A', 'B', 'A', 'B'],\n        'values': [10, 20, 30, 40, 50, 60]}\n\ndf = pd.DataFrame(data)\n\n# Create a pivot table\npivot_table = df.pivot_table(values='values', index='category', aggfunc='sum')\n\nprint(pivot_table)\n          \n          values\ncategory       \nA            90\nB           120\n\nvalues: The column to aggregate (in this case, ‘values’).\nindex: The column to use as the index of the pivot table (in this case, ‘category’).\naggfunc: The aggregation function to use when summarizing the data (in this case, we use ‘sum’ to add up the values).\n\n\n\n\n\n1df['Rolling Mean'] = df['Value'].rolling(window=3).mean()\n\n2df['Expanding Sum'] = df['Value'].expanding().sum()\n\n3df['Shifted Value'] = df['Value'].shift(1)\n\n4df['Shifted Value'] = df['Value'].shift(-1)\n\n5df['Rank'] = df['Value'].rank()\n\n6df['Percentage Change'] = df['Value'].pct_change()\n\n7weekly_rolling_mean = df['data'].resample('W').mean().rolling(window=3).mean()\n\n1\n\nComputes rolling statistics, such as rolling mean, sum, etc., over a specified window of rows.\n\n2\n\nComputes expanding statistics, which includes all preceding rows.\n\n3\n\nShifts the values of a column by a specified number of periods. Here it shifts down the current row by one row. SQL equivalent of LAG.\n\n4\n\nShifts up the current row by one row. SQL equivalent of LEAD.\n\n5\n\nAssigns a rank to each value in a column based on a specified ordering.\n\n6\n\nComputes the percentage change between the current and a prior element.\n\n7\n\nResample to weekly data and calculate the rolling mean over a window of 3 weeks.\n\n\n\n\n\n\n\nThe lead function provides information about future values. For example, if you have a time series dataset and you apply a lead function with a lead of 1, it will give you the value of the next time period.\nLeads are used to make predictions or forecasts based on historical data. For instance, if you’re trying to predict sales for the next month, you might use a lead function to access the current month’s data.\n\n\n\nThe lag function provides information about past values. For example, if you have a time series dataset and you apply a lag function with a lag of 1, it will give you the value of the previous time period.\nLags are used for various purposes, including trend analysis, identifying patterns, and calculating changes over time. For instance, if you want to calculate the month-to-month growth rate, you might use a lag function to access the previous month’s data.\nThere is also method chaining. To know more on that and others Pandas concepts, refer to my other notes."
  },
  {
    "objectID": "notes/data-analysis/pandas/index.html#explore-columns",
    "href": "notes/data-analysis/pandas/index.html#explore-columns",
    "title": "Notes on Pandas",
    "section": "",
    "text": "1df.columns\n\n2df.drop(['#columntodrop'],axis=1, inplace=True/False)\n\n3df = df[['','']].copy()\n\n#It is generally advised to pick the columns needed than drop from the dataframe. How to list all columns? `df.columns`. Then make a variable and assign the columns needed.\ncols = ['col1', 'col2', 'col3']\ndf = df[cols] \n\n4df.isna()\ndf.isnull()\n\n5df.isna().sum()\n\n6df.nunique()\n\n7df['column_name'].unique()\n\n8df['column_name'].value_counts()\n\n9df.corr()\n\n1\n\nList all the column names.\n\n2\n\nMention the columns to drop/remove from DF.\n\n3\n\nTells pandas this is a new DF and not a reference to the prev DF.\n\n4\n\nDisplays true or false for check if there is a null in values.\n\n5\n\nGives the sum of null values in each columns.\n\n6\n\nReturns the number of unique values in each column.\n\n7\n\nReturns an array of unique values in a specific column.\n\n8\n\nReturns a Series containing counts of unique values in a specific column.\n\n9\n\nCalculates the correlation between numeric columns.\n\n\n\n\n\n1df['cleandt'] = pd.to_datetime(df['ColumnName'])\n\n2df.rename(columns={'':'','':''})\n\n3df.columns = [col.replace(' ', '') for col in df.columns]\n\n1\n\nChange a column to datetime format.\n\n2\n\nRename column names.\n\n3\n\nRemove white spaces in columns names.\n\n\n\n\n\n1df.duplicated()\n\n2df.loc[df.duplicated()]\n\n3df.loc[df.duplicated(subset=['ColumnName'])]\ndf.loc[df.duplicated(df.query('column_name==\"\"'))]\n\n4df_new = df.loc[~df.duplicated(subset=['ColumnName','ColumnName','ColumnName'])].reset_index(drop=True)\n\n1\n\nTrue or False for duplicated rows.\n\n2\n\nSelect rows that are duplicated.\n\n3\n\nFinds duplicate rows based on the ‘ColumnName’ column. The result will be a DF containing the duplicate rows.\n\n4\n\nInverse of duplicated values. This way only non-duplicated rows can be selected and assigned to new DF. We reset_index to reset the index.\n\n\n\n\n\n1df.groupby('column_name').agg({'column_to_aggregate': 'aggregation_function'})\n\n2df['column_name'].sum() .mean() .median() .min() .max() .std() .var() .count()\n\n3df['column_name'].quantile(q) .cumsum() .cumprod()\n\n4df.agg({'column_name_1': 'mean', 'column_name_2': 'sum'})\n\n5df.resample('D').sum()\n\n1\n\nGroups data by a specific column and applies an aggregation function to another column.\n\n2\n\nAggregate functions that return a single value.\n\n3\n\nCalculates quantile(0.25), cumulative sum and product.\n\n4\n\nCalculates a separate aggregate for each column.\n\n5\n\nIf the date is the index, then the DF can be resampled by specified time frequency and aggregated.\n\n\n\n\n\nPivot table is allows to reorganize and aggregate data based on one or more columns. It provides a way to create a multidimensional view of your data. It is a powerful reporting tool.\nimport pandas as pd\n\n# Create a sample DataFrame\ndata = {'category': ['A', 'B', 'A', 'B', 'A', 'B'],\n        'values': [10, 20, 30, 40, 50, 60]}\n\ndf = pd.DataFrame(data)\n\n# Create a pivot table\npivot_table = df.pivot_table(values='values', index='category', aggfunc='sum')\n\nprint(pivot_table)\n          \n          values\ncategory       \nA            90\nB           120\n\nvalues: The column to aggregate (in this case, ‘values’).\nindex: The column to use as the index of the pivot table (in this case, ‘category’).\naggfunc: The aggregation function to use when summarizing the data (in this case, we use ‘sum’ to add up the values)."
  },
  {
    "objectID": "notes/data-analysis/pandas/index.html#windowed-aggregates",
    "href": "notes/data-analysis/pandas/index.html#windowed-aggregates",
    "title": "Notes on Pandas",
    "section": "",
    "text": "1df['Rolling Mean'] = df['Value'].rolling(window=3).mean()\n\n2df['Expanding Sum'] = df['Value'].expanding().sum()\n\n3df['Shifted Value'] = df['Value'].shift(1)\n\n4df['Shifted Value'] = df['Value'].shift(-1)\n\n5df['Rank'] = df['Value'].rank()\n\n6df['Percentage Change'] = df['Value'].pct_change()\n\n7weekly_rolling_mean = df['data'].resample('W').mean().rolling(window=3).mean()\n\n1\n\nComputes rolling statistics, such as rolling mean, sum, etc., over a specified window of rows.\n\n2\n\nComputes expanding statistics, which includes all preceding rows.\n\n3\n\nShifts the values of a column by a specified number of periods. Here it shifts down the current row by one row. SQL equivalent of LAG.\n\n4\n\nShifts up the current row by one row. SQL equivalent of LEAD.\n\n5\n\nAssigns a rank to each value in a column based on a specified ordering.\n\n6\n\nComputes the percentage change between the current and a prior element.\n\n7\n\nResample to weekly data and calculate the rolling mean over a window of 3 weeks."
  },
  {
    "objectID": "notes/data-analysis/pandas/index.html#notes-on-lead-and-lag-functions",
    "href": "notes/data-analysis/pandas/index.html#notes-on-lead-and-lag-functions",
    "title": "Notes on Pandas",
    "section": "",
    "text": "The lead function provides information about future values. For example, if you have a time series dataset and you apply a lead function with a lead of 1, it will give you the value of the next time period.\nLeads are used to make predictions or forecasts based on historical data. For instance, if you’re trying to predict sales for the next month, you might use a lead function to access the current month’s data.\n\n\n\nThe lag function provides information about past values. For example, if you have a time series dataset and you apply a lag function with a lag of 1, it will give you the value of the previous time period.\nLags are used for various purposes, including trend analysis, identifying patterns, and calculating changes over time. For instance, if you want to calculate the month-to-month growth rate, you might use a lag function to access the previous month’s data.\nThere is also method chaining. To know more on that and others Pandas concepts, refer to my other notes."
  },
  {
    "objectID": "notes/data-engineering/Debezium-CDC/index.html",
    "href": "notes/data-engineering/Debezium-CDC/index.html",
    "title": "Debezium",
    "section": "",
    "text": "These are my notes about Debezium. There will be typos or misunderstood concepts. Please as always reach out to me to rectify them.\nMy sources were Baeldung and official Debezium documentation. Also the FAQ section of the official site gives all the necessary reading."
  },
  {
    "objectID": "notes/data-engineering/Debezium-CDC/index.html#what-is-debezium",
    "href": "notes/data-engineering/Debezium-CDC/index.html#what-is-debezium",
    "title": "Debezium",
    "section": "What is Debezium?",
    "text": "What is Debezium?\nIt is an open source, low latency distributed platform for capturing change in data from a source and syncing with a target."
  },
  {
    "objectID": "notes/data-engineering/Debezium-CDC/index.html#why-is-it-needed",
    "href": "notes/data-engineering/Debezium-CDC/index.html#why-is-it-needed",
    "title": "Debezium",
    "section": "Why is it needed?",
    "text": "Why is it needed?\nMost companies still use batch processing that means - a. data is not synced immediately, b. more resources are required when sync happens, c. data replication happens at specific intervals.\nHowever, what if streaming data is used or quick reporting on the new data is required? Then we need some kind of a platform/service that periodically checks the source and when an event change occurs, it has to pick the change and transfer it somewhere for analysis or storage. Debezium as a microservice provides that functionality."
  },
  {
    "objectID": "notes/data-engineering/Debezium-CDC/index.html#advantages-of-using-debezium",
    "href": "notes/data-engineering/Debezium-CDC/index.html#advantages-of-using-debezium",
    "title": "Debezium",
    "section": "Advantages of using Debezium",
    "text": "Advantages of using Debezium\n\nUpstream data(source) change is incrementally pushed downstream(sink) – continuous sync,\nInstant updates eliminates bulk load updates – Data transfer cost less,\nFewer resources required,"
  },
  {
    "objectID": "notes/data-engineering/Debezium-CDC/index.html#use-cases",
    "href": "notes/data-engineering/Debezium-CDC/index.html#use-cases",
    "title": "Debezium",
    "section": "Use cases",
    "text": "Use cases\n\nKeep different data sources in sync,\nUpdate or invalidate a cache,\nUpdate search indexes,\nData sync across microservices"
  },
  {
    "objectID": "notes/data-engineering/Debezium-CDC/index.html#debezium-architecture",
    "href": "notes/data-engineering/Debezium-CDC/index.html#debezium-architecture",
    "title": "Debezium",
    "section": "Debezium architecture",
    "text": "Debezium architecture\nDebezium is basically a handshake service/protocol for source and target. It is achieved through connectors.\n\nAs seen in the image, Debezium architecture consists of three components – external source DBs, Debezium server, and downstream applications such as Redis, Amazon Kinesis, Google Pub/Sub or Apache Kakfa. Debezium server acts as a mediator to capture and stream real-time data change between source DBs and consumer applications.\nIf we look at the entire data pipeline as shown in the below image,\n\nthe Debezium source connectors monitor and capture real-time data updates puts them into Kafka topics. These topics capture updates in the forms of commit log, which is ordered sequentially for easy retrieval. These records are then transfered to downstream applications using sink connectors.\nIf Debezium connects to Apache Kafka, it generally uses Apache Kafka Connect(AKC). Like Debezium, AKC is also distributed to manage Kafka brokers."
  },
  {
    "objectID": "notes/data-engineering/Debezium-CDC/index.html#debezium-connectors-vs-kafka-connect",
    "href": "notes/data-engineering/Debezium-CDC/index.html#debezium-connectors-vs-kafka-connect",
    "title": "Debezium",
    "section": "Debezium connectors vs Kafka Connect",
    "text": "Debezium connectors vs Kafka Connect\nDebezium(DBZ) provides a library of CDC connectors whereas Kafka Connect comprises JDBC connectors to interact with external or downstream applications.\nDBZ connectors can only be used as source connectors to monitor external DBs whereas AKC can be both source and sink connectors.\nIn Kafka Connect, the JDBC source connector imports or reads real-time messages from any external data source, while the JDBC sink connector distributes real-time records across multiple consumer applications.\nJDBC connectors do not capture and stream deleted records, whereas CDC connectors are capable of streaming all real-time updates, including deleted entries.\nMoreover, JDBC connections always query database updates at certain and predetermined intervals, while CDC connectors regularly record and transmit real-time event changes as soon as they occur on the respective database systems."
  },
  {
    "objectID": "notes/data-engineering/Debezium-CDC/index.html#connector-data",
    "href": "notes/data-engineering/Debezium-CDC/index.html#connector-data",
    "title": "Debezium",
    "section": "Connector data",
    "text": "Connector data\nThe change stream consists of schema and payload.\nThe schema is not to be confused with DB schema. This schema describes the data types of all the fields in the payload section. Usually for JSON messages, schema is not included.\nThe change event data stream payload looks something like this -\n\n\nchange event data stream\n\n{\n  \"value\":{\n    \"before\":null,\n    \"after\":{\n      \"id\":89,\n      \"name\":\"Colleen Myers\",\n      \"description\":\"Nothing evening stand week reveal quickly man traditional. True positive second because lose detail.\\nNice enough become woman then staff along. Life receive account. Many exist data thousand.\",\n      \"price\":98.0\n    },\n    \"source\":{\n      \"version\":\"2.2.0.Alpha3\",\n      \"connector\":\"postgresql\",\n      \"name\":\"debezium\",\n      \"ts_ms\":1692626411411,\n      \"snapshot\":\"false\",\n      \"db\":\"postgres\",\n      \"sequence\":\"[\\\"23395760\\\",\\\"23395904\\\"]\",\n      \"schema\":\"commerce\",\n      \"table\":\"products\",\n      \"txId\":847,\n      \"lsn\":23395904,\n      \"xmin\":null\n    },\n    \"op\":\"c\",\n    \"ts_ms\":1692626411879,\n    \"transaction\":null\n  }\n}\n\nThis is for inserting an data entry. For an update the stream looks like -\n{\n  \"value\":{\n    \"before\":{\n      \"id\":95,\n      \"name\":\"Steven Cowan\",\n      \"description\":\"Heavy rise something sell case institution chair. Control them might court surface none property. Subject behind them. Quickly near trial.\",\n      \"price\":75.0\n    },\n    \"after\":{\n      \"id\":95,\n      \"name\":\"Yvonne Collins\",\n      \"description\":\"Heavy rise something sell case institution chair. Control them might court surface none property. Subject behind them. Quickly near trial.\",\n      \"price\":75.0\n    },\n    \"source\":{\n      \"version\":\"2.2.0.Alpha3\",\n      \"connector\":\"postgresql\",\n      \"name\":\"debezium\",\n      \"ts_ms\":1692626421005,\n      \"snapshot\":\"false\",\n      \"db\":\"postgres\",\n      \"sequence\":\"[\\\"23399328\\\",\\\"23399456\\\"]\",\n      \"schema\":\"commerce\",\n      \"table\":\"products\",\n      \"txId\":854,\n1      \"lsn\":23399456,\n      \"xmin\":null\n    },\n    \"op\":\"u\",\n    \"ts_ms\":1692626421499,\n    \"transaction\":null\n  }\n}\n\n1\n\nLSN- Log sequence number. An unique number for every change entry. Used to track and order transactions and changes within the transaction log."
  },
  {
    "objectID": "notes/data-engineering/Debezium-CDC/index.html#streaming-changes---postgresql",
    "href": "notes/data-engineering/Debezium-CDC/index.html#streaming-changes---postgresql",
    "title": "Debezium",
    "section": "Streaming Changes - PostgreSQL",
    "text": "Streaming Changes - PostgreSQL\nThe PostgreSQL connector typically spends the vast majority of its time streaming changes from the PostgreSQL server to which it is connected. This mechanism relies on PostgreSQL’s replication protocol. This protocol enables clients to receive changes from the server as they are committed in the server’s transaction log at certain positions, which are referred to as Log Sequence Numbers (LSNs).\nThe Debezium PostgreSQL connector acts as a PostgreSQL client. When the connector receives changes it transforms the events into Debezium create, update, or delete events that include the LSN of the event. The PostgreSQL connector forwards these change events in records to the Kafka Connect framework, which is running in the same process. The Kafka Connect process asynchronously writes the change event records in the same order in which they were generated to the appropriate Kafka topic.1"
  },
  {
    "objectID": "notes/data-engineering/Debezium-CDC/index.html#setting-up-permissions",
    "href": "notes/data-engineering/Debezium-CDC/index.html#setting-up-permissions",
    "title": "Debezium",
    "section": "Setting up permissions",
    "text": "Setting up permissions\nUse this link to set up permissions.\nCreate Debezium user with minimum privielges to avoid security breaches."
  },
  {
    "objectID": "notes/data-engineering/Debezium-CDC/index.html#postgres-source-connector-config",
    "href": "notes/data-engineering/Debezium-CDC/index.html#postgres-source-connector-config",
    "title": "Debezium",
    "section": "Postgres source connector config",
    "text": "Postgres source connector config\n\n\npg-src-connector.json\n\n{\n    \"name\": \"pg-src-connector\",\n    \"config\": {\n1        \"connector.class\": \"io.debezium.connector.postgresql.PostgresConnector\",\n        \"tasks.max\": \"1\",\n2        \"database.hostname\": \"postgres\",\n        \"database.port\": \"5432\",\n        \"database.user\": \"postgres\",\n        \"database.password\": \"postgres\",\n        \"database.dbname\": \"postgres\",\n        \"database.server.name\": \"postgres\",\n        \"database.include.list\": \"postgres\",\n3        \"topic.prefix\": \"debezium\",\n4        \"schema.include.list\": \"commerce\"\n    }\n}\n\n\n1\n\na postgres DB connector\n\n2\n\nDB’s configurations\n\n3\n\nKafka topic prefix that is used in Kafka topic.\n\n4\n\nThe tables in the schema the server monitors for changes\n\n\nSo the Debezium server will monitor postgres DB as user postgres and the same password at port 5432 on the tables in schema commerce.\nThe kafka topic prefix is debezium, the schema commerce which has two tables - products and users.\nSo, the connector would stream records to these two Kafka topics:\n\ndebezium.commerce.products and\ndebezium.commerice.users\n\nread more on topic names here."
  },
  {
    "objectID": "notes/data-engineering/Debezium-CDC/index.html#postgresql-on-aws-rds",
    "href": "notes/data-engineering/Debezium-CDC/index.html#postgresql-on-aws-rds",
    "title": "Debezium",
    "section": "Postgresql on AWS RDS",
    "text": "Postgresql on AWS RDS\nUsing this link."
  },
  {
    "objectID": "notes/data-engineering/Debezium-CDC/index.html#common-issue-with-decimal-data-types",
    "href": "notes/data-engineering/Debezium-CDC/index.html#common-issue-with-decimal-data-types",
    "title": "Debezium",
    "section": "Common issue with Decimal data types",
    "text": "Common issue with Decimal data types\nDebezium that uses Kafka connect serialises decimal values for Kafka connect to understand. That means Debezium, converts to BigDecimal binary and encodes in Base64. At the output or downstream, onus is on the user to decode and convert back to the original value.\nSo, a decimal or numeric data type at the source, will become something like eA== as it passes through Debezium and reaches Kafka topic.\nA possible solution is to use REAL data type in SQL. However, this might not be possible in many real-life scenarios. Hence, a property decimal.handling.mode in connector can be set to either string or double. In string case, proper de-serialiser has to be implemented at the receiver. With double precision is sometimes lost.\nIt is quite a conundrum. Read more on this issue and solution here, here and here."
  },
  {
    "objectID": "notes/data-engineering/Debezium-CDC/index.html#footnotes",
    "href": "notes/data-engineering/Debezium-CDC/index.html#footnotes",
    "title": "Debezium",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://debezium.io/documentation/reference/stable/connectors/postgresql.html#postgresql-streaming-changes↩︎"
  },
  {
    "objectID": "notes/data-engineering/apache-Kafka/commands-cheatsheet.html",
    "href": "notes/data-engineering/apache-Kafka/commands-cheatsheet.html",
    "title": "Kafka and Kafka connect command cheatsheet",
    "section": "",
    "text": "These commands I used many times. This page notes down them for quick reference in future."
  },
  {
    "objectID": "notes/data-engineering/apache-Kafka/commands-cheatsheet.html#kafka-connect",
    "href": "notes/data-engineering/apache-Kafka/commands-cheatsheet.html#kafka-connect",
    "title": "Kafka and Kafka connect command cheatsheet",
    "section": "Kafka Connect",
    "text": "Kafka Connect\nKafka connect uses Rest API so we can interact with a simple curl command.\n\n\nCommands for connectors\n\n#sudo apt instal jq -y\n1curl -X GET http://localhost:8083/connectors| jq\n2curl -X GET http://localhost:8083/connectors?expand=status | jq\n3curl -X GET http://localhost:8083/connectors?expand=info | jq\n4curl -X GET http://localhost:8083/connectors/&lt;connector-name&gt; | jq\n5curl -X DELETE http://localhost:8083/connectors/&lt;connector-name&gt; | jq\n\n\n1\n\nGet list of all connectors\n\n2\n\nGet status of all connectors\n\n3\n\nGet info of all connectors\n\n4\n\nGet a connector name and use it here to get information of it\n\n5\n\nDelete a respective connector\n\n\nTo know more on jq read here."
  },
  {
    "objectID": "notes/data-engineering/apache-Kafka/commands-cheatsheet.html#kafka-commands",
    "href": "notes/data-engineering/apache-Kafka/commands-cheatsheet.html#kafka-commands",
    "title": "Kafka and Kafka connect command cheatsheet",
    "section": "Kafka commands",
    "text": "Kafka commands\n\n\nCommands for Kafka\n\n1docker exec -it kafka bin/kafka-topics.sh --list --bootstrap-server localhost:9092\n2docker exec -it kafka bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic debezium.commerce.products --from-beginning\n3docker exec -it kafka bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test_debezium.commerce.products --from-beginning | grep -i \"\\\"price\\\":\"\n\n\n1\n\nList all topics connected to the bootstrap-server\n\n2\n\nConsume messages with the console consumer\n\n3\n\nConsume messages with the console consumer but with a grep filter."
  },
  {
    "objectID": "notes/data-engineering/apache-Kafka/commands-cheatsheet.html#kafka-configuration",
    "href": "notes/data-engineering/apache-Kafka/commands-cheatsheet.html#kafka-configuration",
    "title": "Kafka and Kafka connect command cheatsheet",
    "section": "Kafka configuration",
    "text": "Kafka configuration\n\n\nkafka configuration\n\n  zookeeper:\n    image: debezium/zookeeper:2.4\n    container_name: zookeeper\n    ports:\n      - \"2181:2181\"\n    networks:\n      - my_network\n  kafka:\n    container_name: kafka\n    image: debezium/kafka:latest\n    ports:\n      - \"9093:9093\"\n    environment:\n      - ZOOKEEPER_CONNECT=zookeeper:2181\n      - KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=LISTENER_INT:PLAINTEXT,LISTENER_EXT:PLAINTEXT\n      - KAFKA_ADVERTISED_LISTENERS=LISTENER_INT://kafka:9092,LISTENER_EXT://localhost:9093\n      - KAFKA_LISTENERS=LISTENER_INT://0.0.0.0:9092,LISTENER_EXT://0.0.0.0:9093\n      - KAFKA_INTER_BROKER_LISTENER_NAME=LISTENER_INT\n    depends_on:\n      - zookeeper\n    networks:\n      - my_network\n\nnetworks:\n  my_network:"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Deepak Ramani blog",
    "section": "",
    "text": "Title\n\n\nDate\n\n\nDescription\n\n\nReading Time\n\n\n\n\n\n\nTesting Data Pipelines\n\n\nSep 2, 2023\n\n\nA post on how to test data pipelines\n\n\n10 min\n\n\n\n\nStoring secrets in JSON files\n\n\nAug 30, 2023\n\n\na short post on showing how to use secrets in JSON files\n\n\n6 min\n\n\n\n\nChange Data Capture with Debezium, Kafka, S3\n\n\nAug 29, 2023\n\n\nUsing Debezium capture data changes and stream it downstream to consumers.\n\n\n19 min\n\n\n\n\nBuilding Data Pipeline - Part 4 - CI/CD\n\n\nJun 13, 2023\n\n\nBuild CI/CD pipeline with Github actions\n\n\n5 min\n\n\n\n\nTerraform - Attach IAM policies to a role\n\n\nJun 12, 2023\n\n\nA short post on how to attach policies to an IAM role within Terraform\n\n\n4 min\n\n\n\n\nBuilding Data Pipelines - Part 3 - Terraform\n\n\nJun 11, 2023\n\n\nManage AWS hosted web app with Terraform\n\n\n9 min\n\n\n\n\nBuilding Data Pipelines - Part 2 - AWS Cloud\n\n\nJun 4, 2023\n\n\nA post on migrating the web app to AWS cloud\n\n\n10 min\n\n\n\n\nBuilding Data Pipelines - Part 1 - Docker\n\n\nJun 3, 2023\n\n\nA short post on using docker to deploy lambda function locally\n\n\n4 min\n\n\n\n\nDeploy Web Application with Docker\n\n\nJun 3, 2023\n\n\nA short guide on how to deploy a flask application using docker\n\n\n7 min\n\n\n\n\nGetting started with Terraform\n\n\nJun 2, 2023\n\n\nAn introductory post on getting started with Terraform in Mac and Ubuntu and spin up a docker nginx container\n\n\n7 min\n\n\n\n\nGetting started with S3 using boto3\n\n\nApr 27, 2023\n\n\nAn introduction to S3 with boto3 AWS python SDK\n\n\n6 min\n\n\n\n\nSetup Skim PDF reader with VimTeX in Mac OS\n\n\nFeb 6, 2023\n\n\nA short post to setup Skim pdf reader with Vimtex plugin in Mac OS.\n\n\n4 min\n\n\n\n\nUsing json_normalize Pandas function\n\n\nSep 24, 2022\n\n\nA tutorial with examples on flattening JSON object using json_normalize pandas function\n\n\n15 min\n\n\n\n\nManage dotfiles with GNU Stow\n\n\nJan 9, 2022\n\n\nA guide to manage all the dotfiles.\n\n\n4 min\n\n\n\n\nSetting up Kaggle on Linux/Mac\n\n\nSep 18, 2021\n\n\nA guide to setup Kaggle API on Linux/Mac.\n\n\n3 min\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#blog-posts",
    "href": "index.html#blog-posts",
    "title": "Deepak Ramani blog",
    "section": "",
    "text": "Title\n\n\nDate\n\n\nDescription\n\n\nReading Time\n\n\n\n\n\n\nTesting Data Pipelines\n\n\nSep 2, 2023\n\n\nA post on how to test data pipelines\n\n\n10 min\n\n\n\n\nStoring secrets in JSON files\n\n\nAug 30, 2023\n\n\na short post on showing how to use secrets in JSON files\n\n\n6 min\n\n\n\n\nChange Data Capture with Debezium, Kafka, S3\n\n\nAug 29, 2023\n\n\nUsing Debezium capture data changes and stream it downstream to consumers.\n\n\n19 min\n\n\n\n\nBuilding Data Pipeline - Part 4 - CI/CD\n\n\nJun 13, 2023\n\n\nBuild CI/CD pipeline with Github actions\n\n\n5 min\n\n\n\n\nTerraform - Attach IAM policies to a role\n\n\nJun 12, 2023\n\n\nA short post on how to attach policies to an IAM role within Terraform\n\n\n4 min\n\n\n\n\nBuilding Data Pipelines - Part 3 - Terraform\n\n\nJun 11, 2023\n\n\nManage AWS hosted web app with Terraform\n\n\n9 min\n\n\n\n\nBuilding Data Pipelines - Part 2 - AWS Cloud\n\n\nJun 4, 2023\n\n\nA post on migrating the web app to AWS cloud\n\n\n10 min\n\n\n\n\nBuilding Data Pipelines - Part 1 - Docker\n\n\nJun 3, 2023\n\n\nA short post on using docker to deploy lambda function locally\n\n\n4 min\n\n\n\n\nDeploy Web Application with Docker\n\n\nJun 3, 2023\n\n\nA short guide on how to deploy a flask application using docker\n\n\n7 min\n\n\n\n\nGetting started with Terraform\n\n\nJun 2, 2023\n\n\nAn introductory post on getting started with Terraform in Mac and Ubuntu and spin up a docker nginx container\n\n\n7 min\n\n\n\n\nGetting started with S3 using boto3\n\n\nApr 27, 2023\n\n\nAn introduction to S3 with boto3 AWS python SDK\n\n\n6 min\n\n\n\n\nSetup Skim PDF reader with VimTeX in Mac OS\n\n\nFeb 6, 2023\n\n\nA short post to setup Skim pdf reader with Vimtex plugin in Mac OS.\n\n\n4 min\n\n\n\n\nUsing json_normalize Pandas function\n\n\nSep 24, 2022\n\n\nA tutorial with examples on flattening JSON object using json_normalize pandas function\n\n\n15 min\n\n\n\n\nManage dotfiles with GNU Stow\n\n\nJan 9, 2022\n\n\nA guide to manage all the dotfiles.\n\n\n4 min\n\n\n\n\nSetting up Kaggle on Linux/Mac\n\n\nSep 18, 2021\n\n\nA guide to setup Kaggle API on Linux/Mac.\n\n\n3 min\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#projects",
    "href": "index.html#projects",
    "title": "Deepak Ramani blog",
    "section": "Projects",
    "text": "Projects\n\n\n\n\n\nProject\n\n\nDescription\n\n\n\n\n\n\nCapture Change data using Debezium and Kafka\n\n\nA data pipeline that looks for changes and streams it through Kafka downstream for analytics. \n\n\n\n\nGrocery Unit Sale Predictor - a MLOPS project\n\n\nAn end-to-end ML project that designs and manages a ML model production workflow. \n\n\n\n\nAutonomous driving framework - a DL project\n\n\nAn end-to-end autonomous driving neural network ML model using simpler CNN architecture. \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-06-13-Github-Actions-CI-CD-TF/index.html",
    "href": "posts/2023-06-13-Github-Actions-CI-CD-TF/index.html",
    "title": "Building Data Pipeline - Part 4 - CI/CD",
    "section": "",
    "text": "The Continuous Integration and Continuous Delivery abbreviated as CI/CD is an important part of software development cycle. This part provides the essential step in checking everything before the final product is ready to delivered to the customer. In our case, this step should create infrastructure to deploy the web application into the cloud. In other words, automate the tasks we did in part 3.\nThere are many tools for CI/CD. We will be using Github Actions as our code is in Github and make sense to automate the build, test and deploy pipeline with it. Our workflow will consist of running CI upon a pull request and CD when this pull request is merged to a particular branch."
  },
  {
    "objectID": "posts/2023-06-13-Github-Actions-CI-CD-TF/index.html#triggers",
    "href": "posts/2023-06-13-Github-Actions-CI-CD-TF/index.html#triggers",
    "title": "Building Data Pipeline - Part 4 - CI/CD",
    "section": "Triggers",
    "text": "Triggers\nWe want to setup event based triggers to activate our workflows. Since we don’t to distrub main branch code base, we create two branches - develop and feature/ci-cd. We switch to the feature/ci-cd branch and define our workflows.\nIn ci-test.yml the trigger is set on pull_request on the branch develop. Meaning we commit our changes to the feature/ci-cd branch and then do a pull_request for develop branch. A pull in Github speak means “pulling/updating” new contents from another remote branch. Pull requests are usually done in UI as it is easy to review, clarify, approve changes and merge request.\nHowever, in cd-deploy.yml we need our infrastructure setup deployed. So it makes sense to put a trigger upon commits being pushed into the develop branch. SO the trigger is push. This action is activated when the pull_request is merged into the develop branch."
  },
  {
    "objectID": "posts/2023-06-13-Github-Actions-CI-CD-TF/index.html#jobs",
    "href": "posts/2023-06-13-Github-Actions-CI-CD-TF/index.html#jobs",
    "title": "Building Data Pipeline - Part 4 - CI/CD",
    "section": "Jobs",
    "text": "Jobs\n\nci-test.yml\nIn CI, it is critical to check if all the parts are integrated together correctly. In our case, we need to confirm whether tf-plan works with our Terraform configuration.\nIn Github Actions, jobs are can be run on several hosted systems through actions. We will use ubuntu-latest with action version 3. That will in run calls aws-actions for credentials. We provide our AWS secrets to our Github secrets page. Github says the secrets are encrypted before reaching them. They also take measures to prevents secrets appearing in logs. 2\nThe actions access the AWS secrets to setup our final task to run our infrastructure plan.\n\n\nGithub Secrets\n\nenv:\n  AWS_DEFAULT_REGION: \"us-east-1\"\n  AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}\n  AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n\njobs:\n  tf-plan:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - name: Configure AWS Credentials\n        uses: aws-actions/configure-aws-credentials@v2\n        with:\n1          aws-access-key-id: ${{ env.AWS_ACCESS_KEY_ID }}\n          aws-secret-access-key: ${{ env.AWS_SECRET_ACCESS_KEY }}\n          aws-region: ${{ env.AWS_DEFAULT_REGION }} \n\n\n1\n\n`${{ secrets.AWS_ACCESS_KEY_ID }} can also be directly used\n\n\nSince the pipeline is used in production environment, we configure our terraform backend with our new production tfstate and use the plan command to check if our plan is valid, error-free and suitable to our needs.\n\n\nTF Plan\n\n- name: TF plan\n   id: plan\n1    working-directory: \"infrastructure\"\n     run: |\n      terraform init -backend-config=\"key=mlops-grocery-sales-prod.tfstate\" --reconfigure && terraform plan --var-file vars/prod.tfvars\n\n\n1\n\nChange to infrastructure directory to execute the terraform command\n\n\n\n\ncd-deploy.yml\nCI in this case does only tf-plan but usually it runs several unit and integration tests. We could combine these two files into one if we want but we will leave things as it is.\nSimilarly, the cd-deply.yml workflow file, check the tf-plan. Upon its success triggers the tf-apply job.\n\n\ntf-apply\n\n- name: TF Apply\n    id: tf-apply\n    working-directory: \"infrastructure\"\n    if: ${{ steps.tf-plan.outcome }} == \"success\"\n    run: |\n      terraform apply -auto-approve -var-file=vars/prod.tfvars\n      echo \"name=rest_api_url::$(terraform output rest_api_url | xargs)\" &gt;&gt; $GITHUB_OUTPUT\n      echo \"name=ecr_repo::$(terraform output run_id | xargs)\" &gt;&gt; $GITHUB_OUTPUT\n      echo \"name=run_id::$(terraform output run_id | xargs)\" &gt;&gt; $GITHUB_OUTPUT\n      echo \"name=lambda_function::$(terraform output lambda_function | xargs)\" &gt;&gt; $GITHUB_OUTPUT\n\n$GITHUB_OUTPUT takes Terraform outputs and displays them in the logs."
  },
  {
    "objectID": "posts/2023-06-13-Github-Actions-CI-CD-TF/index.html#deleting-the-infrastructure",
    "href": "posts/2023-06-13-Github-Actions-CI-CD-TF/index.html#deleting-the-infrastructure",
    "title": "Building Data Pipeline - Part 4 - CI/CD",
    "section": "Deleting the infrastructure",
    "text": "Deleting the infrastructure\nIf for some reason you wish to delete the resources created in the AWS Cloud, please follow these steps -\n\nGoto infrastructure’s main.tf file. Comment out all the modules. Also comment out contents in outputs.tf\nThen commit the changes, push, pull the request and merge the changes.\n\nThis should delete all the resources. Confirm with AWS Console.\nAlternatively we can use terraform destroy ourselves if we know which tfstate and tfvars were used.\n\n\ntf-destroy\n\n1terraform init -backend-config=\"key=mlops-grocery-sales-prod.tfstate\" --reconfigure\n2terraform destroy -var-file=vars/prod.tfvars\n\n\n1\n\nSince we know which Terraform statefile is used for production\n\n2\n\nWhich production variable file is used"
  },
  {
    "objectID": "posts/2023-06-13-Github-Actions-CI-CD-TF/index.html#footnotes",
    "href": "posts/2023-06-13-Github-Actions-CI-CD-TF/index.html#footnotes",
    "title": "Building Data Pipeline - Part 4 - CI/CD",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://docs.github.com/en/actions/learn-github-actions/understanding-github-actions#create-an-example-workflow↩︎\nhttps://docs.github.com/en/actions/security-guides/security-hardening-for-github-actions#using-secrets↩︎"
  },
  {
    "objectID": "posts/2023-09-02-testing-data-pipelines/index.html",
    "href": "posts/2023-09-02-testing-data-pipelines/index.html",
    "title": "Testing Data Pipelines",
    "section": "",
    "text": "Testing is one of the important phases in software development cycle before a software is turned to production.\n\nTesting is a way to simulate and the application for all possible actions from users before delivery. It reveals flaws in development and sometimes even upto design phase. Imagine a scenario in which a user has to enter price of a product. Mistakenly they enter 1w instead of 12. If this pattern happens regularly, and in the monthly report, these numbers won’t add up at all. With testing, we can set guardrails to protect against such mistakes.\nIn my post, I designed a pipeline to check a Postgres DB for data changes and transport those changes through Apache Kafka and store in S3-like storage. However, I didn’t have time to write proper test cases. After publishing that post, I created unit and integration test cases. In this post we will go through how each test case was developed."
  },
  {
    "objectID": "posts/2023-09-02-testing-data-pipelines/index.html#source-system",
    "href": "posts/2023-09-02-testing-data-pipelines/index.html#source-system",
    "title": "Testing Data Pipelines",
    "section": "Source system",
    "text": "Source system\nSource system in our case is just two components – a python script that generates data and Postgres database that stores the generated data. With automated testing, we will do checks on individual component – unit tests, a combined test – integration test.\n\nSetup\nI created a directory tests to mimic a testing environment. File structure drawn using this wonderful utility app.\n\n\ntests directory structure\n\n.\n├── postgres\n1│   └── init.sql\n2├── Makefile\n3├── docker-compose-test.yml\n└── tests\n4    ├── Pipfile\n    ├── Pipfile.lock\n    ├── integration\n5    │   └── test_datagen_db.py\n6    ├── test-setup-connections.sh\n7    ├── test_Dockerfile\n    ├── unit\n8    │   ├── test_datagen.py\n9    │   └── test_pg_src_db.py\n10    └── user_product_data.py\n\n\n1\n\nInitilisation script to setup DB\n\n2\n\nMakefile with added commands for testing\n\n3\n\nDocker-compose template file for testing\n\n4\n\nPipfile contains dev packages for testing\n\n5\n\nA testing script to test integration between datagen and DB\n\n6\n\nA shell script to establish connectors\n\n7\n\nTest dockerfile to copy all the files and install dependencies\n\n8\n\nUnit test script for datagen\n\n9\n\nUnit test script for Postgres source DB\n\n10\n\nOriginal script for which testing automation is written"
  },
  {
    "objectID": "posts/2023-09-02-testing-data-pipelines/index.html#docker-compose-file",
    "href": "posts/2023-09-02-testing-data-pipelines/index.html#docker-compose-file",
    "title": "Testing Data Pipelines",
    "section": "Docker-compose file",
    "text": "Docker-compose file\nSince we’re going to create a separate testing environment, it would be useful to have a separate docker-compose-test.yml file too.\n\nChanges\nChange the host port number\nWhy? Because if we run both testing and live environment in the same machine(most often it wouldn’t be), then there will be clashing of ports.\n\nSo, for DB host port changes from 5432 to 15432,\nZookeeper - 2181 to 12181,\nKafka - 9093 to 19093 and also corresponding environment changes,\nDebezium Kafka Connect - 8083 to 18083.\n\nCreating a new test network\nnetworks:\n  my_test_network:\nEnvironment\n\nCreating separate variables to pass test environment parameters."
  },
  {
    "objectID": "posts/2023-09-02-testing-data-pipelines/index.html#dockerfile",
    "href": "posts/2023-09-02-testing-data-pipelines/index.html#dockerfile",
    "title": "Testing Data Pipelines",
    "section": "Dockerfile",
    "text": "Dockerfile\nWe are trying to reproduce live version in testing. That means the test-suite must be built the same way. However, as we run pytest manually and not as an entrypoint with the docker, environment must be made ready for container. This is done with ENV keyword. Remember to pass these variables in docker-compose.yml.\nENV TEST_POSTGRES_USER = ${TEST_POSTGRES_USER}\nENV TEST_POSTGRES_PASSWORD = ${TEST_POSTGRES_PASSWORD}\nENV TEST_POSTGRES_DB = ${TEST_POSTGRES_DB}\nENV TEST_POSTGRES_HOST = ${TEST_POSTGRES_HOST}\nENV DB_SCHEMA = ${DB_SCHEMA}\nAlso we need to set python path to make python execute tests. That is achieved with ENV PYTHONPATH=/test_app/tests/.\nFinally we can keep our container alive with command tail -f /dev/null. This is a unix feature which continuously runs this command and flushes the output to a null directory."
  },
  {
    "objectID": "posts/2023-09-02-testing-data-pipelines/index.html#pipfile-and-pipfile.lock",
    "href": "posts/2023-09-02-testing-data-pipelines/index.html#pipfile-and-pipfile.lock",
    "title": "Testing Data Pipelines",
    "section": "Pipfile and Pipfile.lock",
    "text": "Pipfile and Pipfile.lock\nThe initial requirements are still the same but for testing we add pytest. pipenv allows to categorise packages such as pytest as dev-packages which makes it useful to install only when necessary. To install we use pipenv install --dev --system --deploy. --dev flag installs the dev-packages."
  },
  {
    "objectID": "posts/2023-09-02-testing-data-pipelines/index.html#unit-testing",
    "href": "posts/2023-09-02-testing-data-pipelines/index.html#unit-testing",
    "title": "Testing Data Pipelines",
    "section": "Unit testing",
    "text": "Unit testing\nTesting a unit or single component is called unit testing. Here there are two units – Datagen and source DB.\n\nDatagen unit testing\nA test file in python is named with test prefix. pytest picks up those file automatically to run tests. The unit/component function we are testing is imported in the test file. Here they are generate_user_data and generate_product_data.\nAny function with test as prefix is recognised by pytest library. And assert keyword is used to compare the actual and expected output. This comparision can any condition depending on the case.\nHere we determine if the generated user and product data are of dictionary data type. And determine if the data generated is not empty. Upon successful testing, the pytest should return with 100% and no errors.\n\n\nSource DB\nUnlike datagen testing, source DB testing involves little more preparatory steps such as connection parameters.\nHere we test\n\nWhether DB is reachable with the given connection parameters,\nWhether a simple select 1 command returns the value 1,\nWhether the schema created using init.sql script contains the tables we created – products and users,\n\n– Whether are there any missing tables.\n\n\n\n\n\n\nNote\n\n\n\nAs environment variable I used TEST_SCHEMA but unfortunately the script failed to recognise the variable. If you find out why please let me know.\n\n\nTo execute a function only once per run we can leverage pytest’s scope as module. @pytest.fixture(scope=\"module\")"
  },
  {
    "objectID": "posts/2023-09-02-testing-data-pipelines/index.html#integration-testing",
    "href": "posts/2023-09-02-testing-data-pipelines/index.html#integration-testing",
    "title": "Testing Data Pipelines",
    "section": "Integration testing",
    "text": "Integration testing\nIntegration testing is a test of two or more components such as datagen and source DB.\nWe put this file inside integration directory and as usual prefix it with test, import env variables and setup DB connection.\n\nChanges to original script\nIn the first version of the script to generate data and push it to the DB, I didn’t modularise the functionalities. Every operation – data generation, DB connect, inseration, update, delete was written under one function. Had I not implemented testing, this simple function would be sufficient. Any failure would only point to this single function. However, for testing we must test each operation. That means each operation has be modularised or in other words has to be its own function.\nYes, this made for further changes but the code looks robust and easy to spot failures. This is important point to remember.\n\n\nTesting capabilities\nWe check each operation:\n\nGeneration and inseration of data into respective tables,\nUpdate some records\nDelete some records\nFinally execute all operation sequentially and verify the total number of records remaining. This way we know the operation performed without errors.\n\nOfcourse, these test cases are the basics. Several more can be added to test specific or edge cases.\nA successful testing will results in 100% message being displayed. Now we can be sure the source end of the data pipeline works. Next would be test the middle part of the pipeline. Since the next section would be to test at Kafka and as it is connected with Debezium it is an integration test."
  },
  {
    "objectID": "posts/2023-09-02-testing-data-pipelines/index.html#middle-system-debezium-kafka-connect-and-kafka",
    "href": "posts/2023-09-02-testing-data-pipelines/index.html#middle-system-debezium-kafka-connect-and-kafka",
    "title": "Testing Data Pipelines",
    "section": "Middle system – Debezium Kafka connect and Kafka",
    "text": "Middle system – Debezium Kafka connect and Kafka\nTesting at Kafka means we update docker-compose-test.yml with zookeeper, kafka and debezium with Aiven S3 connector. Please refer to the source code for more info.\nWe use kafka-python library to test at Kafka. Add in kafka-python = \"*\" to the Pipfile and read its dev install.\nFor testing at Kafka, we will need the Kafka topics that be monitored by Kafka broker and also a connection to the source DB to verify if contents came through.\nKAFKA_TOPIC_PRODUCTS = \"test_debezium.commerce.products\" \nKAFKA_TOPIC_USERS = \"test_debezium.commerce.users\"\nTEST_POSTGRES_USER = os.getenv(\"TEST_POSTGRES_USER\")\nTEST_POSTGRES_PASSWORD = os.getenv(\"TEST_POSTGRES_PASSWORD\")\nTEST_POSTGRES_HOSTNAME = os.getenv(\"TEST_POSTGRES_HOST\")\nTEST_POSTGRES_DB = os.getenv(\"TEST_POSTGRES_DB\")\nSCHEMA = os.getenv(\"DB_SCHEMA\", \"commerce\")\nLike previously we introduce pytest’s module functionality for DB connection. In addition we also make KafkaConsumer for products and users similarly so.\n@pytest.fixture(scope=\"module\")\ndef get_consumer_users():\n    consumer_users = KafkaConsumer(\n        KAFKA_TOPIC_USERS,\n        bootstrap_servers=['kafka:9092'],\n        auto_offset_reset=\"earliest\",\n        consumer_timeout_ms=1000\n    )\n    yield consumer_users\n    consumer_users.close()\nThen we test the following:\n\nTest if bootstrap server is connected\n\ndef test_bootstrap_connection(get_consumer_products, get_consumer_users):\n  assert get_consumer_products.bootstrap_connected()\n  assert get_consumer_users.bootstrap_connected()\n\nTest if topic name is correctly subscribed\n\ndef test_topic_name(get_consumer_products, get_consumer_users):\n  assert get_consumer_products.subscription() == {KAFKA_TOPIC_PRODUCTS}\n  assert get_consumer_users.subscription() == {KAFKA_TOPIC_USERS}\n\nTest to check if the total number of transactions is expected\n\ndef test_total_transactions(get_consumer_users, get_consumer_products):\n  lsn_u=[]\n  lsn_p=[]\n  for msg in get_consumer_users:\n        if msg.value:\n            json_object = json.loads(msg.value)\n1            lsn_u.append(json_object['payload']['source']['lsn'])\n\n  for msg in get_consumer_products:\n        if msg.value:\n            json_object = json.loads(msg.value)\n            lsn_p.append(json_object['payload']['source']['lsn'])\n\n  assert len(lsn_u) == 35\n  assert len(lsn_p) == 35\n\n1\n\nParsing through JSON Object to get to the value\n\n\n\nTest if a username from the DB is present in the message\n\ndef test_username_present(get_db_connection):\n  \n  usernames =[]\n\n1  consumer_users = KafkaConsumer(\n        KAFKA_TOPIC_USERS,\n        bootstrap_servers=['kafka:9092'],\n        auto_offset_reset=\"earliest\",\n        consumer_timeout_ms=1000\n    )\n  for msg in consumer_users:\n        if msg.value:\n            json_object = json.loads(msg.value)\n            if json_object['payload']['after']:\n                usernames.append(json_object['payload']['after']['username'])\n\n  consumer_users.close()\n  cur = get_db_connection.cursor()\n  cur.execute(f\"SELECT username FROM {SCHEMA}.users\")\n  result = cur.fetchone()[0]\n\n  assert result in usernames\n\n1\n\nFor some weird reason, the KafkaConsumer pytest module failed to work\n\n\nThe next part would be verify if the Aiven S3 sink connector picked up our messages and stored in Minio S3 storage as JSON files. Once the testing for those have been written, this blog will be updated."
  },
  {
    "objectID": "posts/2023-06-11-terraform-aws/index.html",
    "href": "posts/2023-06-11-terraform-aws/index.html",
    "title": "Building Data Pipelines - Part 3 - Terraform",
    "section": "",
    "text": "In part 2 we saw how to migrate to AWS cloud. In bigger projects where so many resources are present, it is near impossible to remember and manage them using the console. Terraform provides that solution. It allows to track resources just like code. We saw about the use of Terraform in my introduction post on Terraform.\nIn this part, we will take the data pipeline from part 2 and use Terraform to manage the infrastructure."
  },
  {
    "objectID": "posts/2023-06-11-terraform-aws/index.html#module-block-1---ecr",
    "href": "posts/2023-06-11-terraform-aws/index.html#module-block-1---ecr",
    "title": "Building Data Pipelines - Part 3 - Terraform",
    "section": "Module block 1 - ECR",
    "text": "Module block 1 - ECR\n\n\ninfrastructure/modules/ecr/main.tf\n\nmodule \"ecr_image\" {\n1  source = \"./modules/ecr\"\n  ecr_repo_name = \"${var.ecr_repo_name}_${var.project_id}\"\n  account_id = local.account_id\n2  lambda_function_local_path = var.lambda_function_local_path\n3  docker_image_local_path = var.docker_image_local_path\n}\n\n\n1\n\npath to the ecr module\n\n2\n\npath of the lambda_function.py\n\n3\n\npath of dockerfile to create docker image\n\n\nInside the module ecr, we create a main.tf and varible.tf. Variables passed into the module and also ones newly used inside it have to be defined inside variable.tf.\n\nBuilding the docker container image and uploading to ECR\nUsually docker container image building is part of the CI/CD pipeline but since the lambda function requires us to have the image, we build it locally and upload using Terraform’s local-exec provisioner. However, Terraform advises caution with the use of provisioners. Read more on that here.\n\n\ninfrastructure/modules/ecr/main.tf\n\n1resource \"null_resource\" \"ecr_image\" {\n  triggers = {\n    \"python_file\" = md5(file(var.lambda_function_local_path))\n    \"docker_file\" = md5(file(var.docker_image_local_path))\n  }\n\n2  provisioner \"local-exec\" {\n    command = &lt;&lt;EOF\n            aws ecr get-login-password --region ${var.ecr_region} | docker login --username AWS --password-stdin ${var.account_id}.dkr.ecr.${var.ecr_region}.amazonaws.com\n            cd ${path.module}/../..\n            docker build -t ${aws_ecr_repository.repo.repository_url}:${var.ecr_image_tag} .\n            docker push ${aws_ecr_repository.repo.repository_url}:${var.ecr_image_tag}\n        EOF\n  }\n}\n\n\n1\n\nA null_resource block is a feature of Terraform’s. With a help of triggers meta-argument, we can observe any change to lambda_function or dockerfile.\n\n2\n\nWhen there is a change, a trigger condition is active and local-exec is executed. The image is built and uploaded."
  },
  {
    "objectID": "posts/2023-06-11-terraform-aws/index.html#module-block-2---lambda-function",
    "href": "posts/2023-06-11-terraform-aws/index.html#module-block-2---lambda-function",
    "title": "Building Data Pipelines - Part 3 - Terraform",
    "section": "Module block 2 - Lambda Function",
    "text": "Module block 2 - Lambda Function\nOur ECR image is ready to be used as source for Lambda Function. With a depends_on meta-argument, this condition is ensured.\nOur lambda_function inside the container image requires three environment variables: artifact_bucket, run_id, dbtable_name. These variables are passed into the lamda function module as arguments.\n\n\ninfrastructure/modules/lambda/main.tf\n\nresource \"aws_lambda_function\" \"lambda_function\" {\n  function_name = var.lambda_function_name\n  description = \"Sales Forecast lambda function from ECR image from TF\"\n  image_uri = var.image_uri \n  package_type = \"Image\"\n1  role = aws_iam_role.lambda_exec.arn\n  tracing_config {\n    mode = \"Active\"\n  }\n  memory_size = 1024\n  timeout = 30\n2  environment {\n    variables = {\n      S3_BUCKET_NAME = var.artifact_bucket \n      RUN_ID = var.mlflow_run_id \n      DBTABLE_NAME = var.dbtable_name\n    }\n  }\n}\n\n3resource \"aws_cloudwatch_log_group\" \"lambda_log_group\" {\n  name = \"/aws/lambda/${aws_lambda_function.lambda_function.function_name}\"\n  retention_in_days = 30\n}\n\n\n1\n\nIAM Role attached to the Lambda function.\n\n2\n\nEnvironment variables for the lambda function to predict sales.\n\n3\n\nSetting Cloudwatch logs retention period.\n\n\n\nIAM Roles and Polices\nThe AWS Lambda function is the business layer of our app. It plays a crucial role in predicting the sales output. Therefore it needs access to retrieve the trained model from the artifact_bucket and store the predicted results in the DynamoDB table. These operations are only possible if we give AWS Lambda function permission.\nAn IAM role lambda_exec is created.\n\n\nIAM Role-&gt;lambda_exec\n\nresource \"aws_iam_role\" \"lambda_exec\" {\n    name = \"iam_${var.lambda_function_name}\"\n    assume_role_policy = jsonencode({\n        \"Version\": \"2012-10-17\",\n        \"Statement\": [{\n                \"Action\": \"sts:AssumeRole\",\n                \"Principal\": {\n1                    \"Service\": \"lambda.amazonaws.com\"\n                },\n                \"Effect\": \"Allow\",\n                \"Sid\":\"\"\n          }]\n  }) \n}\n\n\n1\n\nRole just for lambda function service\n\n\nTo this roles several policies are added. We need three policies for - Basic lambda execution, Access S3 artifact bucket, Put items into DynamoDB table. These three policies are defined and attached using aws_iam_role_policy_attachment resource block."
  },
  {
    "objectID": "posts/2023-06-11-terraform-aws/index.html#module-block-3---dynamodb",
    "href": "posts/2023-06-11-terraform-aws/index.html#module-block-3---dynamodb",
    "title": "Building Data Pipelines - Part 3 - Terraform",
    "section": "Module block 3 - Dynamodb",
    "text": "Module block 3 - Dynamodb\nSimilar to the previous two blocks, dynamodb module is called with necessary arguments.\nresource \"aws_dynamodb_table\" \"sales_preds_table_fromtf\" {\n  name = var.dynamodb_tablename\n1  billing_mode = \"PAY_PER_REQUEST\"\n  table_class  = \"STANDARD_INFREQUENT_ACCESS\"\n2  hash_key = var.dynamodb_hashkey\n3  range_key = var.dynamodb_rangekey\n  \n  attribute {\n    name = var.dynamodb_hash_key\n    type = \"N\"\n  }\n\n  attribute {\n    name = var.dynamodb_range_key\n    type = \"N\"\n  }\n}\n\n1\n\nBilling mode is set as “On-Demand”\n\n2\n\nHash key is the Partition Key\n\n3\n\nRange Key is the Sort Key"
  },
  {
    "objectID": "posts/2023-06-11-terraform-aws/index.html#module-block-4---api-gateway",
    "href": "posts/2023-06-11-terraform-aws/index.html#module-block-4---api-gateway",
    "title": "Building Data Pipelines - Part 3 - Terraform",
    "section": "Module block 4 - API Gateway",
    "text": "Module block 4 - API Gateway\nAPI Gateway management with Terraform follows all the steps we need manually in the console. The steps are -\n(1). Create rest api with resource aws_api_gateway_rest_api\n(2). Create gateway resource with aws_api_gateway_resource and give the endpoint path as predict_sales.\n(3). Define the gateway method with rest_api_post_method as POST.\n(4). Setup POST method’s reponse upon succesfull execution with a code 200.\n(5). Integrate and deploy the gateway with aws_api_gateway_integration and aws_api_gateway_deployment respectively.\n(6). Stage the deployment with rest_api_stage and get the invoke_url. For this we can use the output block.\noutput \"rest_api_url\" {\n  value = \"${aws_api_gateway_deployment.sales_pred_deployment.invoke_url}${aws_api_gateway_stage.rest_api_stage.stage_name}${aws_api_gateway_resource.rest_api_predict_resource.path}\"\n}\n(7). Define IAM policy for rest api with aws_api_gateway_rest_api_policy and give access to API gateway to invoke the lambda function with aws_lambda_permission.\n\nVariables\nIn Terraform we can pass variable values in many ways. One of those ways is through .tfvars file. This file is exclusively for variables. These variable files are extremely helpful when we need different variable names for development, staging and production. The syntax for supplying the file is like so: -var-file vars/stg.tfvars."
  },
  {
    "objectID": "posts/2023-06-11-terraform-aws/index.html#initialise-backend",
    "href": "posts/2023-06-11-terraform-aws/index.html#initialise-backend",
    "title": "Building Data Pipelines - Part 3 - Terraform",
    "section": "Initialise backend",
    "text": "Initialise backend\nterraform init initialises the Terraform backend, checks where the state file has to be saved and if it is remote, availability of the bucket is validated, installs all the provider plugins."
  },
  {
    "objectID": "posts/2023-06-11-terraform-aws/index.html#plan-and-apply-changes",
    "href": "posts/2023-06-11-terraform-aws/index.html#plan-and-apply-changes",
    "title": "Building Data Pipelines - Part 3 - Terraform",
    "section": "Plan and Apply Changes",
    "text": "Plan and Apply Changes\nWith terraform plan -var-file vars/stg.tfvars we can see what new resources be created/changed/destroyed. This gives as a plan and confirmation of our setup.\nterraform apply -var-file vars/stg.tfvars will apply our configurations. At the end the rest_api_url will be displayed. \nTake that, put it in an api client and supplying our sample JSON input {\"find\": {\"date1\": \"2017-08-28\", \"store_nbr\": 19}}. You should see the status code as 200 and the body containing the predictions with a confirmation saying the item has been successfully created.\n\nWe can confirm it by going to Dynamodb console and checking the items in the table"
  },
  {
    "objectID": "posts/2023-06-11-terraform-aws/index.html#destroying-resources",
    "href": "posts/2023-06-11-terraform-aws/index.html#destroying-resources",
    "title": "Building Data Pipelines - Part 3 - Terraform",
    "section": "Destroying resources",
    "text": "Destroying resources\nUpon completion of the task it is always good practice to destroy the resources to avoid incurring unnecessary costs.\nUse terraform destroy -var-file vars/stg.tfvars to destroy the resources and leave it as we started. Remember in real production environment, destroy command should never be used. Instead delete the resources that are unnecesary and run apply command again."
  },
  {
    "objectID": "posts/2023-06-04-api-lambda-ecr/index.html",
    "href": "posts/2023-06-04-api-lambda-ecr/index.html",
    "title": "Building Data Pipelines - Part 2 - AWS Cloud",
    "section": "",
    "text": "In part 1, we saw how to make and test lambda function locally. In part 2, we will switch focus to AWS cloud provider. We will migrate our code to use AWS cloud platform.\n\n\nWe will use these services to form our data pipeline:\n\nS3 - to download our predicted sales price table.\nECR - to upload our docker image with installed dependencies.\nAWS Lambda - to make the business logic of the web application. It will use ECR image as source. Sends predicted outputs to DynamoDB.\nAPI Gateway - Rest API to invoke AWS Lambda function.\nDynamoDB - to store the predicted sales prices. Invoked from Lambda."
  },
  {
    "objectID": "posts/2023-06-04-api-lambda-ecr/index.html#services-used",
    "href": "posts/2023-06-04-api-lambda-ecr/index.html#services-used",
    "title": "Building Data Pipelines - Part 2 - AWS Cloud",
    "section": "",
    "text": "We will use these services to form our data pipeline:\n\nS3 - to download our predicted sales price table.\nECR - to upload our docker image with installed dependencies.\nAWS Lambda - to make the business logic of the web application. It will use ECR image as source. Sends predicted outputs to DynamoDB.\nAPI Gateway - Rest API to invoke AWS Lambda function.\nDynamoDB - to store the predicted sales prices. Invoked from Lambda."
  },
  {
    "objectID": "posts/2023-06-04-api-lambda-ecr/index.html#lambda-function-to-predict-sales",
    "href": "posts/2023-06-04-api-lambda-ecr/index.html#lambda-function-to-predict-sales",
    "title": "Building Data Pipelines - Part 2 - AWS Cloud",
    "section": "Lambda function to predict sales",
    "text": "Lambda function to predict sales\nTo build this pipeline we will start with our lambda_function using MLflow to download ML trained artifact from S3 bucket to predict sales prices. We will first check if it is possible to connect to S3 and then move forward to complete cloud solution.\n\n\nlambda_function.py\n\nimport os\n\nimport mlflow\nimport pandas as pd\n\nRUN_ID = os.getenv(\"RUN_ID\")  # \"5651db4644334361b10296c51ba3af3e\"\nS3_BUCKET_NAME = os.getenv(\n    \"S3_BUCKET_NAME\"\n)  # \"mlops-project-sales-forecast-bucket\"\nEXPERIMENT_ID = 1\nFILE_ADDRESS = \"artifacts/predictions/lgb_preds.parquet\"\npred_s3_location = (\n    f\"s3://{S3_BUCKET_NAME}/{EXPERIMENT_ID}/{RUN_ID}/{FILE_ADDRESS}\"\n)\n\n\ndef read_parquet_files(filename: str):\n    \"\"\"\n    Read parquet file format for given filename and returns the contents\n    \"\"\"\n    df = pd.read_parquet(filename, engine=\"pyarrow\")\n    return df\n\n\nif os.path.exists(\"lgb_preds.parquet\"):\n    df_test_preds = read_parquet_files(\"lgb_preds.parquet\")\nelse:\n    s3_file = mlflow.artifacts.download_artifacts(\n        artifact_uri=pred_s3_location, dst_path=\"/tmp\"\n    )  # /tmp is added as lambda gives write access only to that folder. Otherwise use \"./\" .\n    df_test_preds = read_parquet_files(s3_file)\n\n\ndf_items = read_parquet_files(\"items.parquet\")\n\n\ndef predict(find, item_idx: int):\n    \"\"\"\n    Takes the json inputs, processes it and outputs the unit sales\n    \"\"\"\n    try:\n        idx = pd.IndexSlice\n        # df_items.sample(1).index[0]\n        x = df_test_preds.loc[idx[find[\"store_nbr\"], item_idx, find[\"date1\"]]][\n            \"unit_sales\"\n        ]\n    except KeyError:\n        print(\"This item is not present this store. Try some other item\")\n        return -0.0\n    else:\n        return float(round(x, 2))\n\n\ndef lambda_handler(event, context=None) -&gt; dict:\n    \"\"\"\n    lambda handler for predict method\n    \"\"\"\n\n    find = event[\"find\"]\n    item = df_items.sample(1)\n    item_idx, item_family = item.index[0], item[\"family\"].values[0]\n    pred_unit_sales = predict(find, item_idx)\n\n    result = {\n        \" Store\": find[\"store_nbr\"],\n        \" item\": int(item_idx),\n        \"Family\": item_family,\n        \"Prediction date\": find[\"date1\"],\n        \"Unit_sales\": pred_unit_sales,\n    }\n    return result"
  },
  {
    "objectID": "posts/2023-06-04-api-lambda-ecr/index.html#building-container-image",
    "href": "posts/2023-06-04-api-lambda-ecr/index.html#building-container-image",
    "title": "Building Data Pipelines - Part 2 - AWS Cloud",
    "section": "Building container image",
    "text": "Building container image\nBuilding the docker image is same as previous post.\n\n\nTerminal 1\n\ndocker build -t lambda-app:v1 .\n\n\nUsing sensitive/secret access keys.\nSince we are using MLflow’s download artifact functionality directly from the S3, we need to supply our AWS access key to the docker as environment variables. Care needs to be taken when supplying sensitive information. Most hard code these secrets in their code. This is a bad practice as it will make its way to the code repository eventually. Next possible option is to supply as environment variable in the terminal. However, a simple history command will reveal the secrets. So, to avoid that we can set HIST_IGNORE_SPACE and also add keywords to HISTIGNORE ensuring any command with a leading space in front of it will not be stored in history cache.\n\n\nTerminal 1\n\n1set +o history\nexport HISTIGNORE= \\\n2        \"ls*:cat*:*AWS*:*SECRET*:*KEY*:*PASS*:*TOKEN*\"\n export AWS_ACCESS_KEY_ID=xxxx\n export AWS_SECRET_ACCESS_KEY=xxxtt\n\n\n1\n\nDisables history for the current terminal session.\n\n2\n\nHistory doesn’t store any command that has leading space. Highly useful for sensitive environment variables.\n\n\n\n\nRunning the container and testing locally\nUnlike last time to run the docker image as container requires a few arguments. We give them to docker as -e environment variables. Setting environment variables in the terminal is not going to pass over to the docker container. So we need to explicitly supply AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, RUN_ID and S3_BUCKET_NAME while executing docker run command in addition to the regular arguments.\n\n\nTerminal 1\n\ndocker run \\\n    -it --rm \\\n    -p 9000:8080 \\\n    -e AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID} \\\n    -e AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY} \\\n    -e RUN_ID=5651db4644334361b10296c51ba3af3e \\\n    -e S3_BUCKET_NAME=mlops-project-sales-forecast-bucket \\\n    lambda-app:v1\n\nTo test run the command in terminal 2,\n\n\ntest_docker_fn_docker.py in Terminal 2\n\npython test_docker_fn_docker.py\n\nWe will get a prediction as we have done in the previous post."
  },
  {
    "objectID": "posts/2023-06-04-api-lambda-ecr/index.html#uploading-container-image-to-ecr",
    "href": "posts/2023-06-04-api-lambda-ecr/index.html#uploading-container-image-to-ecr",
    "title": "Building Data Pipelines - Part 2 - AWS Cloud",
    "section": "Uploading container image to ECR",
    "text": "Uploading container image to ECR\nFor this section we need awscli package. If don’t have I recommend installing it. sudo apt install awscli. AWS-CLI allows for swifter creation of resources without overly relying on the console.\nWe need our ACCOUNT_ID. It can be found in the AWS console.\n\n\nTerminal\n\n1docker build -t lambda-app:v1 .\n2export ACCOUNT_ID=xxxx\naws ecr create-repository --repository-name lambda-images\ndocker tag lambda-app:v1 ${ACCOUNT_ID}.dkr.ecr.us-east-1.amazonaws.com/lambda-images:app\n3$(aws ecr get-login --no-include-email)\ndocker push ${ACCOUNT_ID}.dkr.ecr.us-east-1.amazonaws.com/lambda-images:app\n\n\n1\n\nOptional as we already built the image.\n\n2\n\nReplace with your account ID.\n\n3\n\nRemember we supplied aws access key and secret before. The session borrows them for ECR login. If it is a new session, those variables have to be given again.\n\n\n  In the screenshots we can see that our container image is uploaded to the registry."
  },
  {
    "objectID": "posts/2023-06-04-api-lambda-ecr/index.html#setup-aws-lambda-function",
    "href": "posts/2023-06-04-api-lambda-ecr/index.html#setup-aws-lambda-function",
    "title": "Building Data Pipelines - Part 2 - AWS Cloud",
    "section": "Setup AWS Lambda function",
    "text": "Setup AWS Lambda function\nThis part is a bit tedious as it is not as straight forward as running commands. I’d like to point out to a detailed guide which has all the steps to create a AWS Lambda function, attach policies, configure and supply environment variable. I urge you to visit that link. Including them here will make it too long, full of screenshots and frankly boring.\n\nTesting the Lambda function\nIn the Test tab, create a test event with our good old sample JSON object. {\"find\":{\"date1\":\"2017-08-26\",\"store_nbr\":20}}.   Hitting the Test button should give us a prediction. as seen in the second image.\nSo our function is able to access S3 artifact bucket to download and give out predictions. Meaning our attached policies are correct. Testing once in the console is fine but the customer wouldn’t want to do that. We need something that triggers the lambda function - we need an API Gateway."
  },
  {
    "objectID": "posts/2023-06-04-api-lambda-ecr/index.html#aws-api-gateway",
    "href": "posts/2023-06-04-api-lambda-ecr/index.html#aws-api-gateway",
    "title": "Building Data Pipelines - Part 2 - AWS Cloud",
    "section": "AWS API Gateway",
    "text": "AWS API Gateway\nA front-end developer, for example, can create a button that calls on the Lambda function. From customer’s viewpoint it is as simple as a click of a button. For us though, it is much more than that.\n\nCreate Rest API, test and deploy it\nAgain I advise you to refer my other guide to create an endpoint. This endpoint is same as the Flask one we encountered in the last posts.\nUpon deploying we get an invoke url. This url is without an endpoint. \nWe need to append predict-sales, our resource endpoint, to the url. The final url would look like https://eecweeeeeg.execute-api.us-east-1.amazonaws.com/stg-lambda-app-for-blog/predict-sales\nUse the following JSON object to test it out in an API platform client such as Thunder Client in VSCode, Postman etc.\n{\"find\": {\"date1\": \"2017-08-26\", \"store_nbr\": 20}}"
  },
  {
    "objectID": "posts/2023-06-04-api-lambda-ecr/index.html#aws-dynamodb",
    "href": "posts/2023-06-04-api-lambda-ecr/index.html#aws-dynamodb",
    "title": "Building Data Pipelines - Part 2 - AWS Cloud",
    "section": "AWS DynamoDB",
    "text": "AWS DynamoDB\nWe want to store these predicted outputs to a DB. With the help of console, create a table sales_predictions. Give store_id as partition_key and item_id as sort_key. Rest of the setting can be default.\nThen add policy that give Lambda permission to write objects into DB. Refer my other notes"
  },
  {
    "objectID": "posts/2023-06-04-api-lambda-ecr/index.html#modify-aws-lambda",
    "href": "posts/2023-06-04-api-lambda-ecr/index.html#modify-aws-lambda",
    "title": "Building Data Pipelines - Part 2 - AWS Cloud",
    "section": "Modify AWS Lambda",
    "text": "Modify AWS Lambda\n\n\nlambda_function.py\n\nimport os\n\nimport mlflow\nimport pandas as pd\nimport boto3\nimport json\n\nRUN_ID = os.getenv(\"RUN_ID\")  # \"5651db4644334361b10296c51ba3af3e\"\nS3_BUCKET_NAME = os.getenv(\n    \"S3_BUCKET_NAME\"\n)  # \"mlops-project-sales-forecast-bucket\"\nEXPERIMENT_ID = 1\nFILE_ADDRESS = \"artifacts/predictions/lgb_preds.parquet\"\npred_s3_location = (\n    f\"s3://{S3_BUCKET_NAME}/{EXPERIMENT_ID}/{RUN_ID}/{FILE_ADDRESS}\"\n)\n\n# Initialize the SNS client object outside of the handler\n1dynamodb = boto3.resource('dynamodb')\n\ndef read_parquet_files(filename: str):\n    \"\"\"\n    Read parquet file format for given filename and returns the contents\n    \"\"\"\n    df = pd.read_parquet(filename, engine=\"pyarrow\")\n    return df\n\n\nif os.path.exists(\"lgb_preds.parquet\"):\n    df_test_preds = read_parquet_files(\"lgb_preds.parquet\")\nelse:\n    s3_file = mlflow.artifacts.download_artifacts(\n        artifact_uri=pred_s3_location, dst_path=\"/tmp\"\n    )  # /tmp is added as lambda gives write access only to that folder. Otherwise use \"./\" .\n    df_test_preds = read_parquet_files(s3_file)\n\n\ndf_items = read_parquet_files(\"items.parquet\")\n\n\ndef predict(find, item_idx: int):\n    \"\"\"\n    Takes the json inputs, processes it and outputs the unit sales\n    \"\"\"\n    try:\n        idx = pd.IndexSlice\n        # df_items.sample(1).index[0]\n        x = df_test_preds.loc[idx[find[\"store_nbr\"], item_idx, find[\"date1\"]]][\n            \"unit_sales\"\n        ]\n    except KeyError:\n        print(\"This item is not present this store. Try some other item\")\n        return -0.0\n    else:\n        return float(round(x, 2))\n\n\ndef lambda_handler(event, context=None) -&gt; dict:\n    \"\"\"\n    lambda handler for predict method\n    \"\"\"\n\n    find = event[\"find\"]\n    item = df_items.sample(1)\n    item_idx, item_family = item.index[0], item[\"family\"].values[0]\n    pred_unit_sales = predict(find, item_idx)\n\n    result = {\n        \"store_id\": find[\"store_nbr\"],\n        \"item_id\": int(item_idx),\n        \"family\": item_family,\n        \"prediction_date\": find[\"date1\"],\n        \"unit_sales\": str(pred_unit_sales),\n    }\n\n2    table = dynamodb.Table(\"sales_preds_for_blog\")\n    table.put_item(\n3        Item = result\n    )\n    return {\n      'statusCode': 200,\n      'body': 'successfully created item!',\n      }\n\n\n1\n\nCreate dynamodb resource\n\n2\n\nAccess already created table sales_preds_for_blog\n\n3\n\nInsert item into the table\n\n\nRebuild the docker image, retag it, publish it to the ECR repo. Then in AWS Lambda, use deploy new image to select the latest build. Test with sample input."
  },
  {
    "objectID": "posts/2023-02-06-skim-vimtex/index.html",
    "href": "posts/2023-02-06-skim-vimtex/index.html",
    "title": "Setup Skim PDF reader with VimTeX in Mac OS",
    "section": "",
    "text": "VimTeX plugin written by Karl Yngve Lervåg is one of the goto plugins to manage LaTeX files with Vim/Neovim text editors. VimTeX allows integration with several PDF viewers. In Mac OS, Skim and Zathura PDF readers allow easy integration with LaTeX. Since Zathura’s installation in Mac OS involves more steps, we will be using Skim for this post."
  },
  {
    "objectID": "posts/2023-02-06-skim-vimtex/index.html#install-skim",
    "href": "posts/2023-02-06-skim-vimtex/index.html#install-skim",
    "title": "Setup Skim PDF reader with VimTeX in Mac OS",
    "section": "Install Skim",
    "text": "Install Skim\nWith Homebrew\n\n\nTerminal\n\nbrew install --cask skim\n\nOr download the dmg file of the current version(as of writing latest version is v1.6.8) from Skim’s website."
  },
  {
    "objectID": "posts/2023-02-06-skim-vimtex/index.html#install-vimtex",
    "href": "posts/2023-02-06-skim-vimtex/index.html#install-vimtex",
    "title": "Setup Skim PDF reader with VimTeX in Mac OS",
    "section": "Install VimTeX",
    "text": "Install VimTeX\nUsing vim-plug plugin manager we add the following line to .vimrc or init.vim or init.lua\n\n\nInside init.vim\n\nPlug 'lervag/vimtex'"
  },
  {
    "objectID": "posts/2023-02-06-skim-vimtex/index.html#pdf-preview",
    "href": "posts/2023-02-06-skim-vimtex/index.html#pdf-preview",
    "title": "Setup Skim PDF reader with VimTeX in Mac OS",
    "section": "Pdf preview",
    "text": "Pdf preview\nConversion between TeX and PDF is one of the most common operations while writing a scientific document. Though it is possible to open the PDF file in one of the commercially available PDF readers, a seamless integration with neovim(in our case) is appreciated. This is where Skim comes into the picture. By default, Skim allows native, seamless integration with the LaTex editor of choice. In our case, we can make VimTex interact with Skim with just a few lines of config."
  },
  {
    "objectID": "posts/2023-02-06-skim-vimtex/index.html#configurations",
    "href": "posts/2023-02-06-skim-vimtex/index.html#configurations",
    "title": "Setup Skim PDF reader with VimTeX in Mac OS",
    "section": "Configurations",
    "text": "Configurations\n\nMinimal setup and Forward Search\nWe require the following lines to make VimTeX talk to Skim within neovim. This direction of communication, is known as forward search.\n\n\nInside init.vim\n\n# Hover over the number at the end of each line to see its importance\n1let g:vimtex_view_method = 'skim'\n\n\n2let g:vimtex_view_skim_sync = 1\n3let g:vimtex_view_skim_activate = 1\n\n\n1\n\nChoose which program to use to view PDF file.\n\n2\n\nValue 1 allows forward search after every successful compilation.\n\n3\n\nValue 1 allows change focus to skim after command :VimtexView is given.\n\n\nThe forward search allows any change made in the TeX file automatically refreshes Skim to reflect those changes in PDF. One of the other common uses is cursor sync between the TeX file and PDF. Setting let g:vimtex_view_skim_sync allows placing the cursor in some position in the Tex file sync with the same position in the PDF after every successful compilation(:VimtexCompile). Setting let g:vimtex_view_skim_activate allows to shift focus of control from neovim to Skim and bring it to foreground.\n\n\nInverse or Backward Search\nSo far there was only one channel of communication between neovim(editor) and Skim. A backward communication is possible but it took quite bit of hacking to get it to work. More on that read this jdhao’s post. However, with the release of VimTex v2.8, it has become simple to setup.\nConsider a scenario where we are going through a paper and find an error, instead of going back to source TeX file and finding the error location can be cumbersome. Using backward search, we can go to the error location from PDF to TeX. For Skim, to activate backward search press shift and command together and click the position in PDF using the mouse. That location gets reflected in the editor in the background. For more information, see :h :VimtexInverseSearch\nNatively, every instance of neovim starts a server 1. With Skim as client and nvim as server, we can interact in that direction.\nIn order to do so, in the preferances pane of Skim, navigate to Sync tab. There, in the PDF-TeX Sync support, make preset as custom, command as nvim(use vim if you use vim editor), and set arguments as --headless -c \"VimtexInverseSearch %line '%file'\".\n\n\n\n\n\n\n\nImportant\n\n\n\nSkim must be started by VimTeX (either through compiler callback or explicitly via lv) for backward sync to work! (This is how Skim “knows” which neovim instance – terminal or GUI – to sync to.)"
  },
  {
    "objectID": "posts/2023-02-06-skim-vimtex/index.html#conclusion",
    "href": "posts/2023-02-06-skim-vimtex/index.html#conclusion",
    "title": "Setup Skim PDF reader with VimTeX in Mac OS",
    "section": "Conclusion",
    "text": "Conclusion\nWith just four lines of settings in the init.vim file and a line in Skim preferances, we can activate both forward and backward search features with VimTeX."
  },
  {
    "objectID": "posts/2023-02-06-skim-vimtex/index.html#footnotes",
    "href": "posts/2023-02-06-skim-vimtex/index.html#footnotes",
    "title": "Setup Skim PDF reader with VimTeX in Mac OS",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn the nvim command line, run :echo v:servername to know the name of the server↩︎"
  },
  {
    "objectID": "posts/2022-01-09-dotfiles/index.html",
    "href": "posts/2022-01-09-dotfiles/index.html",
    "title": "Manage dotfiles with GNU Stow",
    "section": "",
    "text": "In this post, I will try to guide in organise your dotfiles in the cloud and manage them using GNU Stow."
  },
  {
    "objectID": "posts/2022-01-09-dotfiles/index.html#what-are-dotfiles",
    "href": "posts/2022-01-09-dotfiles/index.html#what-are-dotfiles",
    "title": "Manage dotfiles with GNU Stow",
    "section": "What are dotfiles?",
    "text": "What are dotfiles?\nFor a casual user, the term dotfiles may sound strange and confusing but it is nothing but application(app) configuration files in developer talk. The apps generally refer to certain files to configure itself.\nPeople usually store these files in a remote location such as a Github repository and retrieve them when needed.\nDotfiles allow personalisation. They can be restored in a new machine saving time. Preparing and organising the dotfiles with some initial effort, help developers save a lot of time later.\nA few examples of dotfiles are .bashrc, .vimrc, .gitignore.\n\n\n\n\n\n\nImportant\n\n\n\nPay attention to personal information inside these files. Never store secure keys, passwords in public domains.\n\n\n\n\nThings to know\n\nWhich app’s config files need to stored.\nWhere do those config files are located.\n\n\n\nCommon config files that need storing\n\n.bashrc or .zshrc\n.vimrc or init.vim(in the case of neovim)\n.gitignore_global and .gitconfig\nTerminal emulator config files\nIDE of choice config files\nAnyother config you want to save\n\nIn fact, if there is an app that you have configured heavily and frequently use, its config files must be stored. In the case the said app doesn’t allow exporting of configurations, it is highly recommended to move onto one that allows it.\n\n\nWhere are most required dotfiles located?\nMost files are present in $HOME or $XDG_CONFIG_HOME directories. $XDG_CONFIG_HOME defines the base directory relative to which user-specific configuration files should be stored. If $XDG_CONFIG_HOME is either not set or empty, a default equal to $HOME/.config should be used."
  },
  {
    "objectID": "posts/2022-01-09-dotfiles/index.html#gnu-stow",
    "href": "posts/2022-01-09-dotfiles/index.html#gnu-stow",
    "title": "Manage dotfiles with GNU Stow",
    "section": "GNU Stow",
    "text": "GNU Stow\nSome prominent results when googled for storing dotfiles are this Atlassian tutorial and using yadm. However, I found those harder to get started.\nGNU Stow on the other hand is an easy-to-use symlink farm manager. As described in their website, it takes distinct packages of software and/or data located in separate directories on the filesystem, and makes them appear to be installed in the same place.\nThis strategy works brilliantly for dotfiles. Borrowing explanation from Brandon Invergo’s article:\n\nThe procedure is simple. I created the ${HOME}/dotfiles directory and then inside it I made subdirectories for all the programs whose configurations I wanted to manage. Inside each of those directories, I moved in all the appropriate files, maintaining the directory structure of my home directory. So, if a file normally resides at the top level of your home directory, it would go into the top level of the program’s subdirectory. If a file normally goes in the default ${XDG_CONFIG_HOME}/${PKGNAME} location (${HOME}/.config/${PKGNAME}), then it would instead go in ${HOME}/dotfiles/${PKGNAME}/.config/${PKGNAME} and so on.\n\n\nInstall Stow\n\n\nTerminal\n\n1sudo apt stow\n\n2brew install stow\n\n\n1\n\nUbuntu\n\n2\n\nHomebrew Mac\n\n\n\n\nPlacing the files\nNow, it might look complex at first. Let me explain with some examples. - .bashrc or .zshrc are present/needed in $HOME directory, so inside $HOME/dotfiles create a subdirectory with bashrc or zshrc and place the original .bashrc or .zshrc file appropriately inside their folder. GNU Stow understands that the dotfile, when symlinked, will create a symlink-copy in the $HOME directory. For future modifications, file in either locations can be edited. But for simplicity, use $HOME/dotfiles directory. - A complicated example would be a config file located deep inside subfolders: nvim’s or neovim’s init.vim or init.lua file. It is present in $HOME/.config/nvim/init.vim. For Stow to understand, it must be placed like this – $HOME/dotfiles/nvim/.config/nvim/init.vim\nFor further reading, I recommend brilliantly written Jake Weisler’s post on GNU Stow.\n\n\nUseful Stow commands\nIf correctly installed, then running the command stow --help should list options to use Stow. Most used commands are\n\n\nTerminal\n\n1stow &lt;packagename&gt;\n2stow -n &lt;packagename&gt;\n3stow -D &lt;packagename&gt;\n4stow -R &lt;packagename&gt;\n\n\n1\n\nactivates symlink\n\n2\n\ntrial runs or simulates symlink generation. Effective for checking for errors\n\n3\n\ndelete stowed package\n\n4\n\nrestows package\n\n\n\n\nActivating Stow\nSo if we have created three subdirectories inside dotfiles say zsh, git, nvim, then\n\n\nTerminal\n\nstow bash git nvim\n\nwill activate their symlinks.\nIf returned to $HOME and $XDG_CONFIG_HOME and verified, then we will see,\n\n\nTerminal\n\n.gitconfig -&gt; .dotfiles/git/.gitconfig\n.zshrc -&gt; .dotfiles/zsh/.zshrc\nnvim -&gt; ../.dotfiles/nvim/.config/nvim\n\nThe most awesome thing in all this is, the directory structure needs to be created only once. For future requirement, one simply clones the dotfiles directory and activates symlinks."
  },
  {
    "objectID": "posts/2022-01-09-dotfiles/index.html#storing-files-in-git",
    "href": "posts/2022-01-09-dotfiles/index.html#storing-files-in-git",
    "title": "Manage dotfiles with GNU Stow",
    "section": "Storing files in Git",
    "text": "Storing files in Git\nThe dotfiles directory now becomes important to store in a remote location for safe keeping. Usually a git repository is the preferred method. For instructions on how to use git, look up various tutorials on Git in the internet.\nIn summary, I have written a short, albeit technical write up on GNU Stow, and its uses for storing dotfiles. Feel free to ask questions in the comments or via various means linked in the blog."
  },
  {
    "objectID": "posts/2023-06-02-Terraform-setup/index.html",
    "href": "posts/2023-06-02-Terraform-setup/index.html",
    "title": "Getting started with Terraform",
    "section": "",
    "text": "Terraform from Hasicorp is Infrastructure as Code(IaC) tool that is gaining popularity among the data engineers. It lets us build, change and version infrastructure safely and efficiently. As to why one needs Terraform, I shall direct you to this comprehensive article.\nIn this post, I will write a small guide to get us started with Terraform. This post then can be used as a starting guide for my future posts involving Terraform.\n\n\n\n\n\n\nNote\n\n\n\nNote that this installation guide is only for Mac(M1 silicon) and Ubuntu(GCP E2-medium instance). Also this guide will look similar to the example provided in Hashicorp’s website. The intention is to have a guide for my benefit in one place.\n\n\n\n\nA fully functional Macbook or an Ubuntu GCP/AWS instance with sudo privileges."
  },
  {
    "objectID": "posts/2023-06-02-Terraform-setup/index.html#what-is-main.tf",
    "href": "posts/2023-06-02-Terraform-setup/index.html#what-is-main.tf",
    "title": "Getting started with Terraform",
    "section": "What is main.tf?",
    "text": "What is main.tf?\nIt is a terraform configuration file that describes infrastructure in Terraform. Each terraform config needs a separate directory with a single main.tf.\nThis file contains:\n\nTerraform block\nProviders\nResources\n\n\nTerraform block\nThis block contains Terraform settings for require providers that Terraform will use to provision infrastructure. In our case it is kreuzwerker/docker as source. kreuzwereker is the provider/developer/host and docker is the product we are interested in using. We can specify the minimum version to install. If not mentioned, Terraform will install the lastest version available. Terraform registry is default place to look for popular providers such as AWS, GCP, Azure.\nterraform {\n  required_providers {\n    docker = {\n      source  = \"kreuzwerker/docker\"\n      version = \"~&gt; 3.0.1\"\n    }\n  }\n}\n\n\nProviders\nNow that Terraform knows the “source”, we need to provide a “provider”. Docker is our choice. In this block we can configure docker provider.\nprovider \"docker\" {}\n\n\nResources\nA resource block is used to define components of the provider previously mentioned. So we have a docker provider. We need a docker image and build a container for that image.\nresource \"docker_image\" \"nginx\" {\n  name         = \"nginx\"\n  keep_locally = false\n}\nresource \"docker_container\" \"nginx\" {\n  image = docker_image.nginx.image_id\n  name  = \"tutorial\"\n  ports {\n    internal = 80\n    external = 8000\n  }\n}\nThe resource block takes two strings - resource type and resource name. docker_image is the type and nginx is the name. These two together docker_image.nginx form a unique ID for the resource. For docker_container resource, we use the image from the previous block as reference. So a container is created with that image and ports are also mapped."
  },
  {
    "objectID": "posts/2023-06-02-Terraform-setup/index.html#validate-and-initialise-the-configuration",
    "href": "posts/2023-06-02-Terraform-setup/index.html#validate-and-initialise-the-configuration",
    "title": "Getting started with Terraform",
    "section": "Validate and initialise the configuration",
    "text": "Validate and initialise the configuration\nTerraform has a bunch of CLI commands to make things easier and faster. One needs to remember only 5-6 commands. Rest can be looked up later.\nTo validate the configuration we use\n\n\nTerminal\n\n1terrform validate\n\n\n1\n\nReturns “Success! The configuration is valid.” if syntactically valid.\n\n\nThen we need to initialise our configuration. This process will download and install the providers mentioned. This command will create a hidden directory .terraform and install docker provider. Terraform also creates a lock file named .terraform.lock.hcl which specifies the exact provider versions used, so that you can control when you want to update the providers used for your project."
  },
  {
    "objectID": "posts/2023-06-02-Terraform-setup/index.html#plan-and-create-infrastructure",
    "href": "posts/2023-06-02-Terraform-setup/index.html#plan-and-create-infrastructure",
    "title": "Getting started with Terraform",
    "section": "Plan and create infrastructure",
    "text": "Plan and create infrastructure\nTerraform allows us to see what will be created through terraform plan commands. This command is used to give an overview of the things that will be created. An extra, precautionary step. It looks trivial for our use case but imagine a big organisation with several team members managing infrastructure. Even deleting a resource accidently will create chaos. It is always advised to run terraform plan, overview and then go ahead with creating resources.\nterraform apply is then given to apply the configuration. This is then approved to actually create resources. In our case a nginx docker image is downloaded and container using that image is created with port forwarding."
  },
  {
    "objectID": "posts/2023-06-02-Terraform-setup/index.html#result",
    "href": "posts/2023-06-02-Terraform-setup/index.html#result",
    "title": "Getting started with Terraform",
    "section": "Result",
    "text": "Result\nA nginx server has been started and we can access it through http:127.0.0.1:8000. We will be greeted with a message “Welcome to nginx!”."
  },
  {
    "objectID": "posts/2023-06-02-Terraform-setup/index.html#terraform-state",
    "href": "posts/2023-06-02-Terraform-setup/index.html#terraform-state",
    "title": "Getting started with Terraform",
    "section": "Terraform state",
    "text": "Terraform state\nTerraform has a state(status) file which stores the current state(metadata) of the infrastructure. Any changes to the resource configuration will be reflected into this file. If working in a team, it is advised to store this state file remotely such as s3, gcs etc with versioning and state locking.\n\nTweaking the infrastructure\nTo understand state file better, we shall change the port from 8000 to 8010 in the docker_container resource block.\nresource \"docker_container\" \"nginx\" {\n  image = docker_image.nginx.image_id\n  name  = \"tutorial\"\n  ports {\n    internal = 80\n    external = 8010\n  }\n}\nYou will see that there will be two state files - terraform.tfstate and terraform.tfstate.backup. On the first run both file’s contents will be same. However, now that we changed the port to 8010, terraform.tfstate will change 8010 but the backup file will retain 8000. This is Terraform’s rollback feature.\nApply the changes using terraform apply."
  },
  {
    "objectID": "posts/2023-06-02-Terraform-setup/index.html#destroy-infrastructure",
    "href": "posts/2023-06-02-Terraform-setup/index.html#destroy-infrastructure",
    "title": "Getting started with Terraform",
    "section": "Destroy infrastructure",
    "text": "Destroy infrastructure\nTo destroy resources after use, we can use terraform destroy command. Review the plan and approve it. Be cautious using this command in an organisation."
  },
  {
    "objectID": "posts/2023-06-12-Attach-iam-policy-to-roles/index.html",
    "href": "posts/2023-06-12-Attach-iam-policy-to-roles/index.html",
    "title": "Terraform - Attach IAM policies to a role",
    "section": "",
    "text": "To access services in AWS, permissions must be given to the services.\nTo control access, AWS has Identity and Access Management (IAM). Each user can take multiple roles. Each role can be restricted to only certain services. This restriction is given in the form of policies.\nPolicies are basically protocols defined in JSON format that allow smooth, secure communication between services.\nTerraform allows for infrastructure management. In this post we will see how to define policies and attach them to a role for our task."
  },
  {
    "objectID": "posts/2023-06-12-Attach-iam-policy-to-roles/index.html#aws-lambda---s3-bucket",
    "href": "posts/2023-06-12-Attach-iam-policy-to-roles/index.html#aws-lambda---s3-bucket",
    "title": "Terraform - Attach IAM policies to a role",
    "section": "AWS Lambda -> S3 bucket",
    "text": "AWS Lambda -&gt; S3 bucket\nFor our task we have a lambda function. It has to retrieve a file from a S3 bucket. Therefore, we need to give lambda function to access this S3 bucket.\n\n\nLambda-S3 policy\n\nresource \"aws_iam_policy\" \"lambda_s3artifact_role_policy\" {\n1  name = \"policy-s3-artifact-access-to-lambda\"\n  description = \"IAM Policy for s3-artifact-access-to-lambda\"\n  policy = jsonencode({\n\"Version\": \"2012-10-17\",\n\"Statement\": [{\n    \"Sid\": \"VisualEditor0\",\n    \"Effect\": \"Allow\",\n    \"Action\": [\n        \"s3:Get*\",\n2        \"s3:List*\"\n    ],\n    \"Resource\": [\n        \"arn:aws:s3:::${var.artifact_bucket}\",\n3        \"arn:aws:s3:::${var.artifact_bucket}/*\"\n    ]}]\n })\n}\n\n\n1\n\nIAM policy name.\n\n2\n\nWhat can this lambda function to do with the S3 bucket.\n\n3\n\nWhich S3 bucket to access."
  },
  {
    "objectID": "posts/2023-06-12-Attach-iam-policy-to-roles/index.html#aws-lambda---dynamodb",
    "href": "posts/2023-06-12-Attach-iam-policy-to-roles/index.html#aws-lambda---dynamodb",
    "title": "Terraform - Attach IAM policies to a role",
    "section": "AWS Lambda -> DynamoDB",
    "text": "AWS Lambda -&gt; DynamoDB\nAfter the file retrieval, the lambda function performs some function and outputs some result. We would like this result to stored inside a DynamoDB table. That means permission to DynamoDB table.\nThat is given by -\n\n\nLambda-DynamoDB policy\n\nresource \"aws_iam_policy\" \"lambda_dynamodb\" {\n  name = \"policy_lambda_access_to_dynamodb\"\n  description = \"\"\n  policy = jsonencode({\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [{\n        \"Effect\": \"Allow\",\n        \"Action\": [\n            \"dynamodb:BatchGetItem\",\n            \"dynamodb:GetItem\",\n            \"dynamodb:Query\",\n            \"dynamodb:Scan\",\n            \"dynamodb:BatchWriteItem\",\n            \"dynamodb:PutItem\",\n1            \"dynamodb:UpdateItem\",\n            \"logs:CreateLogStream\",\n            \"logs:PutLogEvents\",\n2            \"logs:CreateLogGroup\"\n        ],\n3        \"Resource\": \"arn:aws:dynamodb:${var.dynamodb_region}:${var.dynamodb_accountid}:table/${var.dbtable_name}\"\n    }]\n  })\n}\n\n\n1\n\nWhat dynamodb functions can the lambda function do?\n\n2\n\nEnabling logging functionality with Cloudwatch.\n\n3\n\nWhich DynamoDB table to access?"
  }
]