[
  {
    "objectID": "notes/py_for_finance.html",
    "href": "notes/py_for_finance.html",
    "title": "LinkedIn course - Financial data analysis notes",
    "section": "",
    "text": "For financial data analysis, it is important to know certain terminologies, their purpose, and how to calculate them.\nI’m taking Matt Harrison’s Linkedin course on Python for Finance. Here I note down all the Pandas techniques and concepts explored in the course. As always if you find an error, don’t hesitate to contact me."
  },
  {
    "objectID": "notes/py_for_finance.html#chaining",
    "href": "notes/py_for_finance.html#chaining",
    "title": "LinkedIn course - Financial data analysis notes",
    "section": "Chaining",
    "text": "Chaining\nMatt introduces one of the features in Pandas called chaining. It allows reading the code as a recipe. One can simply go through from top to bottom and understand how the code works. We leverage pipe() pandas function. We can use it call any function.\nFrom the two stocks, PFE and JNJ, we need only PFE. So, we can try to use chaining principle.\n\nWithout chaining\n\ndef fix_cols(df):\n    cols = df.columns\n    outer = [col[0] for col in cols]\n    df.columns = outer\n    return df\n\npfe_df1 = pharma_df.iloc[:,1::2]\npfe_df1 = fix_cols(pfe_df1)\npfe_df1\n\n\n\n\n\n\n\n\nAdj Close\nClose\nHigh\nLow\nOpen\nVolume\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n2015-01-02\n21.793072\n29.724857\n30.151802\n29.620493\n29.667933\n16371571\n\n\n2015-01-05\n21.674822\n29.563566\n29.800758\n29.421251\n29.743834\n24786391\n\n\n2015-01-06\n21.855675\n29.810247\n30.227703\n29.525618\n29.667933\n29468681\n\n\n2015-01-07\n22.154793\n30.218216\n30.237192\n29.962049\n30.094877\n20248816\n\n\n2015-01-08\n22.606922\n30.834915\n30.967743\n30.569260\n30.683111\n49169522\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n2023-04-21\n39.779552\n40.209999\n40.299999\n39.910000\n40.090000\n19227100\n\n\n2023-04-24\n39.482765\n39.910000\n40.200001\n39.709999\n40.189999\n17633700\n\n\n2023-04-25\n38.908978\n39.330002\n39.919998\n39.279999\n39.750000\n24492400\n\n\n2023-04-26\n38.216469\n38.630001\n39.189999\n38.400002\n39.160000\n22401400\n\n\n2023-04-27\n38.325291\n38.740002\n38.830002\n38.310001\n38.619999\n22434000\n\n\n\n\n2094 rows × 6 columns\n\n\n\n\n\nWith chaining\n\npfe_df = (pharma_df\n .iloc[:,1::2] # retrieves only PFE stock data\n .pipe(fix_cols) # Removes the ticker and just shows \n)\npfe_df\n\n\n\n\n\n\n\n\nAdj Close\nClose\nHigh\nLow\nOpen\nVolume\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n2015-01-02\n21.793072\n29.724857\n30.151802\n29.620493\n29.667933\n16371571\n\n\n2015-01-05\n21.674822\n29.563566\n29.800758\n29.421251\n29.743834\n24786391\n\n\n2015-01-06\n21.855675\n29.810247\n30.227703\n29.525618\n29.667933\n29468681\n\n\n2015-01-07\n22.154793\n30.218216\n30.237192\n29.962049\n30.094877\n20248816\n\n\n2015-01-08\n22.606922\n30.834915\n30.967743\n30.569260\n30.683111\n49169522\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n2023-04-21\n39.779552\n40.209999\n40.299999\n39.910000\n40.090000\n19227100\n\n\n2023-04-24\n39.482765\n39.910000\n40.200001\n39.709999\n40.189999\n17633700\n\n\n2023-04-25\n38.908978\n39.330002\n39.919998\n39.279999\n39.750000\n24492400\n\n\n2023-04-26\n38.216469\n38.630001\n39.189999\n38.400002\n39.160000\n22401400\n\n\n2023-04-27\n38.325291\n38.740002\n38.830002\n38.310001\n38.619999\n22434000\n\n\n\n\n2094 rows × 6 columns\n\n\n\nAs you can see this makes an easier reading. We use the pipe() to call our fix_cols function. The resulting dataframe has only the outer level column names. Indeed, I agree that as more analysis are added, it gets complicated and harder to understand. Indeed, the intermediate calculation steps are not shown in the final version which makes it difficult to visualise the operation instantaneously."
  },
  {
    "objectID": "notes/py_for_finance.html#returns",
    "href": "notes/py_for_finance.html#returns",
    "title": "LinkedIn course - Financial data analysis notes",
    "section": "Returns",
    "text": "Returns\nHow much percentage of return can be expected? With pandas, we can simply use .pct_change() function and get the values. Plotting them is as simple as shown previously.\n\n(pfe_df\n .pct_change()\n .Close\n .plot()\n)\n\n\nHistogram can be an option but it doesn’t show negative swing. Somewhat appropriate would be to use bar plot.\n\n(pfe_df\n .pct_change()\n .Close\n .iloc[-100:] #last 100 rows\n .plot.bar()\n)\n\n\nThis plot shows the negative trends but the X-axis is illegible. We don’t know on which date the closing stock prices changed. This is because Pandas converts/groups whatever on the x-axis into categorical variables. For example, for categorical variable such as elephants, dogs and cats this works but for dates that isn’t correct.\nWhat if we explicitly say the x-axis as dates.\n\nfig, ax = plt.subplots(figsize=(10,4))\n(pfe_df\n .pct_change()\n .Close\n .iloc[-100:]\n .plot.bar(ax=ax)\n)\nax.xaxis.set_major_locator(dates.MonthLocator())\nax.xaxis.set_major_formatter(dates.DateFormatter('%b-%y'))\nax.xaxis.set_minor_locator(dates.DayLocator())\n\n\n1970?!? Still Pandas converts dates to categorical variables.\nThe solution Matt suggests is to use matplotlib.\n\ndef my_bar(series, ax):\n    ax.bar(series.index, series)\n    ax.xaxis.set_major_locator(dates.MonthLocator())\n    ax.xaxis.set_major_formatter(dates.DateFormatter('%b-%y'))\n    ax.xaxis.set_minor_locator(dates.DayLocator())\n    return series\n\nfig, ax = plt.subplots(figsize=(10,4))\n_ = ( pfe_df\n .pct_change()\n .Close\n .iloc[-100:]\n .pipe(my_bar, ax)\n)\n\n\nLooks good now."
  },
  {
    "objectID": "notes/py_for_finance.html#cumulative-returns",
    "href": "notes/py_for_finance.html#cumulative-returns",
    "title": "LinkedIn course - Financial data analysis notes",
    "section": "Cumulative returns",
    "text": "Cumulative returns\nCumulative returns shows the investment amount gained or lost over time. The formula is given by \\[\ncumulative\\_return = \\frac{(current\\_price - original\\_price)}{(curent\\_price)}\n\\]\n\n(pfe_df\n .Close\n .sub(pfe_df.Close[0])\n .div(pfe_df.Close[0])\n .plot()\n)\n\n&lt;AxesSubplot:xlabel='Date'&gt;\n\n\n\n\n\nAnother alternate way is to numpy’s cumprod function.\n\n(pfe_df\n .Close\n .add(1)\n .cumprod()\n .sub(1)\n .plot()\n)\n\n/Users/dross/miniforge3/envs/mlops/lib/python3.9/site-packages/numpy/core/fromnumeric.py:57: RuntimeWarning:\n\noverflow encountered in accumulate\n\n\n\n&lt;AxesSubplot:xlabel='Date'&gt;\n\n\n\n\n\nAs you can see both plots give the same result.\nIf we’re to use .pipe here, we can do like so:\n\ndef calc_cumrets(df, col):\n     ser = df[col]\n     return (ser\n             .sub(ser[0])\n             .div(ser[0])\n            )\n(pfe_df\n .pipe(calc_cumrets,'Close')\n .plot()\n)\n\n&lt;AxesSubplot:xlabel='Date'&gt;\n\n\n\n\n\n\nLambda functions or anonymous functions\nUsing lambda functions we can make impropmtu functions and use it with our chaining.\nWe would traditionally call a function like so:\n\ndef get_returns(df):\n    return calc_cumrets(df, 'Close')\n\nget_returns(pfe_df)\n\nDate\n2015-01-02    0.000000\n2015-01-05   -0.005426\n2015-01-06    0.002873\n2015-01-07    0.016598\n2015-01-08    0.037344\n                ...   \n2023-04-21    0.352740\n2023-04-24    0.342647\n2023-04-25    0.323135\n2023-04-26    0.299586\n2023-04-27    0.303286\nName: Close, Length: 2094, dtype: float64\n\n\nHowever, if we are to use lambda, then the above code can be written as:\n\n(lambda df: get_returns(df))(pfe_df)\n\nDate\n2015-01-02    0.000000\n2015-01-05   -0.005426\n2015-01-06    0.002873\n2015-01-07    0.016598\n2015-01-08    0.037344\n                ...   \n2023-04-21    0.352740\n2023-04-24    0.342647\n2023-04-25    0.323135\n2023-04-26    0.299586\n2023-04-27    0.303286\nName: Close, Length: 2094, dtype: float64\n\n\nNow, with cumulative returns calculation, it would be useful if those values can be assigned to a new column in the dataframe. It is here that a Pandas feature in .assign function is helpful. It helps create new columns. We can couple .assign and lambda together.\n\npfe_df = (pfe_df\n .assign(cum_rets=lambda df:calc_cumrets(df, 'Close'))\n)"
  },
  {
    "objectID": "notes/py_for_finance.html#volatility",
    "href": "notes/py_for_finance.html#volatility",
    "title": "LinkedIn course - Financial data analysis notes",
    "section": "Volatility",
    "text": "Volatility\nVolatility is a statistical measure of the dispertion of the returns for a given market index in this case stocks. In most cases, higher the volatility, the riskier the stock. It is often measured from either standard deviation or variance between returns from that stock. Remember standard deviation is the measure of deviation of the data relative to its mean.\nJust like mean(), we can calculate std().\n\n(pfe_df\n .Close\n #.mean()\n .std()\n)\n\n6.69077286484766\n\n\nThe .assign() allows consective chaining methods to use these newly created columns. In the below code block, we can use the pct_change_close created in the first line in to the second line. Then, we can calculate 30 day rolling volatility. Rolling is nothing but a time frame in which the volatility is calculated. We can see that for the first 15 days the volatility is NaN(not a number) and on the 30th day, there is an entry.\n\n(pfe_df\n .assign(pct_change_close=pfe_df.Close.pct_change())\n .pct_change_close\n .rolling(30)\n .std()\n)\n\nDate\n2015-01-02         NaN\n2015-01-05         NaN\n2015-01-06         NaN\n2015-01-07         NaN\n2015-01-08         NaN\n                ...   \n2023-04-21    0.008945\n2023-04-24    0.009058\n2023-04-25    0.009192\n2023-04-26    0.009718\n2023-04-27    0.009543\nName: pct_change_close, Length: 2094, dtype: float64\n\n\n\n#rolling volatility\n(pfe_df\n .assign(close_volatility=pfe_df.rolling(30).Close.std(),\n         percent_volatility=pfe_df.Close.pct_change().rolling(30).std())\n .iloc[:,-2:] # fetch only the last two columns\n .plot(subplots=True)\n\n)\n\narray([&lt;AxesSubplot:xlabel='Date'&gt;, &lt;AxesSubplot:xlabel='Date'&gt;],\n      dtype=object)\n\n\n\n\n\nWe can also use .resample to calculate 15 day volatility as we have date as index.\n\n# 15 day volatility\n(pfe_df\n .assign(pct_change_close=pfe_df.Close.pct_change())\n .resample('15D')\n .std()\n)\n\n\n\n\n\n\n\n\nAdj Close\nClose\nHigh\nLow\nOpen\nVolume\ncum_rets\npct_change_close\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n2015-01-02\n0.423588\n0.577756\n0.554235\n0.536322\n0.565220\n8.716662e+06\n0.019437\n0.009335\n\n\n2015-01-17\n0.408931\n0.557768\n0.552896\n0.473576\n0.428281\n6.992238e+06\n0.018764\n0.011556\n\n\n2015-02-01\n0.864666\n1.092056\n1.124398\n1.137237\n1.027460\n1.347935e+07\n0.036739\n0.013012\n\n\n2015-02-16\n0.103079\n0.139368\n0.153115\n0.158683\n0.142704\n5.436585e+06\n0.004689\n0.006524\n\n\n2015-03-03\n0.232107\n0.313819\n0.310265\n0.285296\n0.329080\n6.907642e+06\n0.010557\n0.008226\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2023-02-19\n0.912909\n0.922787\n0.942723\n1.064260\n1.025597\n4.710137e+06\n0.031044\n0.011505\n\n\n2023-03-06\n0.510382\n0.515904\n0.386680\n0.472090\n0.610293\n1.148802e+07\n0.017356\n0.011168\n\n\n2023-03-21\n0.414519\n0.419005\n0.428108\n0.305953\n0.396622\n3.416600e+06\n0.014096\n0.009084\n\n\n2023-04-05\n0.495242\n0.500601\n0.483455\n0.430794\n0.424291\n4.807599e+06\n0.016841\n0.008860\n\n\n2023-04-20\n0.647205\n0.654208\n0.613951\n0.713320\n0.629458\n2.507162e+06\n0.022009\n0.010312\n\n\n\n\n203 rows × 8 columns\n\n\n\n\n# 15 day rolling volatility\n(pfe_df\n .assign(pct_change_close=pfe_df.Close.pct_change())\n .rolling(window=15, min_periods=15)\n .std()\n)\n\n\n\n\n\n\n\n\nAdj Close\nClose\nHigh\nLow\nOpen\nVolume\ncum_rets\npct_change_close\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n2015-01-02\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2015-01-05\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2015-01-06\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2015-01-07\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2015-01-08\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2023-04-21\n0.591463\n0.597864\n0.638385\n0.562327\n0.596404\n3.967966e+06\n0.020113\n0.009691\n\n\n2023-04-24\n0.657297\n0.664410\n0.695511\n0.616351\n0.626652\n3.915626e+06\n0.022352\n0.009383\n\n\n2023-04-25\n0.771667\n0.780017\n0.784388\n0.725364\n0.709656\n4.093416e+06\n0.026241\n0.008983\n\n\n2023-04-26\n0.957201\n0.967559\n0.939399\n0.930587\n0.843585\n4.133347e+06\n0.032551\n0.009559\n\n\n2023-04-27\n1.047608\n1.058944\n1.047459\n1.068349\n1.011878\n3.287318e+06\n0.035625\n0.008130\n\n\n\n\n2094 rows × 8 columns\n\n\n\nWhat happens if the assinged new column name is same as the pandas function name and we have to use further for our analysis? We can include that ‘assigned’ column within [](square) braces and use it. In the below example, we can see how pct_change conflicts with pandas and is therefore must be put inside [] to access it.\n\n# 15 day rolling volatility\n(pfe_df\n .assign(pct_change=pfe_df.Close.pct_change())\n .rolling(window=15, min_periods=15)\n .std()\n #.pct_change\n [\"pct_change\"]\n .plot()\n)\n\n&lt;AxesSubplot:xlabel='Date'&gt;"
  },
  {
    "objectID": "notes/py_for_finance.html#moving-averages-or-rolling-windows",
    "href": "notes/py_for_finance.html#moving-averages-or-rolling-windows",
    "title": "LinkedIn course - Financial data analysis notes",
    "section": "Moving averages or rolling windows",
    "text": "Moving averages or rolling windows\nMoving average(MA) of a stock is calculated to help smooth out the price data by creating a constantly updated average price. It helps to mitigate the impacts of random, short-term fluctuations on the prices of the stock over a time period. There are two types of moving averages - simple which is just the arithmetic mean of the given prices over a specified number of days and exponential which is the weighted average that gives significance to the recent prices than old ones, making it an indicator that is more responsive to new infotmation.\nMA is used to identify the tread direction of a stock or to determine its support and resistance level as it depends on the past prices. The longer the period for the MA, the greater the lag. A 200-day MA has much greater lag than 20-day MA. The gold standard used by investers are 50-day and 200-day MAs.\nShorter MA for short-term investment and longer MA for long-term. A rising MA means upward trend and declining means downward trend.\n\nWhat is a Golden Cross?\nA golden cross is a chart pattern in which a short-term moving average crosses above a long-term moving average. The golden cross is a bullish breakout pattern formed from a crossover involving a security’s short-term moving average such as the 15-day moving average, breaking above its long-term moving average, such as the 50-day moving average. As long-term indicators carry more weight, the golden cross indicates a bull market on the horizon and is reinforced by high trading volumes.\n\n\nLag\nThese lags can be calculated in Pandas using shift function. shift(1) means shift index one place down. shift(2) means two places down. For example,\n\n(pfe_df\n .assign(s1=pfe_df.Close.shift(1),\n         s2=pfe_df.Close.shift(2))\n [[\"s1\",\"s2\"]]\n)\n\n\n\n\n\n\n\n\ns1\ns2\n\n\nDate\n\n\n\n\n\n\n2015-01-02\nNaN\nNaN\n\n\n2015-01-05\n29.724857\nNaN\n\n\n2015-01-06\n29.563566\n29.724857\n\n\n2015-01-07\n29.810247\n29.563566\n\n\n2015-01-08\n30.218216\n29.810247\n\n\n...\n...\n...\n\n\n2023-04-21\n39.849998\n40.240002\n\n\n2023-04-24\n40.209999\n39.849998\n\n\n2023-04-25\n39.910000\n40.209999\n\n\n2023-04-26\n39.330002\n39.910000\n\n\n2023-04-27\n38.630001\n39.330002\n\n\n\n\n2094 rows × 2 columns\n\n\n\nthe Close value in the first row will be on the second row for shift(1) and two rows down for shift(2).\nNow for simple 3-day moving average, we need to average Close, s1, and s2. We can do it manually using a lambda and use the rolling pandas with window=3 specified.\n\n(pfe_df\n .assign(s1=pfe_df.Close.shift(1),\n         s2=pfe_df.Close.shift(2),\n         ma3=lambda df_:df_.loc[:,[\"Close\", \"s1\", \"s2\"]].mean(axis='columns'),\n         ma3_builtin=pfe_df.Close.rolling(3).mean()\n        )\n[[\"s1\",\"s2\",\"ma3\",\"ma3_builtin\"]]\n)\n\n\n\n\n\n\n\n\ns1\ns2\nma3\nma3_builtin\n\n\nDate\n\n\n\n\n\n\n\n\n2015-01-02\nNaN\nNaN\n29.724857\nNaN\n\n\n2015-01-05\n29.724857\nNaN\n29.644212\nNaN\n\n\n2015-01-06\n29.563566\n29.724857\n29.699557\n29.699557\n\n\n2015-01-07\n29.810247\n29.563566\n29.864010\n29.864010\n\n\n2015-01-08\n30.218216\n29.810247\n30.287793\n30.287793\n\n\n...\n...\n...\n...\n...\n\n\n2023-04-21\n39.849998\n40.240002\n40.100000\n40.100000\n\n\n2023-04-24\n40.209999\n39.849998\n39.989999\n39.989999\n\n\n2023-04-25\n39.910000\n40.209999\n39.816667\n39.816667\n\n\n2023-04-26\n39.330002\n39.910000\n39.290001\n39.290001\n\n\n2023-04-27\n38.630001\n39.330002\n38.900002\n38.900002\n\n\n\n\n2094 rows × 4 columns"
  },
  {
    "objectID": "notes/py_for_finance.html#plotting-mas",
    "href": "notes/py_for_finance.html#plotting-mas",
    "title": "LinkedIn course - Financial data analysis notes",
    "section": "Plotting MAs",
    "text": "Plotting MAs\nWe are getting comfortable with plotting. We select the columns needed to plotted - [‘Close’, ‘ma3_builtin’] for last 200 rows.\n\n(pfe_df\n .assign(s1=pfe_df.Close.shift(1),\n         s2=pfe_df.Close.shift(2),\n         ma3=lambda df_:df_.loc[:,[\"Close\", \"s1\", \"s2\"]].mean(axis='columns'),\n         ma3_builtin=pfe_df.Close.rolling(3).mean()\n        )\n [['Close', 'ma3_builtin']]\n .iloc[-200:]\n .plot()\n)\n\n&lt;AxesSubplot:xlabel='Date'&gt;\n\n\n\n\n\nAs we can see the MA smoothes out the little peaks and troughs.\n\nGolden Cross\nSome experts say if there is a crossover between MA-50 and MA-200, it is an indicator to buy or sell.\n\n(pfe_df\n .assign(ma50=pfe_df.Close.rolling(50).mean(),\n         ma200=pfe_df.Close.rolling(200).mean()\n        )\n [[\"Close\",\"ma50\",\"ma200\"]]\n .iloc[-650:]\n .plot()\n)\n\n&lt;AxesSubplot:xlabel='Date'&gt;"
  },
  {
    "objectID": "notes/py_for_finance.html#obv--on-balance-volume",
    "href": "notes/py_for_finance.html#obv--on-balance-volume",
    "title": "LinkedIn course - Financial data analysis notes",
    "section": "OBV- On-balance Volume",
    "text": "OBV- On-balance Volume\nOBV is one such used for technical analysis. It is a momentum indicator that uses volume to predict changes in stock price.\n\nWhat Does On-Balance Volume Tell You?\nThe actual value of the OBV is unimportant; concentrate on its direction. (source: fidelity)\n\nWhen both price and OBV are making higher peaks and higher troughs, the upward trend is likely to continue.\nWhen both price and OBV are making lower peaks and lower troughs, the downward trend is likely to continue.\nDuring a trading range, if the OBV is rising, accumulation may be taking place—a warning of an upward breakout.\nDuring a trading range, if the OBV is falling, distribution may be taking place—a warning of a downward breakout.\nWhen price continues to make higher peaks and OBV fails to make higher peaks, the upward trend is likely to stall or fail. This is called a negative divergence.\nWhen price continues to make lower troughs and OBV fails to make lower troughs, the downward trend is likely to stall or fail. This is called a positive divergence.\n\n\n\nOBV calculation\nIf today’s close is greater than yesterday’s close then: OBV = Yesterday’s OBV + Today’s Volume\nIf today’s close is less than yesterday’s close then: OBV = Yesterday’s OBV – Today’s Volume\nIf today’s close is equal to yesterday’s close then: OBV = Yesterday’s OBV\n\\[\nOBV = OBV_{prev} + \\begin{cases}\nvolume, \\text{ if close &gt; close}_{prev} \\\\\n0, \\text{ if close = close}_{prev}\\\\\n-volume, \\text{ if close &lt; close}_{prev}\n\\end{cases}\n\\]\nwhere\nOBV = current on-balance volume level\nOBVprev = previous on-balance volume level\nvolume = Latest trading volume amount"
  },
  {
    "objectID": "notes/py_for_finance.html#accumulation-distribution-indicator",
    "href": "notes/py_for_finance.html#accumulation-distribution-indicator",
    "title": "LinkedIn course - Financial data analysis notes",
    "section": "Accumulation distribution indicator",
    "text": "Accumulation distribution indicator"
  },
  {
    "objectID": "notes/py_for_finance.html#rsi---relative-strength-index",
    "href": "notes/py_for_finance.html#rsi---relative-strength-index",
    "title": "LinkedIn course - Financial data analysis notes",
    "section": "RSI - Relative strength index",
    "text": "RSI - Relative strength index"
  },
  {
    "objectID": "notes/python_concepts.html",
    "href": "notes/python_concepts.html",
    "title": "Python concepts notes",
    "section": "",
    "text": "These are some of my notes on python concepts I gathered from various sources."
  },
  {
    "objectID": "notes/python_concepts.html#super",
    "href": "notes/python_concepts.html#super",
    "title": "Python concepts notes",
    "section": "Super()",
    "text": "Super()\nsuper() allows to call the base class implicitly without the need to use the base class name explicitly. The main advantage is seen during multiple inheritance. The child classes that may be using cooperative multiple inheritance will call the correct next parent class function in the Method Resolution Order(MRO).\nSyntax Python3 super().__init__() Python2 and still valid in python3 super(childclassname,self).__init__()\nAvoid using like this – super(self.__class__,self).__init__(). This leads to recursion but fortunately python3 syntax standard side steps this problem.\nMore info in stackoverflow"
  },
  {
    "objectID": "notes/python_concepts.html#pycache__",
    "href": "notes/python_concepts.html#pycache__",
    "title": "Python concepts notes",
    "section": "__Pycache__",
    "text": "__Pycache__\n__pycache__ is a directory containing python3 bytecode compiled and ready to be executed. This is done to speed up loading of the modules. Python caches the compiled version of each module in this directory. More info in the python official docs"
  },
  {
    "objectID": "notes/python_concepts.html#zip-function",
    "href": "notes/python_concepts.html#zip-function",
    "title": "Python concepts notes",
    "section": "Zip function",
    "text": "Zip function\nThis function enables combination of elements in 2 or more lists. If the lists are of unequal lengths then the minimum length is taken.\nFor example - zip([1,2,3],[\"one\", \"two\"]) returns a tuple (1, \"one\"), (2, \"two\"). Notice how 3 is not part of the zip operation."
  },
  {
    "objectID": "notes/python_concepts.html#lambda-function",
    "href": "notes/python_concepts.html#lambda-function",
    "title": "Python concepts notes",
    "section": "Lambda function",
    "text": "Lambda function\nAlso knows as anonymous functions helps reduce the need to define unnecessary custom functions. For example, a function that returns a simple arithmetic operation can be made as a lambda function.\nExample - lambda x,y,...: x+y+... - lambda takes several arguments but returns only one expression."
  },
  {
    "objectID": "notes/python_concepts.html#map-function",
    "href": "notes/python_concepts.html#map-function",
    "title": "Python concepts notes",
    "section": "Map function",
    "text": "Map function\nMap is a function that allows some kind of function upon a sequence. This sequence can be list, tuple, string etc.\nSyntax: map(function, seq)\nlambda functions are commonly used with map functions.\nExample -\n\nx = [1,2,3,4]\ndef square(x):\n    return x*x\nmap(square,x) #--&gt;returns an iterator in python3.\n#To return in the desired sequence, use that as prefix in map. \nlist(map(square,x)) #--&gt; returns as a list.\n\n[1, 4, 9, 16]\n\n\nWith lambda function:\n\na = [1,2,3,4]\nlist(map(lambda x:x*x,a))\n\n[1, 4, 9, 16]\n\n\nWith map more than one sequence can be used -\n\nb = [1, 1, 1, 1]\nlist(map(lambda x,y:x+y, a,b))    \n\n[2, 3, 4, 5]"
  },
  {
    "objectID": "notes/python_concepts.html#filter-function",
    "href": "notes/python_concepts.html#filter-function",
    "title": "Python concepts notes",
    "section": "Filter function",
    "text": "Filter function\nThis function is used to filter the outputs if the sequence satisfies some condition. This can be easily written as a list comprehension or a generator. list(filter(lambda x:x%2==0, range(1,11))"
  },
  {
    "objectID": "notes/python_concepts.html#reduce-function",
    "href": "notes/python_concepts.html#reduce-function",
    "title": "Python concepts notes",
    "section": "Reduce function",
    "text": "Reduce function\nThis was removed from the inbuilt functions in python3 and added to functools. It is similar to map function but unlike map it takes only one iterable. list(reduce(lambda x,y:x+y, a)). Internally assigns x and y and calculates the desired function.\nEach of the above function can be substituted with list comprehension.\n\nnamespace and variable scope\nNamespace is the space occupied by an entity in the program. When two or more programs contain the same name, a method to invoke a unique program is done using its module name.\n\n\nLEGB Rule\nVariable scope has three visibilty levels – builtin, global, enclosed, and local.\nLocal scope - variables defined inside a local function. Their lifecycle ends with the local function. Global scope - variable defined at the top of the module and is available for all the functions underneath it. Enclosed scope - seen in nested function. built-in scope - names within python built-in functionality like print().\nChange a global variable inside a local function? use global keyword. Change a enclosed variable inside an enclosed(nested)-local function? use nonlocal keyword. The order of execution follows local, enclosed, global, and built-in.\n\n\nClosures\nClosure is a concept to invoke an enclosed(nested)-local function outside of its scope. This uses a python’s property – functions are first class object. Example -\n\ndef f():\n    return 2\ng = f\n\ng here gets the function f’s location(reference) or the path of the function till the end of it. This functionality is helpful in accessing an enclosed(nested)-local function beyond its scope.\nexample -\n\ndef f():\n    x = 4\n    def g():\n        y = 3\n        return x+y\n    return g\n\na = f()\nprint(a) #--&gt; returns path till function 'g'\nprint(a()) #--&gt; returns 7\nprint(a.__name__) #--&gt; return function 'g' name.\n\n&lt;function f.&lt;locals&gt;.g at 0x1037b8dc0&gt;\n7\ng\n\n\nWhy closures? * Avoid global values * Data hiding * Implement decorators\n\n\nDecorators\nAny callable object that is used to modify a function or class. Adds additional functionality to existing function or class. Basically a wrapper around the existing function where the existing function code couldn’t be changed but additional features/checks are necessary. It is much easier to use the decorator than writing one. The wrapper becomes complex as the functions it wraps get longer.\nA decorator should: * take a function as parameter * add functionality to the function * function needs to return another function\nTwo types: - Function decorator - Class decorator\n\n\nfunction call as parameter\n\ndef f1():\n    print(\"hello from f1\")\ndef f2(function):\n    print(\"hello from f2\")\n    function()\nf2(f1)\n\nhello from f2\nhello from f1\n\n\n\n\nMultiple decorators\n\ndef f1(func):\n    def inner():\n        return \"first \" + func() +\" first\"\n    return inner\n\ndef f2(func):\n    def wrapper():\n        return \"second \" + func()+\" second\"\n    return wrapper\n\n@f1\n@f2\ndef ordinary():\n    return \"good morning\"\n\nprint(ordinary())\n\nfirst second good morning second first\n\n\n&gt;&gt;&gt;&gt; first second good morning second first\nAt first, the f2 decorator is called and prints second good morning second, then f1 decorator takes that output and prefixes and suffixes with first.\n\n\nDecorators with parameters\nTo pass parameters to a decorator, the nested function in the previous case must be defined inside a function.\n\ndef outer(x):\n    def f1(func):\n        def inner():\n            return func() + x\n        return inner\n    return f1\n\n\n@outer(\" everyone\")\ndef greet():\n    return \"good morning\"\n\nprint(greet)\n\n&lt;function outer.&lt;locals&gt;.f1.&lt;locals&gt;.inner at 0x13f874310&gt;\n\n\n\ndef div1(a,b):\n    return a/b\n\ndef div2(a,b,c):\n    return a/b/c\n\nprint(div1(5,0))\nprint(div2(3,4,5))\n\nZeroDivisionError: division by zero\n\n\nTo protect against division-by-zero error, a decorator is written.\n\ndef div_by_zero_dec(func):\n    def inner(*args):\n        for i in args[1:]:\n            if i == 0:\n               return \"Enter valid non-zero input for denominator\"\n        #gives the general error and not decorator output for div2 function if the input is zero\n        #answer =  [\"Enter valid non-zero input for denominator\" if i == 0 else func(*args) for i in args[1:]]\n        return func(*args)\n        #return answer  \n    return inner\n\n@div_by_zero_dec\ndef div1(a,b):\n    return a/b\n\n@div_by_zero_dec\ndef div2(a,b,c):\n    return a/b/c\n\nprint(div1(5,0))\nprint(div2(3,1,0))\n\nEnter valid non-zero input for denominator\nEnter valid non-zero input for denominator\n\n\n\n\nData hiding\nDecorators inherently hides the original function. To avoid that we can use wraps() method from functools.\n\nfrom functools import wraps\ndef outer(x):\n    def f1(func):\n        @wraps(func)\n        def inner():\n            return func() + x\n        return inner\n    return f1\n\n\n@outer(\" everyone\")\ndef greet():\n    return \"good morning\"\n\nprint(greet.__name__)\n\ngreet\n\n\n\n\nClass decorators\nDecorator function can be applied to class methods also.\n\n#check for name equal to Justin is done with a decorator\ndef check_name(func):\n    def inner(name_ref):\n        if name_ref.name == \"Justin\":\n            return \"There is another Justin\"\n        return func(name_ref)\n    return inner\n\nclass Printing():\n    def __init__(self, name):\n        self.name = name\n    \n    @check_name\n    def print_name(self):\n        print(f\"username is {self.name}\")\n\np = Printing(\"Justin\")\np.print_name()\n#username is Justin\n#There is another Justin\n\n'There is another Justin'\n\n\nTo make a class decorator, we need to know about a special method called __call__. If a __call__ method is defined, the object of a class can be used as function call.\n\nclass Printing():\n    def __init__(self, name):\n        self.name = name\n    \n    def __call__(self):\n        print(f\"username is {self.name}\")\n\np = Printing(\"Lindon\")\np()\n# username is Lindon\n\nusername is Lindon\n\n\n\n\nClass decorator on a function\n\nclass Decorator_greet:\n    def __init__(self, func):\n        self.func = func\n    def __call__(self):\n        return self.func().upper()\n\n@Decorator_greet\ndef greet():\n    return \"good morning\"\n\nprint(greet())\n#GOOD MORNING\n\nGOOD MORNING\n\n\n\n\nBuilt-in decorators\n\n@property\n@classmethod\n@staticmethod\n\n\n\n@property\n\nClass methods as attributes The idea of a wrapper is to make changes to code base without hindering its use for the end user. Using @property decorator, a class variable which is now a class method will give the result if used just like accessing the variable. For example - objectname.function() or objectname.function will give the same result without errors. So the user can access just like they did previously.\n\nBorrowing idea from other programming languages, the private variables are defined with __ prefix. So to access those variables, getter, setter, and deleter methods are necessary. Accessing is done with getter method. So it gets @property decorator. Both setters and deleters get @functionname.setter or @functionname.deleter. (verify this. could be wrong)\n\n@classmethod Insted of self, the classmethod decorator takes cls as first argument for its function. These methods can access and modify class states.\n@staticmethod This is similar to classmethod but takes no predefined argument like instance method(self) or classmethod(cls). These methods can’t access class state. So ideally they are used for checking conditions.\n\nDecorator Template\n\nimport functools\n\ndef my_decorator(func):\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        # Do something before\n        result = func(*args, **kwargs)\n        # Do something after\n        return result\n    return wrapper"
  },
  {
    "objectID": "notes/python_concepts.html#context-managers",
    "href": "notes/python_concepts.html#context-managers",
    "title": "Python concepts notes",
    "section": "Context Managers",
    "text": "Context Managers\nUsually with is used w.r.t to file operations, database connections. In addition to using with and as keywords, we can make custom context managers using @contextlib.contextmanger which is a generator decorator.\n\nfrom contextlib import contextmanager\n\n@contextmanager\ndef opening(filename, method):\n    print(\"Enter\")\n    f = open(filename, method) \n    try:\n        yield f\n    finally:\n        f.close()\n        print(\"Exit\")\n\nwith opening(\"hello.txt\", \"w\") as f:\n    print(\"inside\")\n    f.write(\"hello there\")\n\nEnter\ninside\nExit"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Deepak Ramani's blog",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nReading Time\n\n\n\n\n\n\n\n\n\nJun 2, 2023\n\n\nSetup Terraform\n\n\n7 min\n\n\n\n\n\n\n\nApr 27, 2023\n\n\nGetting started with S3 using boto3\n\n\n6 min\n\n\n\n\n\n\n\nFeb 6, 2023\n\n\nSetup Skim PDF reader with VimTeX in Mac OS\n\n\n4 min\n\n\n\n\n\n\n\nSep 24, 2022\n\n\nUsing json_normalize Pandas function\n\n\n15 min\n\n\n\n\n\n\n\nJan 9, 2022\n\n\nManage dotfiles with GNU Stow\n\n\n4 min\n\n\n\n\n\n\n\nSep 18, 2021\n\n\nSetting up Kaggle on Linux/Mac\n\n\n3 min\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#blog-posts",
    "href": "index.html#blog-posts",
    "title": "Deepak Ramani's blog",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nReading Time\n\n\n\n\n\n\n\n\n\nJun 2, 2023\n\n\nSetup Terraform\n\n\n7 min\n\n\n\n\n\n\n\nApr 27, 2023\n\n\nGetting started with S3 using boto3\n\n\n6 min\n\n\n\n\n\n\n\nFeb 6, 2023\n\n\nSetup Skim PDF reader with VimTeX in Mac OS\n\n\n4 min\n\n\n\n\n\n\n\nSep 24, 2022\n\n\nUsing json_normalize Pandas function\n\n\n15 min\n\n\n\n\n\n\n\nJan 9, 2022\n\n\nManage dotfiles with GNU Stow\n\n\n4 min\n\n\n\n\n\n\n\nSep 18, 2021\n\n\nSetting up Kaggle on Linux/Mac\n\n\n3 min\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#projects",
    "href": "index.html#projects",
    "title": "Deepak Ramani's blog",
    "section": "Projects",
    "text": "Projects\n\n\n\n\n\nProject\n\n\nDescription\n\n\nRole\n\n\n\n\n\n\nGrocery Unit Sale Predictor - a MLOPS project\n\n\nAn end-to-end ML project that designs and manages a ML model production workflow. \n\n\nCreator\n\n\n\n\nAutonomous driving framework - a DL project\n\n\nAn end-to-end autonomous driving neural network ML model using simpler CNN architecture. \n\n\nCreator\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-04-27-getting-started-with-s3/data/30-days-create-folds.html",
    "href": "posts/2023-04-27-getting-started-with-s3/data/30-days-create-folds.html",
    "title": "Deepak Ramani's blog",
    "section": "",
    "text": "::: {#4e7e904b .cell _cell_guid=‘b1076dfc-b9ad-4769-8c92-a6c4dae69d19’ _uuid=‘8f2839f25d086af736a60e9eeb907d3b93b6e0e5’ execution=‘{“iopub.execute_input”:“2021-08-16T20:16:18.360034Z”,“iopub.status.busy”:“2021-08-16T20:16:18.358906Z”,“iopub.status.idle”:“2021-08-16T20:16:19.347479Z”,“shell.execute_reply”:“2021-08-16T20:16:19.348020Z”,“shell.execute_reply.started”:“2021-08-16T20:05:04.468564Z”}’ papermill=‘{“duration”:0.999913,“end_time”:“2021-08-16T20:16:19.348363”,“exception”:false,“start_time”:“2021-08-16T20:16:18.348450”,“status”:“completed”}’ tags=‘[]’ execution_count=1}\nimport numpy as np\nimport pandas as pd\nfrom sklearn import model_selection\n:::\n\ndf_train = pd.read_csv(\"../input/30-days-of-ml/train.csv\")\n\n\ndf_train[\"kfold\"] = -1\n\n\nkf = model_selection.KFold(n_splits=5, shuffle=True, random_state=42)\nfor fold, (train_indicies, valid_indicies) in enumerate(kf.split(X=df_train)):\n    df_train.loc[valid_indicies, \"kfold\"] = fold\n\n\ndf_train.to_csv(\"train_folds.csv\", index=False)"
  },
  {
    "objectID": "posts/2021-09-18-kaggle-setup/index.html",
    "href": "posts/2021-09-18-kaggle-setup/index.html",
    "title": "Setting up Kaggle on Linux/Mac",
    "section": "",
    "text": "Most of latest data science innovations happen at Kaggle. Kaggle hosts, in addtion to competitions, a large collection of datasets from various fields. The easiest way to interact with Kaggle is through its public API via command-line tool(CLI). Setting it up outside of Kaggle kernels is one of first tasks. In this post, I will guide you through that process."
  },
  {
    "objectID": "posts/2021-09-18-kaggle-setup/index.html#installation",
    "href": "posts/2021-09-18-kaggle-setup/index.html#installation",
    "title": "Setting up Kaggle on Linux/Mac",
    "section": "Installation",
    "text": "Installation\n\n\nTerminal\n\npip install --user kaggle\n\n\n\n\n\n\n\nTip\n\n\n\nTip: Install kaggle package inside your conda ML development environment rather than outside of it or in base env.\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nDon’t do sudo pip install kaggle as it would require admin privileges for every run."
  },
  {
    "objectID": "posts/2021-09-18-kaggle-setup/index.html#download-api-token",
    "href": "posts/2021-09-18-kaggle-setup/index.html#download-api-token",
    "title": "Setting up Kaggle on Linux/Mac",
    "section": "Download API token",
    "text": "Download API token\n\nCreate/login into your kaggle account.\nFrom the site header, click on your user profile picture and select Account. You will be land on your profile with account tab active.\nScroll down to API section. Click Create New API Token. A json file will be downloaded your default download directory."
  },
  {
    "objectID": "posts/2021-09-18-kaggle-setup/index.html#move-.json-file-to-the-correct-location",
    "href": "posts/2021-09-18-kaggle-setup/index.html#move-.json-file-to-the-correct-location",
    "title": "Setting up Kaggle on Linux/Mac",
    "section": "Move .json file to the correct location",
    "text": "Move .json file to the correct location\n\nMove it to .kaggle in the home directory. Create if absent.\n\n\n\nTerminal\n\ncd\nmkdir ~/.kaggle\nmv &lt;location&gt;/kaggle.json ~/.kaggle/kaggle.json\n\n\nFor your security, ensure that other users of your computer do not have read access to your credentials. On Unix-based systems you can do this with the following command:\n\n\n\nTerminal\n\nchmod 600 ~/.kaggle/kaggle.json\n\n\nRestart the terminal and navigate to the env where kaggle package is installed if necessary."
  },
  {
    "objectID": "posts/2021-09-18-kaggle-setup/index.html#check-if-it-is-properly-installed",
    "href": "posts/2021-09-18-kaggle-setup/index.html#check-if-it-is-properly-installed",
    "title": "Setting up Kaggle on Linux/Mac",
    "section": "Check if it is properly installed",
    "text": "Check if it is properly installed\n\nRun:\n\n\n\nTerminal\n\n$python\n&gt;&gt;&gt;import kaggle\n\nImporting kaggle shouldn’t return an error. If there is error, check whether you’re in the right env where kaggle is installed.\nIf no error, exit the shell and type the following command in the terminal.\n\n\nTerminal\n\nkaggle competitions list\n\nIf installed properly, the command will list all the entered competitions. 1. If not, the binary path may be incorrect. Usually it is installed in ~/.local/bin Try using\n\n\nTerminal\n\n~/.local/bin/kaggle competitions list\n\n\nIf the above command works, export that binary path to the shell environment(bashrc) so that you might use just kaggle next time."
  },
  {
    "objectID": "posts/2021-09-18-kaggle-setup/index.html#api-usage",
    "href": "posts/2021-09-18-kaggle-setup/index.html#api-usage",
    "title": "Setting up Kaggle on Linux/Mac",
    "section": "API usage",
    "text": "API usage\nIt is time to use the Kaggle API. For example, to see what dataset command offers, in the CLI enter\n\n\nTerminal\n\nkaggle dataset --help\n\n\n\n\n\n\n\nTip\n\n\n\nTip: Remember to comply with competition’s terms and conditions before downloading the dataset. You will get an error forbidden if you try to download before agreeing.\n\n\nFor more info on the API, Kaggle’s github page is an excellent resource."
  },
  {
    "objectID": "posts/2023-06-02-Terraform-setup/index.html",
    "href": "posts/2023-06-02-Terraform-setup/index.html",
    "title": "Setup Terraform",
    "section": "",
    "text": "Terraform from Hasicorp is Infrastructure as Code(IaC) tool that is gaining popularity among the data engineers. It lets us build, change and version infrastructure safely and efficiently. As to why one needs Terraform, I shall direct you to this comprehensive article.\nIn this post, I will write a small guide to get us started with Terraform. This post then can be used as a starting guide for my future posts involving Terraform.\n\n\n\n\n\n\nNote\n\n\n\nNote that this installation guide is only for Mac(M1 silicon) and Ubuntu(GCP E2-medium instance). Also this guide will look similar to the example provided in Hashicorp’s website. The intention is to have a guide for my benefit in one place.\n\n\n\n\nA fully functional Macbook or an Ubuntu GCP/AWS instance with sudo privileges."
  },
  {
    "objectID": "posts/2023-06-02-Terraform-setup/index.html#what-is-main.tf",
    "href": "posts/2023-06-02-Terraform-setup/index.html#what-is-main.tf",
    "title": "Setup Terraform",
    "section": "What is main.tf?",
    "text": "What is main.tf?\nIt is a terraform configuration file that describes infrastructure in Terraform. Each terraform config needs a separate directory with a single main.tf.\nThis file contains:\n\nTerraform block\nProviders\nResources\n\n\nTerraform block\nThis block contains Terraform settings for require providers that Terraform will use to provision infrastructure. In our case it is kreuzwerker/docker as source. kreuzwereker is the provider/developer/host and docker is the product we are interested in using. We can specify the minimum version to install. If not mentioned, Terraform will install the lastest version available. Terraform registry is default place to look for popular providers such as AWS, GCP, Azure.\nterraform {\n  required_providers {\n    docker = {\n      source  = \"kreuzwerker/docker\"\n      version = \"~&gt; 3.0.1\"\n    }\n  }\n}\n\n\nProviders\nNow that Terraform knows the “source”, we need to provide a “provider”. Docker is our choice. In this block we can configure docker provider.\nprovider \"docker\" {}\n\n\nResources\nA resource block is used to define components of the provider previously mentioned. So we have a docker provider. We need a docker image and build a container for that image.\nresource \"docker_image\" \"nginx\" {\n  name         = \"nginx\"\n  keep_locally = false\n}\nresource \"docker_container\" \"nginx\" {\n  image = docker_image.nginx.image_id\n  name  = \"tutorial\"\n  ports {\n    internal = 80\n    external = 8000\n  }\n}\nThe resource block takes two strings - resource type and resource name. docker_image is the type and nginx is the name. These two together docker_image.nginx form a unique ID for the resource. For docker_container resource, we use the image from the previous block as reference. So a container is created with that image and ports are also mapped."
  },
  {
    "objectID": "posts/2023-06-02-Terraform-setup/index.html#validate-and-initialise-the-configuration",
    "href": "posts/2023-06-02-Terraform-setup/index.html#validate-and-initialise-the-configuration",
    "title": "Setup Terraform",
    "section": "Validate and initialise the configuration",
    "text": "Validate and initialise the configuration\nTerraform has a bunch of CLI commands to make things easier and faster. One needs to remember only 5-6 commands. Rest can be looked up later.\nTo validate the configuration we use\n\n\nTerminal\n\n1terrform validate\n\n\n1\n\nReturns “Success! The configuration is valid.” if syntactically valid.\n\n\nThen we need to initialise our configuration. This process will download and install the providers mentioned. This command will create a hidden directory .terraform and install docker provider. Terraform also creates a lock file named .terraform.lock.hcl which specifies the exact provider versions used, so that you can control when you want to update the providers used for your project."
  },
  {
    "objectID": "posts/2023-06-02-Terraform-setup/index.html#plan-and-create-infrastructure",
    "href": "posts/2023-06-02-Terraform-setup/index.html#plan-and-create-infrastructure",
    "title": "Setup Terraform",
    "section": "Plan and create infrastructure",
    "text": "Plan and create infrastructure\nTerraform allows us to see what will be created through terraform plan commands. This command is used to give an overview of the things that will be created. An extra, precautionary step. It looks trivial for our use case but imagine a big organisation with several team members managing infrastructure. Even deleting a resource accidently will create chaos. It is always advised to run terraform plan, overview and then go ahead with creating resources.\nterraform apply is then given to apply the configuration. This is then approved to actually create resources. In our case a nginx docker image is downloaded and container using that image is created with port forwarding."
  },
  {
    "objectID": "posts/2023-06-02-Terraform-setup/index.html#result",
    "href": "posts/2023-06-02-Terraform-setup/index.html#result",
    "title": "Setup Terraform",
    "section": "Result",
    "text": "Result\nA nginx server has been started and we can access it through http:127.0.0.1:8000. We will be greeted with a message “Welcome to nginx!”."
  },
  {
    "objectID": "posts/2023-06-02-Terraform-setup/index.html#terraform-state",
    "href": "posts/2023-06-02-Terraform-setup/index.html#terraform-state",
    "title": "Setup Terraform",
    "section": "Terraform state",
    "text": "Terraform state\nTerraform has a state(status) file which stores the current state(metadata) of the infrastructure. Any changes to the resource configuration will be reflected into this file. If working in a team, it is advised to store this state file remotely such as s3, gcs etc with versioning and state locking.\n\nTweaking the infrastructure\nTo understand state file better, we shall change the port from 8000 to 8010 in the docker_container resource block.\nresource \"docker_container\" \"nginx\" {\n  image = docker_image.nginx.image_id\n  name  = \"tutorial\"\n  ports {\n    internal = 80\n    external = 8010\n  }\n}\nYou will see that there will be two state files - terraform.tfstate and terraform.tfstate.backup. On the first run both file’s contents will be same. However, now that we changed the port to 8010, terraform.tfstate will change 8010 but the backup file will retain 8000. This is Terraform’s rollback feature.\nApply the changes using terraform apply."
  },
  {
    "objectID": "posts/2023-06-02-Terraform-setup/index.html#destroy-infrastructure",
    "href": "posts/2023-06-02-Terraform-setup/index.html#destroy-infrastructure",
    "title": "Setup Terraform",
    "section": "Destroy infrastructure",
    "text": "Destroy infrastructure\nTo destroy resources after use, we can use terraform destroy command. Review the plan and approve it. Be cautious using this command in an organisation."
  },
  {
    "objectID": "projects/mlops.html",
    "href": "projects/mlops.html",
    "title": "mlops",
    "section": "",
    "text": "You can read about it in github."
  },
  {
    "objectID": "posts/2022-01-09-dotfiles/index.html",
    "href": "posts/2022-01-09-dotfiles/index.html",
    "title": "Manage dotfiles with GNU Stow",
    "section": "",
    "text": "In this post, I will try to guide in organise your dotfiles in the cloud and manage them using GNU Stow."
  },
  {
    "objectID": "posts/2022-01-09-dotfiles/index.html#what-are-dotfiles",
    "href": "posts/2022-01-09-dotfiles/index.html#what-are-dotfiles",
    "title": "Manage dotfiles with GNU Stow",
    "section": "What are dotfiles?",
    "text": "What are dotfiles?\nFor a casual user, the term dotfiles may sound strange and confusing but it is nothing but application(app) configuration files in developer talk. The apps generally refer to certain files to configure itself.\nPeople usually store these files in a remote location such as a Github repository and retrieve them when needed.\nDotfiles allow personalisation. They can be restored in a new machine saving time. Preparing and organising the dotfiles with some initial effort, help developers save a lot of time later.\nA few examples of dotfiles are .bashrc, .vimrc, .gitignore.\n\n\n\n\n\n\nImportant\n\n\n\nPay attention to personal information inside these files. Never store secure keys, passwords in public domains.\n\n\n\n\nThings to know\n\nWhich app’s config files need to stored.\nWhere do those config files are located.\n\n\n\nCommon config files that need storing\n\n.bashrc or .zshrc\n.vimrc or init.vim(in the case of neovim)\n.gitignore_global and .gitconfig\nTerminal emulator config files\nIDE of choice config files\nAnyother config you want to save\n\nIn fact, if there is an app that you have configured heavily and frequently use, its config files must be stored. In the case the said app doesn’t allow exporting of configurations, it is highly recommended to move onto one that allows it.\n\n\nWhere are most required dotfiles located?\nMost files are present in $HOME or $XDG_CONFIG_HOME directories. $XDG_CONFIG_HOME defines the base directory relative to which user-specific configuration files should be stored. If $XDG_CONFIG_HOME is either not set or empty, a default equal to $HOME/.config should be used."
  },
  {
    "objectID": "posts/2022-01-09-dotfiles/index.html#gnu-stow",
    "href": "posts/2022-01-09-dotfiles/index.html#gnu-stow",
    "title": "Manage dotfiles with GNU Stow",
    "section": "GNU Stow",
    "text": "GNU Stow\nSome prominent results when googled for storing dotfiles are this Atlassian tutorial and using yadm. However, I found those harder to get started.\nGNU Stow on the other hand is an easy-to-use symlink farm manager. As described in their website, it takes distinct packages of software and/or data located in separate directories on the filesystem, and makes them appear to be installed in the same place.\nThis strategy works brilliantly for dotfiles. Borrowing explanation from Brandon Invergo’s article:\n\nThe procedure is simple. I created the ${HOME}/dotfiles directory and then inside it I made subdirectories for all the programs whose configurations I wanted to manage. Inside each of those directories, I moved in all the appropriate files, maintaining the directory structure of my home directory. So, if a file normally resides at the top level of your home directory, it would go into the top level of the program’s subdirectory. If a file normally goes in the default ${XDG_CONFIG_HOME}/${PKGNAME} location (${HOME}/.config/${PKGNAME}), then it would instead go in ${HOME}/dotfiles/${PKGNAME}/.config/${PKGNAME} and so on.\n\n\nInstall Stow\n\n\nTerminal\n\n1sudo apt stow\n\n2brew install stow\n\n\n1\n\nUbuntu\n\n2\n\nHomebrew Mac\n\n\n\n\nPlacing the files\nNow, it might look complex at first. Let me explain with some examples. - .bashrc or .zshrc are present/needed in $HOME directory, so inside $HOME/dotfiles create a subdirectory with bashrc or zshrc and place the original .bashrc or .zshrc file appropriately inside their folder. GNU Stow understands that the dotfile, when symlinked, will create a symlink-copy in the $HOME directory. For future modifications, file in either locations can be edited. But for simplicity, use $HOME/dotfiles directory. - A complicated example would be a config file located deep inside subfolders: nvim’s or neovim’s init.vim or init.lua file. It is present in $HOME/.config/nvim/init.vim. For Stow to understand, it must be placed like this – $HOME/dotfiles/nvim/.config/nvim/init.vim\nFor further reading, I recommend brilliantly written Jake Weisler’s post on GNU Stow.\n\n\nUseful Stow commands\nIf correctly installed, then running the command stow --help should list options to use Stow. Most used commands are\n\n\nTerminal\n\n1stow &lt;packagename&gt;\n2stow -n &lt;packagename&gt;\n3stow -D &lt;packagename&gt;\n4stow -R &lt;packagename&gt;\n\n\n1\n\nactivates symlink\n\n2\n\ntrial runs or simulates symlink generation. Effective for checking for errors\n\n3\n\ndelete stowed package\n\n4\n\nrestows package\n\n\n\n\nActivating Stow\nSo if we have created three subdirectories inside dotfiles say zsh, git, nvim, then\n\n\nTerminal\n\nstow bash git nvim\n\nwill activate their symlinks.\nIf returned to $HOME and $XDG_CONFIG_HOME and verified, then we will see,\n\n\nTerminal\n\n.gitconfig -&gt; .dotfiles/git/.gitconfig\n.zshrc -&gt; .dotfiles/zsh/.zshrc\nnvim -&gt; ../.dotfiles/nvim/.config/nvim\n\nThe most awesome thing in all this is, the directory structure needs to be created only once. For future requirement, one simply clones the dotfiles directory and activates symlinks."
  },
  {
    "objectID": "posts/2022-01-09-dotfiles/index.html#storing-files-in-git",
    "href": "posts/2022-01-09-dotfiles/index.html#storing-files-in-git",
    "title": "Manage dotfiles with GNU Stow",
    "section": "Storing files in Git",
    "text": "Storing files in Git\nThe dotfiles directory now becomes important to store in a remote location for safe keeping. Usually a git repository is the preferred method. For instructions on how to use git, look up various tutorials on Git in the internet.\nIn summary, I have written a short, albeit technical write up on GNU Stow, and its uses for storing dotfiles. Feel free to ask questions in the comments or via various means linked in the blog."
  },
  {
    "objectID": "posts/2022-09-24-using-json-normalize/index.html",
    "href": "posts/2022-09-24-using-json-normalize/index.html",
    "title": "Using json_normalize Pandas function",
    "section": "",
    "text": "Javascript Object Notation(JSON) is a widely used format for storing and exchanging data. Coming from the relational database, it could be difficult to understand NoSQL databases that use JSON to store data and similarly REST API’s response. JSON is also used in storing football event data. It allows easy addition of features in the future.\nThough JSON format allows for easier exchange of data, for analysis, a tabular form would be appropriate. A JSON structure can be of two forms: a JSON object and list of JSON objects. Since our programming language of choice is Python, those structures can be somewhat called as a dictionary object or list of dicts.\n1\nImporting pandas library,\nimport pandas as pd"
  },
  {
    "objectID": "posts/2022-09-24-using-json-normalize/index.html#flattening-a-simple-json",
    "href": "posts/2022-09-24-using-json-normalize/index.html#flattening-a-simple-json",
    "title": "Using json_normalize Pandas function",
    "section": "1 Flattening a simple JSON",
    "text": "1 Flattening a simple JSON\nA dict\nLet us consider a simple dictionary: 3 keys and their respective values.\n\nviv = {\n    \"player_id\" : 15623, \n    \"player_name\" : \"Vivianne Miedema\", \n    \"jersey_number\" : 11}\nviv\n\n{'player_id': 15623, 'player_name': 'Vivianne Miedema', 'jersey_number': 11}\n\n\nWe use the json_normalize API2 to flatten a JSON dict.\n\ndf = pd.json_normalize(viv);df\n\n\n\n\n\n\n\n\nplayer_id\nplayer_name\njersey_number\n\n\n\n\n0\n15623\nVivianne Miedema\n11\n\n\n\n\n\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1 entries, 0 to 0\nData columns (total 3 columns):\n #   Column         Non-Null Count  Dtype \n---  ------         --------------  ----- \n 0   player_id      1 non-null      int64 \n 1   player_name    1 non-null      object\n 2   jersey_number  1 non-null      int64 \ndtypes: int64(2), object(1)\nmemory usage: 152.0+ bytes\n\n\n\nSide Note: If the data contains something that is not compatible with python, in this case a null variable, there are two choices:\n\n\n\nChange null to None\nPass the data through json.loads function\n\n\nChange null to None\n\nnull = None\nviv1 = { \"player_id\" : 15623, \"player_name\" : \"Vivianne Miedema\", \"jersey_number\" : 11, \"player_nickname\" : null}\nviv1\n\n{'player_id': 15623,\n 'player_name': 'Vivianne Miedema',\n 'jersey_number': 11,\n 'player_nickname': None}\n\n\nMake data as string and pass to json.loads\n\nimport json\nviv1 = '{ \"player_id\" : 15623, \"player_name\" : \"Vivianne Miedema\", \"jersey_number\" : 11, \"player_nickname\" : null}'\nviv1 = json.loads(viv1)\nviv1\n\n{'player_id': 15623,\n 'player_name': 'Vivianne Miedema',\n 'jersey_number': 11,\n 'player_nickname': None}\n\n\n\n1.1 A list of dicts\n\nplayer_list = [\n    { \"player_id\" : 15623, \"player_name\" : \"Vivianne Miedema\", \"jersey_number\" : 11, \"player_nickname\" : null },\n    { \"player_id\" : 10658, \"player_name\" : \"Danielle van de Donk\", \"jersey_number\" : 7, \"player_nickname\" : null }\n]\npd.json_normalize(player_list)\n\n\n\n\n\n\n\n\nplayer_id\nplayer_name\njersey_number\nplayer_nickname\n\n\n\n\n0\n15623\nVivianne Miedema\n11\nNone\n\n\n1\n10658\nDanielle van de Donk\n7\nNone\n\n\n\n\n\n\n\nWe have the JSON list of dicts in a tabular form. All the keys become columns and their values as entries.\nWhen we flattern a list with a key-value pair missing for an entry, instead of an error, NaN(not a number) is stored.\n\nplayer_list = [\n    { \"player_id\" : 15623, \"player_name\" : \"Vivianne Miedema\", \"jersey_number\" : 11, \"player_nickname\" : null },\n    { \"player_id\" : 10658, \"player_name\" : \"Danielle van de Donk\"}\n]\npd.json_normalize(player_list)\n\n\n\n\n\n\n\n\nplayer_id\nplayer_name\njersey_number\nplayer_nickname\n\n\n\n\n0\n15623\nVivianne Miedema\n11.0\nNaN\n\n\n1\n10658\nDanielle van de Donk\nNaN\nNaN\n\n\n\n\n\n\n\nNote: See how player_nickname when not specified also turns to NaN from None."
  },
  {
    "objectID": "posts/2022-09-24-using-json-normalize/index.html#flattening-a-multi-level-json",
    "href": "posts/2022-09-24-using-json-normalize/index.html#flattening-a-multi-level-json",
    "title": "Using json_normalize Pandas function",
    "section": "2 Flattening a multi-level JSON",
    "text": "2 Flattening a multi-level JSON\n\n2.1 A simple dict\n\nat_kick0ff = {\n  \"id\":\"d712fb93-c464-4621-98ba-f2bdcd5641db\",\n  \"timestamp\":\"00:00:00.000\",\n  \"duration\":0.0,\n  \"lineup\":{\n      \"player\":{\n        \"id\":15623,\n        \"name\":\"Vivianne Miedema\"\n      },\n      \"position\":{\n        \"id\":23,\n        \"name\":\"Center Forward\"\n      },\n      \"jersey_number\":11\n    }\n}\nat_kick0ff\n\n{'id': 'd712fb93-c464-4621-98ba-f2bdcd5641db',\n 'timestamp': '00:00:00.000',\n 'duration': 0.0,\n 'lineup': {'player': {'id': 15623, 'name': 'Vivianne Miedema'},\n  'position': {'id': 23, 'name': 'Center Forward'},\n  'jersey_number': 11}}\n\n\n\npd.json_normalize(at_kick0ff)\n\n\n\n\n\n\n\n\nid\ntimestamp\nduration\nlineup.player.id\nlineup.player.name\nlineup.position.id\nlineup.position.name\nlineup.jersey_number\n\n\n\n\n0\nd712fb93-c464-4621-98ba-f2bdcd5641db\n00:00:00.000\n0.0\n15623\nVivianne Miedema\n23\nCenter Forward\n11\n\n\n\n\n\n\n\nYou can see that lineup dictionary key’s nested key-value pairs have been expanded into individual columns. If you feel that is unnecessary, we can restrict expansion by using max_level argument. With max_level=1, the flattening goes one level deeper.\n\npd.json_normalize(at_kick0ff, max_level=1)\n\n\n\n\n\n\n\n\nid\ntimestamp\nduration\nlineup.player\nlineup.position\nlineup.jersey_number\n\n\n\n\n0\nd712fb93-c464-4621-98ba-f2bdcd5641db\n00:00:00.000\n0.0\n{'id': 15623, 'name': 'Vivianne Miedema'}\n{'id': 23, 'name': 'Center Forward'}\n11\n\n\n\n\n\n\n\n\n\n2.2 A list of dicts\n\nfirst_pass = [\n  {\n    \"id\":\"15758edb-58cd-49c4-a817-d2ef48ba3bcf\",\n    \"timestamp\":\"00:00:00.504\",\n    \"type\":{\n      \"id\":30,\n      \"name\":\"Pass\"\n    },\n    \"play_pattern\":{\n      \"id\":9,\n      \"name\":\"From Kick Off\"\n    },\n    \"player\":{\n      \"id\":15623,\n      \"name\":\"Vivianne Miedema\"\n    },\n    \"pass\":{\n      \"recipient\":{\n        \"id\":10666,\n        \"name\":\"Dominique Johanna Anna Bloodworth\"\n      },\n      \"length\":25.455845,\n      \"angle\":-2.3561945,\n      \"height\":{\n        \"id\":1,\n        \"name\":\"Ground Pass\"\n      },\n      \"end_location\":[\n        42.0,\n        22.0\n      ]\n    }\n  }, {\n  \"id\" : \"ab5674a4-e824-4143-9f6f-3f1645557413\",\n  \"timestamp\" : \"00:00:04.201\",\n  \"type\" : {\n    \"id\" : 30,\n    \"name\" : \"Pass\"\n  },\n  \"play_pattern\" : {\n    \"id\" : 9,\n    \"name\" : \"From Kick Off\"\n  },\n  \"player\" : {\n    \"id\" : 10666,\n    \"name\" : \"Dominique Johanna Anna Bloodworth\"\n  },\n  \"location\" : [ 45.0, 29.0 ],\n  \"duration\" : 1.795201,\n  \"pass\" : {\n    \"length\" : 51.62364,\n    \"angle\" : 0.55038595,\n    \"height\" : {\n      \"id\" : 3,\n      \"name\" : \"High Pass\"\n    },\n    \"end_location\" : [ 89.0, 56.0 ]\n  }\n}\n]\n    \npd.json_normalize(first_pass)\n\n\n\n\n\n\n\n\nid\ntimestamp\ntype.id\ntype.name\nplay_pattern.id\nplay_pattern.name\nplayer.id\nplayer.name\npass.recipient.id\npass.recipient.name\npass.length\npass.angle\npass.height.id\npass.height.name\npass.end_location\nlocation\nduration\n\n\n\n\n0\n15758edb-58cd-49c4-a817-d2ef48ba3bcf\n00:00:00.504\n30\nPass\n9\nFrom Kick Off\n15623\nVivianne Miedema\n10666.0\nDominique Johanna Anna Bloodworth\n25.455845\n-2.356194\n1\nGround Pass\n[42.0, 22.0]\nNaN\nNaN\n\n\n1\nab5674a4-e824-4143-9f6f-3f1645557413\n00:00:04.201\n30\nPass\n9\nFrom Kick Off\n10666\nDominique Johanna Anna Bloodworth\nNaN\nNaN\n51.623640\n0.550386\n3\nHigh Pass\n[89.0, 56.0]\n[45.0, 29.0]\n1.795201\n\n\n\n\n\n\n\nLimiting the levels…\n\npd.json_normalize(first_pass, max_level=0)\n\n\n\n\n\n\n\n\nid\ntimestamp\ntype\nplay_pattern\nplayer\npass\nlocation\nduration\n\n\n\n\n0\n15758edb-58cd-49c4-a817-d2ef48ba3bcf\n00:00:00.504\n{'id': 30, 'name': 'Pass'}\n{'id': 9, 'name': 'From Kick Off'}\n{'id': 15623, 'name': 'Vivianne Miedema'}\n{'recipient': {'id': 10666, 'name': 'Dominique...\nNaN\nNaN\n\n\n1\nab5674a4-e824-4143-9f6f-3f1645557413\n00:00:04.201\n{'id': 30, 'name': 'Pass'}\n{'id': 9, 'name': 'From Kick Off'}\n{'id': 10666, 'name': 'Dominique Johanna Anna ...\n{'length': 51.62364, 'angle': 0.55038595, 'hei...\n[45.0, 29.0]\n1.795201"
  },
  {
    "objectID": "posts/2022-09-24-using-json-normalize/index.html#flattening-a-json-nested-list",
    "href": "posts/2022-09-24-using-json-normalize/index.html#flattening-a-json-nested-list",
    "title": "Using json_normalize Pandas function",
    "section": "3 Flattening a JSON nested list",
    "text": "3 Flattening a JSON nested list\n\n3.1 A simple dict\nFor this case, let us consider a simpler example than of football event data. The key info has list of dictionaries inside its structure. We call it nested dict.\n\nawfc = {\n    'team': 'AWFC',\n    'location': 'London',\n    'ranking': 1,\n    'info': {\n        'manager': 'Joe',\n        'contacts': {\n          'email': {\n              'coaching': 'joe@afc.com',\n              'general': 'info@afc.com'\n          },\n          'tel': '123456789',\n      }\n    },\n    'players': [\n      { 'name': 'Viv' },\n      { 'name': 'DvD' },\n      { 'name': 'Kim' }\n    ],\n};awfc\n\n{'team': 'AWFC',\n 'location': 'London',\n 'ranking': 1,\n 'info': {'manager': 'Joe',\n  'contacts': {'email': {'coaching': 'joe@afc.com', 'general': 'info@afc.com'},\n   'tel': '123456789'}},\n 'players': [{'name': 'Viv'}, {'name': 'DvD'}, {'name': 'Kim'}]}\n\n\nThe players column has a list of dicts. So, we can flatten that column using record_path argument.\n\npd.json_normalize(awfc, record_path=['players'])\n\n\n\n\n\n\n\n\nname\n\n\n\n\n0\nViv\n\n\n1\nDvD\n\n\n2\nKim\n\n\n\n\n\n\n\nBut, making a separate table with no reference id has no meaning. To prevent that we can append revelant columns to the new table using meta argument. Here we want their team and Telephone number. The tel key lies within info-&gt;contacts-&gt;tel. So, we need provide that path like so ['info', 'contacts', 'tel'].\n\npd.json_normalize(awfc, record_path=['players'], meta=['team',['info', 'contacts', 'tel']])\n\n\n\n\n\n\n\n\nname\nteam\ninfo.contacts.tel\n\n\n\n\n0\nViv\nAWFC\n123456789\n\n\n1\nDvD\nAWFC\n123456789\n\n\n2\nKim\nAWFC\n123456789\n\n\n\n\n\n\n\nThe order in which those paths are mentioned, the order in which those columns are appended.\n\npd.json_normalize(awfc, record_path=['players'], meta=['team',['info', 'contacts', 'tel'],['info', 'manager']])\n\n\n\n\n\n\n\n\nname\nteam\ninfo.contacts.tel\ninfo.manager\n\n\n\n\n0\nViv\nAWFC\n123456789\nJoe\n\n\n1\nDvD\nAWFC\n123456789\nJoe\n\n\n2\nKim\nAWFC\n123456789\nJoe\n\n\n\n\n\n\n\n\n\n3.2 A list of dicts\n\njson_list = [\n    { \n        'team': 'arsenal', \n        'colour': 'red-white',\n        'info': {\n            'staff': { \n                'physio': 'xxxx', \n                'doctor': 'yyyy' \n            }\n        },\n        'players': [\n            { \n                'name': 'Viv', \n                'sex': 'F', \n                'stats': { 'goals': 101, 'assists': 40 } \n            },\n            { \n                'name': 'Beth', \n                'sex': 'F', \n                'stats': { 'goals': 60, 'assists': 25 } \n            },\n        ]\n    },\n    { \n        'team': 'city', \n        'colour': 'blue',\n        'info': {\n            'staff': { \n                'physio': 'aaaa', \n                'doctor': 'bbbb' \n            }\n        },\n        'players': [\n            { 'name': 'Steph', 'sex': 'F' },\n            { 'name': 'Lucy', 'sex': 'F' },\n        ]\n    },\n]\n\npd.json_normalize(json_list)\n\n\n\n\n\n\n\n\nteam\ncolour\nplayers\ninfo.staff.physio\ninfo.staff.doctor\n\n\n\n\n0\narsenal\nred-white\n[{'name': 'Viv', 'sex': 'F', 'stats': {'goals'...\nxxxx\nyyyy\n\n\n1\ncity\nblue\n[{'name': 'Steph', 'sex': 'F'}, {'name': 'Lucy...\naaaa\nbbbb\n\n\n\n\n\n\n\n\npd.json_normalize(json_list, record_path =['players'])\n\n\n\n\n\n\n\n\nname\nsex\nstats.goals\nstats.assists\n\n\n\n\n0\nViv\nF\n101.0\n40.0\n\n\n1\nBeth\nF\n60.0\n25.0\n\n\n2\nSteph\nF\nNaN\nNaN\n\n\n3\nLucy\nF\nNaN\nNaN\n\n\n\n\n\n\n\nHow about we now append the players’ team, colour, and their physio.\n\npd.json_normalize(\n    json_list, \n    record_path =['players'], \n    meta=['team', 'colour', ['info', 'staff', 'physio']]\n)\n\n\n\n\n\n\n\n\nname\nsex\nstats.goals\nstats.assists\nteam\ncolour\ninfo.staff.physio\n\n\n\n\n0\nViv\nF\n101.0\n40.0\narsenal\nred-white\nxxxx\n\n\n1\nBeth\nF\n60.0\n25.0\narsenal\nred-white\nxxxx\n\n\n2\nSteph\nF\nNaN\nNaN\ncity\nblue\naaaa\n\n\n3\nLucy\nF\nNaN\nNaN\ncity\nblue\naaaa"
  },
  {
    "objectID": "posts/2022-09-24-using-json-normalize/index.html#ignoring-key-errors",
    "href": "posts/2022-09-24-using-json-normalize/index.html#ignoring-key-errors",
    "title": "Using json_normalize Pandas function",
    "section": "4 Ignoring key errors",
    "text": "4 Ignoring key errors\n\njson_list = [\n    { \n        'team': 'arsenal', \n        'colour': 'red-white',\n        'info': {\n            'staff': { \n                'physio': 'xxxx', \n                'doctor': 'yyyy' \n            }\n        },\n        'players': [\n            { \n                'name': 'Viv', \n                'sex': 'F', \n                'stats': { 'goals': 101, 'assists': 40 } \n            },\n            { \n                'name': 'Beth', \n                'sex': 'F', \n                'stats': { 'goals': 60, 'assists': 25 } \n            },\n        ]\n    },\n    { \n        'team': 'city', \n        'colour': 'blue',\n        'info': {\n            'staff': { \n                'doctor': 'bbbb' \n            }\n        },\n        'players': [\n            { 'name': 'Steph', 'sex': 'F' },\n            { 'name': 'Lucy', 'sex': 'F' },\n        ]\n    },\n]\n\nNotice that the key physio is missing from the entry team=city. What happens if we try to access physio key inside meta?\n\npd.json_normalize(\n    json_list, \n    record_path =['players'], \n    meta=['team', 'colour', ['info', 'staff', 'physio']],\n)\n\nKeyError: \"Key 'physio' not found. To replace missing values of 'physio' with np.nan, pass in errors='ignore'\"\n\n\nHow come stats.goals and stats.assists didn’t generate an error but that above does? Because, the meta argument expects values to be present for listed keys in meta by default. We can ignore those errors(as suggested) using errors='ignore'\n\npd.json_normalize(\n    json_list, \n    record_path =['players'], \n    meta=['team', 'colour', ['info', 'staff', 'physio']],\n    errors='ignore'\n)\n\n\n\n\n\n\n\n\nname\nsex\nstats.goals\nstats.assists\nteam\ncolour\ninfo.staff.physio\n\n\n\n\n0\nViv\nF\n101.0\n40.0\narsenal\nred-white\nxxxx\n\n\n1\nBeth\nF\n60.0\n25.0\narsenal\nred-white\nxxxx\n\n\n2\nSteph\nF\nNaN\nNaN\ncity\nblue\nNaN\n\n\n3\nLucy\nF\nNaN\nNaN\ncity\nblue\nNaN"
  },
  {
    "objectID": "posts/2022-09-24-using-json-normalize/index.html#custom-separator-sep",
    "href": "posts/2022-09-24-using-json-normalize/index.html#custom-separator-sep",
    "title": "Using json_normalize Pandas function",
    "section": "5 Custom separator sep",
    "text": "5 Custom separator sep\nWe notice that by default pandas uses . to indicate the direction of the path. We can change that using the sep argument.\n\nTip: Usually an underscore is used instead of .\n\n\njson_list = [\n    { \n        'team': 'arsenal', \n        'colour': 'red-white',\n        'info': {\n            'staff': { \n                'physio': 'xxxx', \n                'doctor': 'yyyy' \n            }\n        },\n        'players': [\n            { \n                'name': 'Viv', \n                'sex': 'F', \n                'stats': { 'goals': 101, 'assists': 40 } \n            },\n            { \n                'name': 'Beth', \n                'sex': 'F', \n                'stats': { 'goals': 60, 'assists': 25 } \n            },\n        ]\n    },\n    { \n        'team': 'city', \n        'colour': 'blue',\n        'info': {\n            'staff': { \n                'physio': 'aaaa', \n                'doctor': 'bbbb' \n            }\n        },\n        'players': [\n            { 'name': 'Steph', 'sex': 'F' },\n            { 'name': 'Lucy', 'sex': 'F' },\n        ]\n    },\n]\n\n\npd.json_normalize(\n    json_list, \n    record_path =['players'], \n    meta=['team', 'colour', ['info', 'staff', 'physio']],\n    sep='-&gt;'\n)\n\n\n\n\n\n\n\n\nname\nsex\nstats-&gt;goals\nstats-&gt;assists\nteam\ncolour\ninfo-&gt;staff-&gt;physio\n\n\n\n\n0\nViv\nF\n101.0\n40.0\narsenal\nred-white\nxxxx\n\n\n1\nBeth\nF\n60.0\n25.0\narsenal\nred-white\nxxxx\n\n\n2\nSteph\nF\nNaN\nNaN\ncity\nblue\naaaa\n\n\n3\nLucy\nF\nNaN\nNaN\ncity\nblue\naaaa"
  },
  {
    "objectID": "posts/2022-09-24-using-json-normalize/index.html#adding-context-to-record-and-meta-data-using-record_prefix-and-meta_prefix",
    "href": "posts/2022-09-24-using-json-normalize/index.html#adding-context-to-record-and-meta-data-using-record_prefix-and-meta_prefix",
    "title": "Using json_normalize Pandas function",
    "section": "6 Adding context to record and meta data using record_prefix and meta_prefix",
    "text": "6 Adding context to record and meta data using record_prefix and meta_prefix\n\npd.json_normalize(\n    json_list, \n    record_path=['players'], \n    meta=['team', 'colour', ['info', 'staff', 'physio']],\n    meta_prefix='meta-',\n    record_prefix='player-',\n    sep='-&gt;'\n)\n\n\n\n\n\n\n\n\nplayer-name\nplayer-sex\nplayer-stats-&gt;goals\nplayer-stats-&gt;assists\nmeta-team\nmeta-colour\nmeta-info-&gt;staff-&gt;physio\n\n\n\n\n0\nViv\nF\n101.0\n40.0\narsenal\nred-white\nxxxx\n\n\n1\nBeth\nF\n60.0\n25.0\narsenal\nred-white\nxxxx\n\n\n2\nSteph\nF\nNaN\nNaN\ncity\nblue\naaaa\n\n\n3\nLucy\nF\nNaN\nNaN\ncity\nblue\naaaa"
  },
  {
    "objectID": "posts/2022-09-24-using-json-normalize/index.html#working-with-a-local-file",
    "href": "posts/2022-09-24-using-json-normalize/index.html#working-with-a-local-file",
    "title": "Using json_normalize Pandas function",
    "section": "7 Working with a local file",
    "text": "7 Working with a local file\nIn most scenarios, we won’t be making new JSON object ourselves instead use JSON formatted files. We make use python’s json module and read the file, then use pandas’ json_normalize to flatten it into a dataframe.\n\nimport json\n# load data using Python JSON module\nwith open('movies.json') as f:\n    data = json.load(f)\n    \n# Normalizing data\npd.json_normalize(data)\n\n\n\n\n\n\n\n\nTitle\nUS Gross\nWorldwide Gross\nUS DVD Sales\nProduction Budget\nRelease Date\nMPAA Rating\nRunning Time min\nDistributor\nSource\nMajor Genre\nCreative Type\nDirector\nRotten Tomatoes Rating\nIMDB Rating\nIMDB Votes\n\n\n\n\n0\nThe Land Girls\n146083\n146083\nNaN\n8000000\nJun 12 1998\nR\nNaN\nGramercy\nNone\nNone\nNone\nNone\nNaN\n6.1\n1071.0\n\n\n1\nFirst Love, Last Rites\n10876\n10876\nNaN\n300000\nAug 07 1998\nR\nNaN\nStrand\nNone\nDrama\nNone\nNone\nNaN\n6.9\n207.0\n\n\n2\nI Married a Strange Person\n203134\n203134\nNaN\n250000\nAug 28 1998\nNone\nNaN\nLionsgate\nNone\nComedy\nNone\nNone\nNaN\n6.8\n865.0\n\n\n3\nFour Rooms\n4301000\n4301000\nNaN\n4000000\nDec 25 1995\nR\nNaN\nMiramax\nOriginal Screenplay\nComedy\nContemporary Fiction\nRobert Rodriguez\n14.0\n6.4\n34328.0\n\n\n4\nThe Four Seasons\n42488161\n42488161\nNaN\n6500000\nMay 22 1981\nNone\nNaN\nUniversal\nOriginal Screenplay\nComedy\nContemporary Fiction\nAlan Alda\n71.0\n7.0\n1814.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n63\nBig Things\n0\n0\nNaN\n50000\nDec 31 2009\nNone\nNaN\nNone\nNone\nNone\nNone\nNone\nNaN\nNaN\nNaN\n\n\n64\nBogus\n4357406\n4357406\nNaN\n32000000\nSep 06 1996\nPG\nNaN\nWarner Bros.\nOriginal Screenplay\nComedy\nFantasy\nNorman Jewison\n40.0\n4.8\n2742.0\n\n\n65\nBeverly Hills Cop\n234760478\n316300000\nNaN\n15000000\nDec 05 1984\nNone\nNaN\nParamount Pictures\nOriginal Screenplay\nAction\nContemporary Fiction\nMartin Brest\n83.0\n7.3\n45065.0\n\n\n66\nBeverly Hills Cop II\n153665036\n276665036\nNaN\n20000000\nMay 20 1987\nR\nNaN\nParamount Pictures\nOriginal Screenplay\nAction\nContemporary Fiction\nTony Scott\n46.0\n6.1\n29712.0\n\n\n67\nBeverly Hills Cop III\n42586861\n119180938\nNaN\n50000000\nMay 25 1994\nR\nNaN\nParamount Pictures\nOriginal Screenplay\nAction\nContemporary Fiction\nJohn Landis\n10.0\n5.0\n21199.0\n\n\n\n\n68 rows × 16 columns"
  },
  {
    "objectID": "posts/2022-09-24-using-json-normalize/index.html#working-with-url",
    "href": "posts/2022-09-24-using-json-normalize/index.html#working-with-url",
    "title": "Using json_normalize Pandas function",
    "section": "8 Working with URL",
    "text": "8 Working with URL\nReading a JSON file from an url needs an extra module in requests as any data from the Internet carries overheads that are necessary for efficient exchange of information(REST API). So, in order to read the file contents, we call upon requests’ text attribute which fetches the contents of the file.\nHere, we use json.loads and not json.load as loads function expects contents(string) rather than a file pointer. If looked closely into the json module, the load calls loads using read() on the file.\n\nimport requests\n\nURL = 'https://vega.github.io/vega-datasets/data/cars.json'\n\ndata = json.loads(requests.get(URL).text)\npd.json_normalize(data)\n\n\n\n\n\n\n\n\nName\nMiles_per_Gallon\nCylinders\nDisplacement\nHorsepower\nWeight_in_lbs\nAcceleration\nYear\nOrigin\n\n\n\n\n0\nchevrolet chevelle malibu\n18.0\n8\n307.0\n130.0\n3504\n12.0\n1970-01-01\nUSA\n\n\n1\nbuick skylark 320\n15.0\n8\n350.0\n165.0\n3693\n11.5\n1970-01-01\nUSA\n\n\n2\nplymouth satellite\n18.0\n8\n318.0\n150.0\n3436\n11.0\n1970-01-01\nUSA\n\n\n3\namc rebel sst\n16.0\n8\n304.0\n150.0\n3433\n12.0\n1970-01-01\nUSA\n\n\n4\nford torino\n17.0\n8\n302.0\n140.0\n3449\n10.5\n1970-01-01\nUSA\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n401\nford mustang gl\n27.0\n4\n140.0\n86.0\n2790\n15.6\n1982-01-01\nUSA\n\n\n402\nvw pickup\n44.0\n4\n97.0\n52.0\n2130\n24.6\n1982-01-01\nEurope\n\n\n403\ndodge rampage\n32.0\n4\n135.0\n84.0\n2295\n11.6\n1982-01-01\nUSA\n\n\n404\nford ranger\n28.0\n4\n120.0\n79.0\n2625\n18.6\n1982-01-01\nUSA\n\n\n405\nchevy s-10\n31.0\n4\n119.0\n82.0\n2720\n19.4\n1982-01-01\nUSA\n\n\n\n\n406 rows × 9 columns"
  },
  {
    "objectID": "posts/2022-09-24-using-json-normalize/index.html#conclusion",
    "href": "posts/2022-09-24-using-json-normalize/index.html#conclusion",
    "title": "Using json_normalize Pandas function",
    "section": "9 Conclusion",
    "text": "9 Conclusion\nWe saw the use of json_normalize function in pandas library. It helps take a JSON data, flatten it, and make it as a dataframe for easier analysis."
  },
  {
    "objectID": "posts/2023-04-27-getting-started-with-s3/index.html",
    "href": "posts/2023-04-27-getting-started-with-s3/index.html",
    "title": "Getting started with S3 using boto3",
    "section": "",
    "text": "Boto3 is an AWS python SDK that allows access to AWS services like EC2 and S3. It provides a python object-oriented API and as well as low-level access to AWS services\nimport boto3, botocore\nimport glob\n\nfiles = glob.glob('data/*') #to upload multiple files\nfiles\n\n['data/Player Data.xlsx',\n 'data/30-days-create-folds.ipynb',\n 'data/ARK_GENOMIC_REVOLUTION_ETF_ARKG_HOLDINGS.csv',\n 'data/star_pattern_turtlesim.png']"
  },
  {
    "objectID": "posts/2023-04-27-getting-started-with-s3/index.html#create-a-session-and-client",
    "href": "posts/2023-04-27-getting-started-with-s3/index.html#create-a-session-and-client",
    "title": "Getting started with S3 using boto3",
    "section": "Create a session and client",
    "text": "Create a session and client\nBoto3’s region defaults to N-Virginia. To create buckets in another region, region name has to be explicitly mentioned using session object.\n\nsession = boto3.Session(region_name='us-east-2')\ns3client = session.client('s3')\ns3resource = boto3.resource('s3')\n\nS3 buckets have to follow bucket naming rules.\n\nbucket_names = ['my-s3bucket1-usohio-region', 'my-s3bucket2-usohio-region']\ns3location = {'LocationConstraint': 'us-east-2'}"
  },
  {
    "objectID": "posts/2023-04-27-getting-started-with-s3/index.html#check-if-bucket-exists-in-s3",
    "href": "posts/2023-04-27-getting-started-with-s3/index.html#check-if-bucket-exists-in-s3",
    "title": "Getting started with S3 using boto3",
    "section": "Check if bucket exists in S3",
    "text": "Check if bucket exists in S3\nChecking for something before creation is one of the important tasks to avoid unnecessary errors. Here we check if the buckets already exists.\n\ndef check_bucket(bucket):\n    \"\"\"\n    Checks if a bucket is present in S3\n    args:\n    bucket: takes bucket name\n    \"\"\"\n    try:\n        s3client.head_bucket(Bucket=bucket)\n        print('Bucket exists')\n        return True\n    except botocore.exceptions.ClientError as e:\n        # If a client error is thrown, then check that it was a 404 error.\n        # If it was a 404 error, then the bucket does not exist.\n        error_code = int(e.response['Error']['Code'])\n        if error_code == 403:\n            print(\"Private Bucket. Forbidden Access!\")\n            return True\n        elif error_code == 404:\n            print(\"Bucket Does Not Exist!\")\n            return False\n\n\nfor bucket in bucket_names: \n    print(check_bucket(bucket))\n\nBucket exists\nTrue\nBucket exists\nTrue"
  },
  {
    "objectID": "posts/2023-04-27-getting-started-with-s3/index.html#create-a-bucket-in-s3",
    "href": "posts/2023-04-27-getting-started-with-s3/index.html#create-a-bucket-in-s3",
    "title": "Getting started with S3 using boto3",
    "section": "Create a bucket in S3",
    "text": "Create a bucket in S3\nIf the buckets don’t exist, we create them. We need to supply bucket name, a dictionary specifying in which region the bucket has to be created.\n\nfor bucket_name in bucket_names: \n    if not(check_bucket(bucket_name)):\n        print('Creating a bucket..')\n        s3client.create_bucket(Bucket = bucket_name, CreateBucketConfiguration=s3location)\n\nBucket exists\nBucket exists"
  },
  {
    "objectID": "posts/2023-04-27-getting-started-with-s3/index.html#bucket-versioning",
    "href": "posts/2023-04-27-getting-started-with-s3/index.html#bucket-versioning",
    "title": "Getting started with S3 using boto3",
    "section": "Bucket Versioning",
    "text": "Bucket Versioning\nBucket versioning initial state is not set by default. The response from when not initialised doesn’t carry status information rather status dict is absent. Status expects two return states: enabled, suspended. On first creation, the status is in disabled, an unknown state.\nSo in order to make it appear in the REST response, bucket must be enabled by calling the BucketVersioning() boto3 resource function. If we then check the status, it will be present in the REST response.\n\ndef get_buckets_versioning_client(bucketname):\n    \"\"\"\n    Checks if bucket versioning is enabled/suspended or initialised\n    Args:\n    bucketname: bucket name to check versioning\n    Returns: response status - enabled or suspended\n    \"\"\"\n    response = s3client.get_bucket_versioning(Bucket = bucketname)\n    if 'Status' in response and (response['Status'] == 'Enabled' or response['Status'] == 'Suspended'):\n        print(f'Bucket {bucketname} status: {response[\"Status\"]}')\n        return response['Status']\n    else:\n        print(f'Bucket versioning not initialised for bucket: {bucketname}. Enabling...')\n        s3resource.BucketVersioning(bucket_name=bucketname).enable()\n        enable_response = s3resource.BucketVersioning(bucket_name=bucket_name).status\n        return enable_response\n\n\nfor bucket_name in bucket_names: \n    version_status = get_buckets_versioning_client(bucket_name)\n    print(f'Versioning status: {version_status}')\n\nBucket my-s3bucket1-usohio-region status: Enabled\nVersioning status: Enabled\nBucket my-s3bucket2-usohio-region status: Enabled\nVersioning status: Enabled"
  },
  {
    "objectID": "posts/2023-04-27-getting-started-with-s3/index.html#to-suspend-bucket-versioning",
    "href": "posts/2023-04-27-getting-started-with-s3/index.html#to-suspend-bucket-versioning",
    "title": "Getting started with S3 using boto3",
    "section": "To suspend bucket versioning",
    "text": "To suspend bucket versioning\n\nfor bucket_name in bucket_names:\n    version_status = get_buckets_versioning_client(bucket_name)\n    print(f'Versioning status: {version_status}')\n    if version_status == 'Enabled':\n        print('Disabling again..')\n        s3resource.BucketVersioning(bucket_name=bucket_name).suspend()\n\nBucket my-s3bucket1-usohio-region status: Enabled\nVersioning status: Enabled\nDisabling again..\nBucket my-s3bucket2-usohio-region status: Enabled\nVersioning status: Enabled\nDisabling again.."
  },
  {
    "objectID": "posts/2023-04-27-getting-started-with-s3/index.html#to-enable-bucket-versioning",
    "href": "posts/2023-04-27-getting-started-with-s3/index.html#to-enable-bucket-versioning",
    "title": "Getting started with S3 using boto3",
    "section": "To enable bucket versioning",
    "text": "To enable bucket versioning\n\nfor bucket_name in bucket_names:\n    version_status = get_buckets_versioning_client(bucket_name)\n    print(f'Versioning status: {version_status}')\n    if version_status == 'Suspended':\n        print('Enabling again..')\n        s3resource.BucketVersioning(bucket_name=bucket_name).enable()\n\nBucket my-s3bucket1-usohio-region status: Suspended\nVersioning status: Suspended\nEnabling again..\nBucket my-s3bucket2-usohio-region status: Suspended\nVersioning status: Suspended\nEnabling again.."
  },
  {
    "objectID": "posts/2023-04-27-getting-started-with-s3/index.html#get-bucket-list-from-s3",
    "href": "posts/2023-04-27-getting-started-with-s3/index.html#get-bucket-list-from-s3",
    "title": "Getting started with S3 using boto3",
    "section": "Get bucket list from S3",
    "text": "Get bucket list from S3\nWe can list the buckets in S3 using list_buckets() client function. It return a dict. We can iterate through Buckets key to find the names of the buckets.\n\nbuckets_list = s3client.list_buckets()\nfor bucket in buckets_list['Buckets']:\n    print(bucket['Name'])\n\nmlops-project-sales-forecast-bucket\nmlops-project-sales-forecast-bucket-dr563105-mlops-project\nmy-s3bucket1-usohio-region\nmy-s3bucket2-usohio-region\ns3-for-terraform-state"
  },
  {
    "objectID": "posts/2023-04-27-getting-started-with-s3/index.html#upload-files-to-s3",
    "href": "posts/2023-04-27-getting-started-with-s3/index.html#upload-files-to-s3",
    "title": "Getting started with S3 using boto3",
    "section": "Upload files to S3",
    "text": "Upload files to S3\nBoto3 allows file upload to S3. The upload_file client function requires three mandatory arguments -\n1. filename of the file to be uploaded\n2. bucket_name, Into which bucket the file would be uploaded\n3. key, name of the file in S3\n\ndef upload_files_to_s3(filename, bucket_name, key=None, ExtraArgs=None):\n    \"\"\"\n    Uploads file to S3 bucket\n    Args:\n    filename: takes local filename to be uploaded\n    bucker_name: name of the bucket into which the file is uploaded\n    key: name of the file in the bucket. Default:None\n    ExtraArgs: other arguments. Default:None\n    \"\"\"\n    if key is None:\n        key = filename\n    \n    try:\n        s3client.upload_file(filename,bucket_name,key)\n        print(f'uploaded file:{filename}')\n    except botocore.exceptions.ClientError as e:\n        print(e)\n\nWe can make use of glob module to upload multiple files in a folder\n\nbucket1_files = [files[1],files[2]]\nbucket2_files = [files[0],files[3]]\nbucket1_files, bucket2_files\n\n(['data/30-days-create-folds.ipynb',\n  'data/ARK_GENOMIC_REVOLUTION_ETF_ARKG_HOLDINGS.csv'],\n ['data/Player Data.xlsx', 'data/star_pattern_turtlesim.png'])\n\n\n\nfor file in bucket1_files:\n    upload_files_to_s3(file,bucket_name=bucket_names[0])\n\nuploaded file:data/30-days-create-folds.ipynb\nuploaded file:data/ARK_GENOMIC_REVOLUTION_ETF_ARKG_HOLDINGS.csv\n\n\n\nfor file in bucket2_files:\n    upload_files_to_s3(file,bucket_name=bucket_names[1])\n\nuploaded file:data/Player Data.xlsx\nuploaded file:data/star_pattern_turtlesim.png"
  },
  {
    "objectID": "posts/2023-04-27-getting-started-with-s3/index.html#get-files-list",
    "href": "posts/2023-04-27-getting-started-with-s3/index.html#get-files-list",
    "title": "Getting started with S3 using boto3",
    "section": "Get files list",
    "text": "Get files list\nGetting the files list from each bucket done using list_objects client function. It returns dict and we can iterate through Contents key to retrieve the filenames.\n\nfor bucket in bucket_names:\n    print(f'Listing object inside bucket:{bucket}')\n    list_obj_response = s3client.list_objects(Bucket=bucket)\n    for obj in list_obj_response['Contents']:\n        print(obj['Key'])\n    print()\n\nListing object inside bucket:my-s3bucket1-usohio-region\ndata/30-days-create-folds.ipynb\ndata/ARK_GENOMIC_REVOLUTION_ETF_ARKG_HOLDINGS.csv\n\nListing object inside bucket:my-s3bucket2-usohio-region\ndata/Player Data.xlsx\ndata/star_pattern_turtlesim.png"
  },
  {
    "objectID": "posts/2023-04-27-getting-started-with-s3/index.html#download-files",
    "href": "posts/2023-04-27-getting-started-with-s3/index.html#download-files",
    "title": "Getting started with S3 using boto3",
    "section": "Download files",
    "text": "Download files\nDownloading a file is very similar to uploading one. We need specify bucket name, name of the file to be downloaded, and the destination filename.\n\nprint(f'Downloading files from bucket:{bucket_names[1]}')\ns3client.download_file(Bucket=bucket_names[1],Key='data/star_pattern_turtlesim.png',Filename='downloaded_turtlesim.jpg')\n\nDownloading files from bucket:my-s3bucket2-usohio-region"
  },
  {
    "objectID": "posts/2023-04-27-getting-started-with-s3/index.html#conclusion",
    "href": "posts/2023-04-27-getting-started-with-s3/index.html#conclusion",
    "title": "Getting started with S3 using boto3",
    "section": "Conclusion",
    "text": "Conclusion\nThis blog post shows how to use the boto3 python SDK to manage S3 aws service. With the help of documentation, we can implement require functionalities."
  },
  {
    "objectID": "posts/2023-02-06-skim-vimtex/index.html",
    "href": "posts/2023-02-06-skim-vimtex/index.html",
    "title": "Setup Skim PDF reader with VimTeX in Mac OS",
    "section": "",
    "text": "VimTeX plugin written by Karl Yngve Lervåg is one of the goto plugins to manage LaTeX files with Vim/Neovim text editors. VimTeX allows integration with several PDF viewers. In Mac OS, Skim and Zathura PDF readers allow easy integration with LaTeX. Since Zathura’s installation in Mac OS involves more steps, we will be using Skim for this post."
  },
  {
    "objectID": "posts/2023-02-06-skim-vimtex/index.html#install-skim",
    "href": "posts/2023-02-06-skim-vimtex/index.html#install-skim",
    "title": "Setup Skim PDF reader with VimTeX in Mac OS",
    "section": "Install Skim",
    "text": "Install Skim\nWith Homebrew\n\n\nTerminal\n\nbrew install --cask skim\n\nOr download the dmg file of the current version(as of writing latest version is v1.6.8) from Skim’s website."
  },
  {
    "objectID": "posts/2023-02-06-skim-vimtex/index.html#install-vimtex",
    "href": "posts/2023-02-06-skim-vimtex/index.html#install-vimtex",
    "title": "Setup Skim PDF reader with VimTeX in Mac OS",
    "section": "Install VimTeX",
    "text": "Install VimTeX\nUsing vim-plug plugin manager we add the following line to .vimrc or init.vim or init.lua\n\n\nInside init.vim\n\nPlug 'lervag/vimtex'"
  },
  {
    "objectID": "posts/2023-02-06-skim-vimtex/index.html#pdf-preview",
    "href": "posts/2023-02-06-skim-vimtex/index.html#pdf-preview",
    "title": "Setup Skim PDF reader with VimTeX in Mac OS",
    "section": "Pdf preview",
    "text": "Pdf preview\nConversion between TeX and PDF is one of the most common operations while writing a scientific document. Though it is possible to open the PDF file in one of the commercially available PDF readers, a seamless integration with neovim(in our case) is appreciated. This is where Skim comes into the picture. By default, Skim allows native, seamless integration with the LaTex editor of choice. In our case, we can make VimTex interact with Skim with just a few lines of config."
  },
  {
    "objectID": "posts/2023-02-06-skim-vimtex/index.html#configurations",
    "href": "posts/2023-02-06-skim-vimtex/index.html#configurations",
    "title": "Setup Skim PDF reader with VimTeX in Mac OS",
    "section": "Configurations",
    "text": "Configurations\n\nMinimal setup and Forward Search\nWe require the following lines to make VimTeX talk to Skim within neovim. This direction of communication, is known as forward search.\n\n\nInside init.vim\n\n# Hover over the number at the end of each line to see its importance\n1let g:vimtex_view_method = 'skim'\n\n\n2let g:vimtex_view_skim_sync = 1\n3let g:vimtex_view_skim_activate = 1\n\n\n1\n\nChoose which program to use to view PDF file.\n\n2\n\nValue 1 allows forward search after every successful compilation.\n\n3\n\nValue 1 allows change focus to skim after command :VimtexView is given.\n\n\nThe forward search allows any change made in the TeX file automatically refreshes Skim to reflect those changes in PDF. One of the other common uses is cursor sync between the TeX file and PDF. Setting let g:vimtex_view_skim_sync allows placing the cursor in some position in the Tex file sync with the same position in the PDF after every successful compilation(:VimtexCompile). Setting let g:vimtex_view_skim_activate allows to shift focus of control from neovim to Skim and bring it to foreground.\n\n\nInverse or Backward Search\nSo far there was only one channel of communication between neovim(editor) and Skim. A backward communication is possible but it took quite bit of hacking to get it to work. More on that read this jdhao’s post. However, with the release of VimTex v2.8, it has become simple to setup.\nConsider a scenario where we are going through a paper and find an error, instead of going back to source TeX file and finding the error location can be cumbersome. Using backward search, we can go to the error location from PDF to TeX. For Skim, to activate backward search press shift and command together and click the position in PDF using the mouse. That location gets reflected in the editor in the background. For more information, see :h :VimtexInverseSearch\nNatively, every instance of neovim starts a server 1. With Skim as client and nvim as server, we can interact in that direction.\nIn order to do so, in the preferances pane of Skim, navigate to Sync tab. There, in the PDF-TeX Sync support, make preset as custom, command as nvim(use vim if you use vim editor), and set arguments as --headless -c \"VimtexInverseSearch %line '%file'\".\n\n\n\n\n\n\n\nImportant\n\n\n\nSkim must be started by VimTeX (either through compiler callback or explicitly via lv) for backward sync to work! (This is how Skim “knows” which neovim instance – terminal or GUI – to sync to.)"
  },
  {
    "objectID": "posts/2023-02-06-skim-vimtex/index.html#conclusion",
    "href": "posts/2023-02-06-skim-vimtex/index.html#conclusion",
    "title": "Setup Skim PDF reader with VimTeX in Mac OS",
    "section": "Conclusion",
    "text": "Conclusion\nWith just four lines of settings in the init.vim file and a line in Skim preferances, we can activate both forward and backward search features with VimTeX."
  },
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Archive",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\n\n\n\n\nJun 2, 2023\n\n\nSetup Terraform\n\n\n\n\nMay 10, 2023\n\n\nNotes - LinkedIn course - Financial data analysis\n\n\n\n\nApr 27, 2023\n\n\nGetting started with S3 using boto3\n\n\n\n\nFeb 6, 2023\n\n\nSetup Skim PDF reader with VimTeX in Mac OS\n\n\n\n\nSep 24, 2022\n\n\nUsing json_normalize Pandas function\n\n\n\n\nSep 7, 2022\n\n\nNotes - Python concepts\n\n\n\n\nJan 9, 2022\n\n\nManage dotfiles with GNU Stow\n\n\n\n\nSep 18, 2021\n\n\nSetting up Kaggle on Linux/Mac\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Deepak Ramani",
    "section": "",
    "text": "Hello, I’m Deepak! I am a passionate software engineer with a strong interest in data engineering, DevOps, and machine learning. I thrive on tackling complex problems and finding innovative solutions through collaborative teamwork. I spend my free time learning new tools and technologies. Currently learning “effective Pandas” and Kubeflow.\nWith a background in software engineering I am driven by a desire to leverage the power of data to drive impactful results. During my master’s thesis, I developed an end-to-end system with a focus on implementing simpler CNN model architectures to achieve comparable performance to the latest models. This experience allowed me to deepen my understanding of data collection, preprocessing, and the fusion techniques necessary for night-time driving and collision avoidance.\nI am an enthusiastic learner who stays up-to-date with the latest industry trends and technologies. My strong problem-solving skills, combined with my passion for teamwork, enable me to contribute to challenging projects and deliver high-quality results. I am now seeking new opportunities in Germany to apply my skills and knowledge in data engineering, DevOps, and machine learning, making a meaningful impact within a dynamic organization.\nFeel free to connect with me to explore collaboration opportunities, share insights, or discuss exciting projects in these fields. Let’s embark on this journey together to drive innovation and solve complex problems."
  }
]