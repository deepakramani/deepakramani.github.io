{
  "hash": "d288a1cae0291c8dbbabe43902edbb0c",
  "result": {
    "markdown": "---\ntitle: \"Debezium\"\ndescription: \"My notes taken while learning about Debezium\"\nauthor: \"Deepak Ramani\"\ndate: \"2023-08-20\"\nformat: \n  html: \n    code-annotations: hover\n    code-overflow: wrap\n---\n\nThese are my notes about Debezium. There will be typos or misunderstood concepts. Please as always reach out to me to rectify them.\n\nMy sources were [Baeldung](https://www.baeldung.com/debezium-intro) and official Debezium [documentation](https://debezium.io/documentation/reference/2.3/). Also the [FAQ](https://debezium.io/documentation/faq/) section of the official site gives all the necessary reading.\n\n## What is Debezium?\n\nIt is an open source, low latency distributed platform for capturing change in data from a source and syncing with a target. \n\n## Why is it needed?\nMost companies still use *batch processing* that means - \na. data is not synced immediately,\nb. more resources are required when sync happens,\nc. data replication happens at specific intervals.\n\nHowever, what if streaming data is used or quick reporting on the new data is required? Then we need some kind of a platform/service that periodically checks the source and when an event change occurs, it has to pick the change and transfer it somewhere for analysis or storage. Debezium as a microservice provides that functionality.\n\n## Advantages of using Debezium\n\n- Upstream data(source) change is incrementally pushed downstream(sink) -- *continuous sync*,\n- Instant updates eliminates bulk load updates -- *Data transfer cost less*,\n- Fewer resources required,\n\n## Use cases\n\n1. Keep different data sources in sync,\n2. Update or invalidate a cache,\n3. Update search indexes,\n4. Data sync across microservices\n\n# Debezium\n\nNow that we know why we need Debezium, let us go a bit in depth in its working.\n\nDebezium as a set of distributed services capture changes in the DBs so that applications can see those changes and respond to them.\n\nDebezium records all row-level changes within each DB table in a *change event stream*, and applications simply read these streams to see the changes events in the same order in which they occurred.\n\nThe beauty of Debezium is that it monitors even if the application is down. Upon restart it will start consuming the events where it left off.\n\nDebezium also provides a library of connectors, supporting multiple DBs. Through these connectors, DBs are monitored and the changes are transported to Kakfa topics for further transporation.\n\nSince it is distributed, it is fault and failure tolerant.\n\n## Debezium architecture\n\nDebezium is basically a handshake service/protocol for source and target. It is achieved through connectors.\n\n[![](dbz-server-arch.jpeg)](https://debezium.io/documentation/reference/stable/architecture.html#_debezium_server \"Debezium basic architecture\")\n\n\nAs seen in the image, Debezium architecture consists of three components -- external source DBs, Debezium server, and downstream applications such as Redis, Amazon Kinesis, Google Pub/Sub or Apache Kakfa. Debezium server acts as a mediator to capture and stream real-time data change between source DBs and consumer applications. \n\nIf we look at the entire data pipeline as shown in the below image,\n\n[![](dbz-arch.png)](https://debezium.io/documentation/reference/stable/architecture.html \"Debezium end-to-end data pipeline\")\n\nthe Debezium source connectors monitor and capture real-time data updates puts them into Kafka topics. These topics capture updates in the forms of *commit* log, which is ordered sequentially for easy retrieval. These records are then transfered to downstream applications using *sink* connectors.\n\nIf Debezium connects to Apache Kafka, it generally uses Apache Kafka Connect(AKC). Like Debezium, AKC is also distributed to manage Kafka brokers.\n\n## Debezium connectors vs Kafka Connect\n\nDebezium(DBZ) provides a library of CDC connectors whereas Kafka Connect comprises JDBC connectors to interact with external or downstream applications. \n\nDBZ connectors can only be used as source connectors to monitor external DBs whereas AKC can be both source and sink connectors.\n\nIn Kafka Connect, the JDBC source connector imports or reads real-time messages from any external data source, while the JDBC sink connector distributes real-time records across multiple consumer applications. \n\nJDBC connectors do not capture and stream deleted records, whereas CDC connectors are capable of streaming all real-time updates, including deleted entries. \n\nMoreover, JDBC connections always query database updates at certain and predetermined intervals, while CDC connectors regularly record and transmit real-time event changes as soon as they occur on the respective database systems.\n\n## Connector data\n\nThe change stream consists of `schema` and `payload`.\n\nThe `schema` is not to be confused with DB `schema`. This schema describes the data types of all the fields in the payload section. Usually for `JSON` messages, schema is not included.\n\n\nThe change event data stream payload looks something like this - \n\n```{.json filename=\"change event data stream\"}\n{\n  \"value\":{\n    \"before\":null,\n    \"after\":{\n      \"id\":89,\n      \"name\":\"Colleen Myers\",\n      \"description\":\"Nothing evening stand week reveal quickly man traditional. True positive second because lose detail.\\nNice enough become woman then staff along. Life receive account. Many exist data thousand.\",\n      \"price\":98.0\n    },\n    \"source\":{\n      \"version\":\"2.2.0.Alpha3\",\n      \"connector\":\"postgresql\",\n      \"name\":\"debezium\",\n      \"ts_ms\":1692626411411,\n      \"snapshot\":\"false\",\n      \"db\":\"postgres\",\n      \"sequence\":\"[\\\"23395760\\\",\\\"23395904\\\"]\",\n      \"schema\":\"commerce\",\n      \"table\":\"products\",\n      \"txId\":847,\n      \"lsn\":23395904,\n      \"xmin\":null\n    },\n    \"op\":\"c\",\n    \"ts_ms\":1692626411879,\n    \"transaction\":null\n  }\n}\n```\n\nThis is for inserting an data entry. For an update the stream looks like -\n\n```{.json}\n{\n  \"value\":{\n    \"before\":{\n      \"id\":95,\n      \"name\":\"Steven Cowan\",\n      \"description\":\"Heavy rise something sell case institution chair. Control them might court surface none property. Subject behind them. Quickly near trial.\",\n      \"price\":75.0\n    },\n    \"after\":{\n      \"id\":95,\n      \"name\":\"Yvonne Collins\",\n      \"description\":\"Heavy rise something sell case institution chair. Control them might court surface none property. Subject behind them. Quickly near trial.\",\n      \"price\":75.0\n    },\n    \"source\":{\n      \"version\":\"2.2.0.Alpha3\",\n      \"connector\":\"postgresql\",\n      \"name\":\"debezium\",\n      \"ts_ms\":1692626421005,\n      \"snapshot\":\"false\",\n      \"db\":\"postgres\",\n      \"sequence\":\"[\\\"23399328\\\",\\\"23399456\\\"]\",\n      \"schema\":\"commerce\",\n      \"table\":\"products\",\n      \"txId\":854,\n      \"lsn\":23399456,\n      \"xmin\":null\n    },\n    \"op\":\"u\",\n    \"ts_ms\":1692626421499,\n    \"transaction\":null\n  }\n}\n```\n\n## Streaming Changes - PostgreSQL\n\nThe PostgreSQL connector typically spends the vast majority of its time streaming changes from the PostgreSQL server to which it is connected. This mechanism relies on PostgreSQL’s replication protocol. This protocol enables clients to receive changes from the server as they are committed in the server’s transaction log at certain positions, which are referred to as Log Sequence Numbers (LSNs).\n\nThe Debezium PostgreSQL connector acts as a PostgreSQL client. When the connector receives changes it transforms the events into Debezium create, update, or delete events that include the LSN of the event. The PostgreSQL connector forwards these change events in records to the Kafka Connect framework, which is running in the same process. The Kafka Connect process asynchronously writes the change event records in the same order in which they were generated to the appropriate Kafka topic.[^1]\n\n[^1]: https://debezium.io/documentation/reference/stable/connectors/postgresql.html#postgresql-streaming-changes\n\n\n## Setting up permissions\n\nUse this [link](https://debezium.io/documentation/reference/stable/connectors/postgresql.html#postgresql-permissions) to set up permissions. \n\nCreate Debezium user with minimum privielges to avoid security breaches.\n\n\n## Postgres source connector config\n\n\n```{json filename=\"pg-src-connector.json\"}\n{\n    \"name\": \"pg-src-connector\",\n    \"config\": {\n        \"connector.class\": \"io.debezium.connector.postgresql.PostgresConnector\", # <1>\n        \"tasks.max\": \"1\",\n        \"database.hostname\": \"postgres\", # <2>\n        \"database.port\": \"5432\",\n        \"database.user\": \"postgres\",\n        \"database.password\": \"postgres\",\n        \"database.dbname\": \"postgres\",\n        \"database.server.name\": \"postgres\",\n        \"database.include.list\": \"postgres\",\n        \"topic.prefix\": \"debezium\", # <3>\n        \"schema.include.list\": \"commerce\" # <4>\n    }\n}\n```\n\n1. a postgres DB connector\n2. DB's configurations\n3. Kafka topic prefix that is used in Kafka topic.\n4. The tables in the schema the server monitors for changes\n\nSo the Debezium server will monitor `postgres` DB as user `postgres` and the same password at port `5432` on the tables in schema `commerce`. \n\nThe kafka topic prefix is `debezium`, the schema `commerce` which has two tables - `products` and `users`. \n\nSo, the connector would stream records to these two Kafka topics:\n\n- `debezium.commerce.products` and \n- `debezium.commerice.users`\n\nread more on topic names [here](https://debezium.io/documentation/reference/stable/connectors/postgresql.html#postgresql-topic-names).\n\n\n## Postgresql on AWS RDS \n\nUsing this [link](https://debezium.io/documentation/reference/stable/connectors/postgresql.html#postgresql-on-amazon-rds).\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}