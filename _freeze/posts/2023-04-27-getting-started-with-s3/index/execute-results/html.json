{
  "hash": "1654705dc3d036f6f970a0d847195d2f",
  "result": {
    "markdown": "---\ntitle: Getting started with S3 using boto3\ndescription: An introduction to S3 with boto3 AWS python SDK\ndate: '2023-04-27'\nimage: logo_awss3.png\ncategories:\n  - boto3\n  - s3\n  - aws\nexecute:\n  freeze: true\n---\n\n**Boto3** is an AWS python SDK that allows access to AWS services like EC2 and S3. It provides a python object-oriented API and as well as low-level access to AWS services\n\n```{.bash filename=\"Terminal\"}\npip install boto3\n```\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport boto3, botocore\nimport glob\n\nfiles = glob.glob('data/*') #to upload multiple files\nfiles\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```\n['data/Player Data.xlsx',\n 'data/30-days-create-folds.ipynb',\n 'data/ARK_GENOMIC_REVOLUTION_ETF_ARKG_HOLDINGS.csv',\n 'data/star_pattern_turtlesim.png']\n```\n:::\n:::\n\n\n## Create a session and client\n\nBoto3's region defaults to N-Virginia. To create buckets in another region, region name has to be explicitly mentioned using session object.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nsession = boto3.Session(region_name='us-east-2')\ns3client = session.client('s3')\ns3resource = boto3.resource('s3')\n```\n:::\n\n\nS3 buckets have to follow bucket naming [rules](https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucketnamingrules.html). \n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nbucket_names = ['my-s3bucket1-usohio-region', 'my-s3bucket2-usohio-region']\ns3location = {'LocationConstraint': 'us-east-2'}\n```\n:::\n\n\n## Check if bucket exists in S3\n\nChecking for something before creation is one of the important tasks to avoid unnecessary errors. Here we check if the buckets already exists.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\ndef check_bucket(bucket):\n    \"\"\"\n    Checks if a bucket is present in S3\n    args:\n    bucket: takes bucket name\n    \"\"\"\n    try:\n        s3client.head_bucket(Bucket=bucket)\n        print('Bucket exists')\n        return True\n    except botocore.exceptions.ClientError as e:\n        # If a client error is thrown, then check that it was a 404 error.\n        # If it was a 404 error, then the bucket does not exist.\n        error_code = int(e.response['Error']['Code'])\n        if error_code == 403:\n            print(\"Private Bucket. Forbidden Access!\")\n            return True\n        elif error_code == 404:\n            print(\"Bucket Does Not Exist!\")\n            return False\n```\n:::\n\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nfor bucket in bucket_names: \n    print(check_bucket(bucket))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBucket exists\nTrue\nBucket exists\nTrue\n```\n:::\n:::\n\n\n## Create a bucket in S3\n\nIf the buckets don't exist, we create them. We need to supply bucket name, a dictionary specifying in which region the bucket has to be created. \n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nfor bucket_name in bucket_names: \n    if not(check_bucket(bucket_name)):\n        print('Creating a bucket..')\n        s3client.create_bucket(Bucket = bucket_name, CreateBucketConfiguration=s3location)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBucket exists\nBucket exists\n```\n:::\n:::\n\n\n## Bucket Versioning\n\nBucket versioning initial state is not set by default. The response from  when not initialised doesn't carry status information rather status dict is absent. Status expects two return states: **enabled**, **suspended**. On first creation, the status is in disabled, an unknown state.\n\nSo in order to make it appear in the REST response, bucket must be enabled by calling the `BucketVersioning()` boto3 resource function. If we then check the status, it will be present in the REST response. \n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\ndef get_buckets_versioning_client(bucketname):\n    \"\"\"\n    Checks if bucket versioning is enabled/suspended or initialised\n    Args:\n    bucketname: bucket name to check versioning\n    Returns: response status - enabled or suspended\n    \"\"\"\n    response = s3client.get_bucket_versioning(Bucket = bucketname)\n    if 'Status' in response and (response['Status'] == 'Enabled' or response['Status'] == 'Suspended'):\n        print(f'Bucket {bucketname} status: {response[\"Status\"]}')\n        return response['Status']\n    else:\n        print(f'Bucket versioning not initialised for bucket: {bucketname}. Enabling...')\n        s3resource.BucketVersioning(bucket_name=bucketname).enable()\n        enable_response = s3resource.BucketVersioning(bucket_name=bucket_name).status\n        return enable_response\n\n```\n:::\n\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nfor bucket_name in bucket_names: \n    version_status = get_buckets_versioning_client(bucket_name)\n    print(f'Versioning status: {version_status}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBucket my-s3bucket1-usohio-region status: Enabled\nVersioning status: Enabled\nBucket my-s3bucket2-usohio-region status: Enabled\nVersioning status: Enabled\n```\n:::\n:::\n\n\n## To suspend bucket versioning\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nfor bucket_name in bucket_names:\n    version_status = get_buckets_versioning_client(bucket_name)\n    print(f'Versioning status: {version_status}')\n    if version_status == 'Enabled':\n        print('Disabling again..')\n        s3resource.BucketVersioning(bucket_name=bucket_name).suspend()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBucket my-s3bucket1-usohio-region status: Enabled\nVersioning status: Enabled\nDisabling again..\nBucket my-s3bucket2-usohio-region status: Enabled\nVersioning status: Enabled\nDisabling again..\n```\n:::\n:::\n\n\n## To enable bucket versioning\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nfor bucket_name in bucket_names:\n    version_status = get_buckets_versioning_client(bucket_name)\n    print(f'Versioning status: {version_status}')\n    if version_status == 'Suspended':\n        print('Enabling again..')\n        s3resource.BucketVersioning(bucket_name=bucket_name).enable()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBucket my-s3bucket1-usohio-region status: Suspended\nVersioning status: Suspended\nEnabling again..\nBucket my-s3bucket2-usohio-region status: Suspended\nVersioning status: Suspended\nEnabling again..\n```\n:::\n:::\n\n\n## Get bucket list from S3\n\nWe can list the buckets in S3 using `list_buckets()` client function. It return a dict. We can iterate through `Buckets` key to find the names of the buckets.\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nbuckets_list = s3client.list_buckets()\nfor bucket in buckets_list['Buckets']:\n    print(bucket['Name'])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nmlops-project-sales-forecast-bucket\nmlops-project-sales-forecast-bucket-dr563105-mlops-project\nmy-s3bucket1-usohio-region\nmy-s3bucket2-usohio-region\ns3-for-terraform-state\n```\n:::\n:::\n\n\n## Upload files to S3\n\nBoto3 allows file upload to S3. The `upload_file` client function requires three mandatory arguments - \n\n    1. filename of the file to be uploaded\n    2. bucket_name, Into which bucket the file would be uploaded\n    3. key, name of the file in S3\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\ndef upload_files_to_s3(filename, bucket_name, key=None, ExtraArgs=None):\n    \"\"\"\n    Uploads file to S3 bucket\n    Args:\n    filename: takes local filename to be uploaded\n    bucker_name: name of the bucket into which the file is uploaded\n    key: name of the file in the bucket. Default:None\n    ExtraArgs: other arguments. Default:None\n    \"\"\"\n    if key is None:\n        key = filename\n    \n    try:\n        s3client.upload_file(filename,bucket_name,key)\n        print(f'uploaded file:{filename}')\n    except botocore.exceptions.ClientError as e:\n        print(e)\n```\n:::\n\n\nWe can make use of `glob` module to upload multiple files in a folder\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\nbucket1_files = [files[1],files[2]]\nbucket2_files = [files[0],files[3]]\nbucket1_files, bucket2_files\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```\n(['data/30-days-create-folds.ipynb',\n  'data/ARK_GENOMIC_REVOLUTION_ETF_ARKG_HOLDINGS.csv'],\n ['data/Player Data.xlsx', 'data/star_pattern_turtlesim.png'])\n```\n:::\n:::\n\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\nfor file in bucket1_files:\n    upload_files_to_s3(file,bucket_name=bucket_names[0])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nuploaded file:data/30-days-create-folds.ipynb\nuploaded file:data/ARK_GENOMIC_REVOLUTION_ETF_ARKG_HOLDINGS.csv\n```\n:::\n:::\n\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\nfor file in bucket2_files:\n    upload_files_to_s3(file,bucket_name=bucket_names[1])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nuploaded file:data/Player Data.xlsx\nuploaded file:data/star_pattern_turtlesim.png\n```\n:::\n:::\n\n\n## Get files list\n\nGetting the files list from each bucket done using `list_objects` client function. It returns dict and we can iterate through `Contents` key to retrieve the filenames.\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\nfor bucket in bucket_names:\n    print(f'Listing object inside bucket:{bucket}')\n    list_obj_response = s3client.list_objects(Bucket=bucket)\n    for obj in list_obj_response['Contents']:\n        print(obj['Key'])\n    print()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nListing object inside bucket:my-s3bucket1-usohio-region\ndata/30-days-create-folds.ipynb\ndata/ARK_GENOMIC_REVOLUTION_ETF_ARKG_HOLDINGS.csv\n\nListing object inside bucket:my-s3bucket2-usohio-region\ndata/Player Data.xlsx\ndata/star_pattern_turtlesim.png\n\n```\n:::\n:::\n\n\n## Download files\n\nDownloading a file is very similar to uploading one. We need specify bucket name, name of the file to be downloaded, and the destination filename.\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\nprint(f'Downloading files from bucket:{bucket_names[1]}')\ns3client.download_file(Bucket=bucket_names[1],Key='data/star_pattern_turtlesim.png',Filename='downloaded_turtlesim.jpg')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDownloading files from bucket:my-s3bucket2-usohio-region\n```\n:::\n:::\n\n\n## Conclusion\n\nThis blog post shows how to use the boto3 python SDK to manage S3 aws service. With the help of documentation, we can implement require functionalities.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}